{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a41e761d-5cd0-4ed5-853a-198bc3352026",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science\n",
    "\n",
    "## Lab 5:  Feed Forward Neural Networks 2 (Training, Evaluation, & Interogation)\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Spring 2022**<br/>\n",
    "**Instructors**: Mark Glickman & Pavlos Protopapas<br/>\n",
    "**Lab Team**: Eleni Kaxiras, Marios Mattheakis, Chris Gumb, Shivas Jayaram\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6241aedb-eaab-4ae4-936d-2a9be8d0eb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4e3c94-e2b1-473c-896d-88b3561d8265",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "- Building a NN /w Keras Quick Review\n",
    "- Learning weights from Data\n",
    "- Evaluating a Keras Model\n",
    "- Inspecting Training History\n",
    "- Multi-class Classification Example (Tabular Data)\n",
    "- Interpreting Our Black Box NN\n",
    "- Bagging Review\n",
    "- Image Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911ef05e-a276-4b20-9760-f7e610972133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddf9345-b456-4f50-8979-3ab4d2ee4bfa",
   "metadata": {},
   "source": [
    "Let's revisit the toy dataset from the first NN lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b109d31-f7d4-4094-93f8-9bc32b25c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load toy data\n",
    "toydata = pd.read_csv('data/toyDataSet_1.csv')\n",
    "x_toy = toydata['x'].values.reshape(-1,1)\n",
    "y_toy = toydata['y'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c982688-175b-44be-907b-74edf81fdc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot toy data\n",
    "ax = plt.gca()\n",
    "ax.scatter(x_toy, y_toy)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Toy Dataset');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f4b5cb-b170-4d6c-aa9d-773ddbd3a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, activations\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input # Input \"layer\"?!?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0a7cf9-f0ba-4390-aa02-daea88aa79b8",
   "metadata": {},
   "source": [
    "Here we construct a sequential Keras model with one Dense hidden layer containing only a single neuron with a relu activation.\\\n",
    "The output layer will be of size 1 and have no activation (e.g., 'linear')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5dae4e-18a4-43f0-aecb-04d46129cdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate sequential Keras model and give it a name\n",
    "toy_model = Sequential(name='toy_model')\n",
    "\n",
    "# Despite designation in Keras, Input is not a true layer\n",
    "# It only specifies the shape of the input\n",
    "toy_model.add(Input(shape=(1,)))\n",
    "              \n",
    "# hidden layer with 1 neurons (or nodes)\n",
    "toy_model.add(Dense(1, activation='relu'))\n",
    "\n",
    "# output layer, one neuron \n",
    "toy_model.add(Dense(1,  activation='linear'))\n",
    "\n",
    "toy_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6f0395-b805-4bfb-b835-48de344c514e",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>Compiling the NN</div>\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0360e0-0d9d-43b6-b221-5ef1afde7986",
   "metadata": {},
   "source": [
    "`model.compile(optimizer, loss, metrics, **kwargs)`\n",
    "\n",
    "`optimizer` - defines how the weights are updated (we'll use SGD)\\\n",
    "`loss` - what the model is trying to minimize\\\n",
    "`metric` - list of metrics to report during training process\n",
    "\n",
    "`compile` is used to configure a NN model be for it can be fit. We aren't ready to fit *just* yet, but we are compiling here because doing so reinitilizes the model weights. We are going to manually set our weights before training so we need to to the compilation first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8755aab7-11a9-4bbb-bfde-906b0e51a545",
   "metadata": {},
   "source": [
    "**Q:** Why do I want metrics if I already have a loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0abc64-0a34-498a-b77e-b40fae724e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers, losses, metrics\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.losses import mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532530e1-031d-4a27-8a16-7eed11ae0a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model.compile(optimizer=SGD(learning_rate=1e-1), loss='mse', metrics=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce92cc48-7c24-4c3a-81b7-f87c6c384b24",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>A little nudge...</div>\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d201bf91-2102-4418-a649-830e583d3fb7",
   "metadata": {},
   "source": [
    "Our toy model is very simply. It only has 4 weights. But the problem there are only 4 possible weight values that would make this a good fit. That is like finding a needle in a haystack. So we will cheat a bit and initialize our weights in the 'neighborhood' of the true weights which generated the data. Our future models will be complex enough that they won't need to worry about finding a specific combination of weights: some local minima (but not all) will do the job just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a3892-b122-4c81-a45f-69899a73dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A FUNCTION THAT READS AND PRINTS OUT THE MODEL WEIGHTS/BIASES\n",
    "def print_weights(model):\n",
    "    weights = model.get_weights()\n",
    "    print(dict(zip([\"w1\", \"b1\", \"w2\", \"b2\"], [weight.flatten()[0] for weight in weights])))\n",
    "\n",
    "# MANUALLY SETTING THE WEIGHTS/BIASES\n",
    "## True weights from data generating function\n",
    "# w1 = 2\n",
    "# b1 = 0.0\n",
    "# w2  = 1\n",
    "# b2  = 0.5\n",
    "\n",
    "# Initialize weights to that 'neighborhood'\n",
    "w1 = 1.85\n",
    "b1 = -0.5\n",
    "w2  = 0.9\n",
    "b2  = 0.4\n",
    "\n",
    "# Store current weight data structure\n",
    "weights = toy_model.get_weights()\n",
    "# hidden layer\n",
    "weights[0][0] = np.array([w1]) #weights \n",
    "weights[1]    = np.array([b1]) # biases\n",
    "# output layer \n",
    "weights[2]    =  np.array([[w2]]) # weights\n",
    "weights[3]    = np.array([b2])    # bias\n",
    "# hidden layer\n",
    "# Set the weights\n",
    "toy_model.set_weights(weights)\n",
    "\n",
    "\n",
    "print('Manually Initialized Weights:')\n",
    "print_weights(toy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e416fd-4b31-4ce3-bb18-82434dc70521",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>Forward Pass Review</b></div>\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30759248-61e2-4898-a833-f1d40cf4e672",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Input, Hidden Layers, and Output Layers**\n",
    "\n",
    "The **forward** pass through an FFNN  is  a sequence of linear (affine) and nonlinear operations (activation). \n",
    "\n",
    "\n",
    "<img src=\"fig/forward.jpg\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df89806-1c6e-4a64-9926-2c148f277a26",
   "metadata": {},
   "source": [
    "We use the model's `predict` method to execut the forward pass with a linspace spanning the range of the `x` data as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ac1d91-dd5c-45d0-b478-c6b7107d1533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "x_lin = np.linspace(x_toy.min(), x_toy.max(), 500)\n",
    "y_hat = toy_model.predict(x_lin)\n",
    "\n",
    "# Plot\n",
    "plt.scatter(x_toy, y_toy, alpha=0.4, lw=4, label='data')\n",
    "plt.plot(x_lin, y_hat, label='NN', ls='--', c='r')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Predictions with Manually Set Weights')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd3bf66-493f-4c34-9eab-e5f1e93f01f6",
   "metadata": {},
   "source": [
    "We'll let back propogation and stochastic gradient descent take it from here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f65789-93d5-4016-8f49-17ab61ad0fd1",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>Back Propogation & SGD Review</b></div>\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3fc58c-d293-4724-99a8-16f967633916",
   "metadata": {
    "tags": []
   },
   "source": [
    "The **backward** pass is the training. It is based on the chain rule of calculus, and it calculates the gradient of the loss w.r.t. the weights. This gradient is used by the optimizer to update the weights to minimize the loss function.\n",
    "\n",
    "<img src=\"fig/dl.jpg\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6131d6d9-01c2-4198-b73f-c14166882587",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Batching, stochastic gradient descent, and epochs**\n",
    "Shuffle and partition the dataset in mini-batches to help escape from local minima. Each batch is seen once per epoch. And thus each observation is also seen once per epoch. We can train the network for as many epochs as we like.\n",
    "\n",
    "<img src=\"fig/batching.jpg\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00898b8-509a-4538-b73e-f34513f8c91c",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>Reproducibility</b></div>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404fe0b1-3d80-44d4-b520-8433d6b28ba6",
   "metadata": {},
   "source": [
    "There is a lot of stochasticity in the training of neural networks, from weight initilizations, to shuffling of data between epochs.\\\n",
    "Below is some code that appears to be working for me to get reproducible results. Though I think some of the steps taken may be purely superstitious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41778686-ea40-41ac-b31d-e9593a5a5dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advice gleaned from: https://deeplizard.com/learn/video/HcW0DeWRggs\n",
    "import os\n",
    "import random as rn\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "tf.random.set_seed(109)\n",
    "np.random.seed(109)\n",
    "rn.seed(109)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d7caa-5e59-43c4-ac9d-4a360213e4b0",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>Fitting the NN</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba718f3d-89eb-4076-895c-ec6162d139aa",
   "metadata": {},
   "source": [
    "`Model.fit(x=None, y=None, batch_size=None, epochs=1, verbose=\"auto\", validation_split=0.0, validation_data=None, shuffle=True, **kwargs)`\n",
    "\n",
    "`batch_size` - number of observations overwhich the loss is calculated before each weight update\\\n",
    "`epochs` - number of times the complete dataset is seen in the fitting process\\\n",
    "`verbose` - you can silence the training output by setting this to `0`\\\n",
    "`validation_split` - splits off a portion of the `x` and `y` training data to be used as validation (see warning below)\\\n",
    "`validation_data` - tuple designating a seperate `x_val` and `y_val` dataset\\\n",
    "`shuffle` - whether to shuffle the training data before each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84658823-fb5a-471b-ad6b-3855f9ae0037",
   "metadata": {},
   "source": [
    "We fit the model for 100 `epochs` and set `batch_size` to 64. The results of `fit()` are then stored in a variable called `history`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b311cb86-5f5f-462e-a34e-ea5b4facc205",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit model and store training histry\n",
    "history = toy_model.fit(x_toy, y_toy, epochs=100, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71af7ba-5ba5-4954-8a48-13ebcd2ddb97",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>Plot Training History</div>\n",
    "\n",
    "`history.history` is a dictionary which contains information from each training epoch (no, I don't know the rationale behind the double name). Use it to plot the loss across epochs. Don't for get those labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0e3841-1181-41b9-889f-f60349463403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.plot(history.history['loss'], c='r')\n",
    "plt.ylabel('MSE loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('NN Training History');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed814d1c-cda6-4a28-8b87-e86a01b45b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights learned for the data\n",
    "print_weights(toy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b559dc7d-b037-4862-b46c-3fa41a9393c0",
   "metadata": {},
   "source": [
    "We can see we've moved much closer to the original weights after fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba711706-e31b-43dc-81f4-8f72e753b406",
   "metadata": {},
   "source": [
    "But visualizing our model's predictions will make this more clear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d00d3-0f9d-4703-8ef4-8fd8cb1c28d8",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>Predict & Plot</div>\n",
    "\n",
    "We use the model's `predict` method on a linspace, `x_lin`, which we construct to span the range of the dataset's $x$ values. We save the resulting predictions in `y_hat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9959bc-0df3-45de-8c7c-a01e06094e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "x_lin = np.linspace(x_toy.min(), x_toy.max(), 500)\n",
    "y_hat = toy_model.predict(x_lin)\n",
    "\n",
    "# Plot\n",
    "plt.scatter(x_toy, y_toy, alpha=0.4, lw=4, label='data')\n",
    "plt.plot(x_lin, y_hat, label='NN', ls='--', c='r')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Predictions After Training')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3a5b72-2e84-44ce-a7a6-a37d59b059cd",
   "metadata": {},
   "source": [
    "Much better! But perhaps you are not impressed yet?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9753f2-5f70-48a3-ad86-a9e85e8e3f6a",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>An Ugly Function</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc092658-203b-4146-b7f9-9e6b5c1e6b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ugly_function(x):\n",
    "    if x < 0:\n",
    "        return np.exp(-(x**2))/2 + 1 + np.exp(-((10*x)**2))\n",
    "    else:\n",
    "        return np.exp(-(x**2)) + np.exp(-((10*x)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2747c95-c730-40e9-9543-4b088dbde60f",
   "metadata": {},
   "source": [
    "How do you feel about the prospect of manually setting the weights to approximate this beauty?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7227f6-e223-472d-9d0a-758eec337405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "x_ugly = np.linspace(-3,3,1500) # create x-values for input\n",
    "y_ugly = np.array(list(map(ugly_function, x_ugly)))\n",
    "\n",
    "# Plot data\n",
    "plt.plot(x_ugly, y_ugly);\n",
    "plt.title('An Ugly Function')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db342804-cdc4-43e5-ae9f-f93708277c02",
   "metadata": {},
   "source": [
    "And here we don't even have the option of cheating by initializing our weights strategically!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63aa163-0bed-4675-b516-09a7e3fbecbc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <strong>🏋🏻‍♂️ TEAM ACTIVITY:</strong> We're Gonna Need a Bigger Model...</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde86997-5b88-4c46-8d23-4d35441729b8",
   "metadata": {},
   "source": [
    "1. Complete the `build_nn` function for quickly constructing different NN architectures.\n",
    "    \n",
    "2. Use `build_nn` to construct an NN to approximate the ugly function\n",
    "    \n",
    "3. Compile the model & print its summary\n",
    "    -  _Tip: Remember, if it is the last line of the cell, Jupyter will display the return value without an explicit call to `print()` required. In fact, Jupyter uses its own `display()` function which often results in prettier output for tables_\n",
    "4. Fit the model\n",
    "\n",
    "5. Plot the training history\n",
    "    \n",
    "Hyperparameters to play with:\n",
    "- Architecture\n",
    "    - Number of hidden layers\n",
    "    - Number of neurons in each hidden layer\n",
    "    - Hidden layers' `activation` function\n",
    "- Training\n",
    "    - `SGD`'s `learning_rate`\n",
    "    - `batch_size`\n",
    "    - `epochs`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289fdb37-50e5-48ac-83ce-4e7c587c1c3b",
   "metadata": {},
   "source": [
    "**NN Build Function**\\\n",
    "**Arguments:**\n",
    "- `name`: str - A name for your NN.\n",
    "- `input_shape`: tuple - number of predictors in input (remember the trailing ','!)\n",
    "- `hidden_dims`: list of int - specifies the number of neurons in each hidden layer\n",
    "    - Ex: [2,4,8] would mean 3 hidden layers with 2, 4, and 8 neurons respectively\n",
    "- `hidden_act`: str (or Keras activation object) - activation function used by all hidden layers\n",
    "- `out_dim`: int - number of output neurons a.k.a 'ouput units'\n",
    "- `out_act`: str (or Keras activation object) - activation function used by output layer\n",
    "\n",
    "**Hint:** We will reuse this function throughout the notebook in different settings, but you should go ahead and set some sensible defaults for *all* of the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52522f7f-9c6d-42fd-99dd-3fd3e56840e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_NN(name='NN', input_shape=(1,), hidden_dims=[2], hidden_act='relu', out_dim=1, out_act='linear'):\n",
    "    # your code here\n",
    "\n",
    "    # end your code here\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c1e2b3-ca7c-40df-9cb9-c0789b34a889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../solutions/sol1_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3741afad-308a-4133-a795-cf2b389e3079",
   "metadata": {},
   "source": [
    "**Build & Print Model Summary**\n",
    "\n",
    "Use `build_NN` to construct your model and store it in a variable called `ugly_model`.\\\n",
    "You can play with `hidden_dims` and `hidden_act`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720891c1-d3e8-4cc7-817e-7833c9182a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5c3bba-27b2-4fd8-aa44-866200f5d040",
   "metadata": {},
   "source": [
    "**Compile**\\\n",
    "Use the `SGD` optimizer and `'mse'` as your loss.\\\n",
    "You can expermiment with `SGD`'s `learning_rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e70d15-6fdf-444c-9a31-2d0d7f86f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f9fbe1-2813-4af1-b078-7ac6b1ed49e2",
   "metadata": {},
   "source": [
    "**Fit**\\\n",
    "Fit `ugly_model` on `x_ugly` and `y_ugly` and story the results in a variable called `history`.\\\n",
    "You can experiment with `epochs` and `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea3befe-3377-46f2-a270-ee7818a34a47",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391bfa5a-b214-41b9-85da-abdaefef75dd",
   "metadata": {},
   "source": [
    "**Plot Training History**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e176270f-9c87-44d0-a26d-3cea3c0cb23b",
   "metadata": {},
   "source": [
    "Plot the model's training history. Don't forget your axis labels!\\\n",
    "**Hint:** Remember, `fit` returns a `history` object which itself has a `history` dictionary attribute. Because this (2nd object) is a dictionary, you can always use its `keys`method if you don't know what's in it. You can also access the history from the model itself. Ex: `ugly_model.history.history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a6c8a1-c8bc-4509-b18a-35a9c62cf514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot History\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19db5da-8711-4db5-8b9e-d8dfaab39379",
   "metadata": {},
   "source": [
    "**Get Predictions**\\\n",
    "Similar to `sklearn` models, `keras` models have a `predict` method. Use your model's `predict` method to predict on `x_ugly` and store the results in a variable called `y_hat_ugly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3330bc76-db76-4118-9081-cf4b2a7b03be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0806279e-25d1-4182-9fcb-2eff186ae97c",
   "metadata": {},
   "source": [
    "**Plot Predictions**\\\n",
    "Run the cell below to compare your model's predictions to the true (ugly) function. Still not quite right? Try tweaking some of the hyperparameters above and re-run the cells in this section to see if you can improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23b2155-fe87-4eb5-b741-678933d0dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "plt.plot(x_ugly, y_ugly, alpha=0.4, lw=4, label='true function')\n",
    "plt.plot(x_ugly, y_hat_ugly, label='NN', ls='--', c='r')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa0b8c1-6f2f-4be4-b7f4-3d5d79100966",
   "metadata": {},
   "source": [
    "**End of Team Activity**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d67f17-33af-4e43-a6f2-0833568fc9c4",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>Multi-class Classification with Keras</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b40fd2-fb50-471f-9898-7dc9c1be3035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ddc6fa-a3ef-43d3-a446-2084e5a3ab5f",
   "metadata": {},
   "source": [
    "So far we've only used our new Keras powers for toy regression problems. We'll move to classification next... but with 3 classes!\n",
    "\n",
    "This example will use `seaborn`'s penguins dataset (a worthy successor to the connonical iris dataset.)\n",
    "\n",
    "We'll build a model to identify a penguin's species from its other features. In the process we'll dust off our Python skills with a quick run through a basic model building workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17accc1f-95c4-412b-96c6-438e6e569f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring on the penguins!\n",
    "penguins = sns.load_dataset('penguins')\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4df759-38f8-4802-ba36-b1773c5fc1de",
   "metadata": {},
   "source": [
    "We have 3 species of penguins living across 3 different islands. There are measurements of bill length, bill depth, flipper length, and body mass. We also have categorcial variable for each penguin's sex giving us a total of 7 features.\n",
    "\n",
    "Here's a plot that tries to show too much at once. But you can ignore the marker shapes and sizes. The bill and flipper length alone ($x$ and $y$ axes) seem too already provide a fair amount of information about the species (color)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89353d87-9714-4f25-8780-a7b0a5170507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot penguins with too much info\n",
    "sns.relplot(data=penguins, x='flipper_length_mm', y='bill_length_mm',\n",
    "            hue='species', style='sex', size='body_mass_g', height=6);\n",
    "plt.title('Penguins!', fontdict={'color': 'teal', 'size': 20, 'weight': 'bold', 'family': 'serif'});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11cae86-cdb5-4a5d-8637-05d25beca2c8",
   "metadata": {},
   "source": [
    "You may have noticed some pesky `NaN`s when we displayed the beginning of the DataFrame.\\\n",
    "We should investigate further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ce5cf4-278c-46f9-b0a2-b1521ab5003e",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>Missingness</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb70802-2660-49d8-96c8-5e0cd568e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many missing values in each column?\n",
    "penguins.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10003c3-3006-427e-900b-215745dbdfbc",
   "metadata": {},
   "source": [
    "Let's take a look at them first all the rows with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e2fe3a-6287-47a7-b5d6-93a130f08e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows with missingness\n",
    "penguins[penguins.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e0242-3867-4cab-a9f7-14c9994ef0ff",
   "metadata": {},
   "source": [
    "Yikes! There are two observations where all predictors except `species` and `island` are missing.\\\n",
    "These rows won't be of any use to us. We see that dropping rows missing `body_mass_g` will take care of most our missingness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8cbf86-02e6-4250-ad56-14de597b77fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the bad rows identified above\n",
    "penguins = penguins.dropna(subset=['body_mass_g'])\n",
    "# Check state of missingness after dropping\n",
    "penguins.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfba3250-8630-42b9-8781-41b5064e535a",
   "metadata": {},
   "source": [
    "It looks like there are 9 rows where `sex` is missing. We can try to **impute** these values.\\\n",
    "But first, take a look at our DataFrame again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24579cf7-2eec-4499-8a46-90fca9643505",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc97b9d-8724-490b-bf04-6ad06b94d052",
   "metadata": {},
   "source": [
    "Notice how the indices go from `2` to `4`. What happened to `3`?\\\n",
    "It was one of the rows we dropped! This issue with the indices can cause headaches later on (think `loc`/`iloc` distinction).\n",
    "But we can make things good as new using the `reset_index` method. Just be sure to set `drop=True`, otherwise the old indices will be added to the DataFrame as a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7916ae-3ea6-45ac-b3a4-0645a0ea294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index\n",
    "penguins = penguins.reset_index(drop=True)\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebce211-89ed-4618-a23d-0b4b2de1bd12",
   "metadata": {},
   "source": [
    "Much better!\\\n",
    "Now, on to imputing the missing `sex` values. Let's take a look at the `value_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6166ca3c-5901-4e7f-81bf-43b31552fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts of each unique value in the dataset\n",
    "penguins.sex.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e418a5-91a8-48be-9388-ebae46ba81e0",
   "metadata": {},
   "source": [
    "It's almost an even split. We'll impute the **mode** because it's a quick fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126d06f0-0a5f-4335-9df9-edd46a563f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mode here should match the value with the most counts above\n",
    "sex_mode = penguins.sex.mode()[0]\n",
    "sex_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c64663-46ed-4b0e-8c3e-9feee3d03648",
   "metadata": {},
   "source": [
    "Finally, we use `fillna` to replace the remaining `NaN`s with the `sex_mode` and confirm that there are no more missing values in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a81519-356d-48e4-a9f3-737fc73dd45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with most common value (i.e, mode)\n",
    "penguins = penguins.fillna(sex_mode)\n",
    "penguins.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46a5791-af00-47a2-bf69-2c8b302188b9",
   "metadata": {},
   "source": [
    "**Q:** Imputing the mode here was very easy, but does this approach make you a bit nervous? Why? Is there some other way we could have imputed this values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d14be10-c126-4a1b-9e1f-3231e1eaaf1d",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>Preprocessing</div>\n",
    "\n",
    "We can't just throw this DataFrame at a neural network as it is. There's some work we need to do first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeb8048-f53f-4d93-bfbc-1e0b3e4a367f",
   "metadata": {},
   "source": [
    "**Separate predictors from response variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9e8c2-8681-4946-8c5d-74ee2ea0ce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate response from predictors\n",
    "response = 'species'\n",
    "X = penguins.drop(response, axis=1)\n",
    "y = penguins[response]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce788dcb-8a38-4c06-8543-7f9bc64ece35",
   "metadata": {},
   "source": [
    "**Encode Categorical Predictor Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec3e24-fe5f-4f5a-b14f-33232882922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the predictor data types\n",
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed547262-aadb-4382-ac8a-962b6313f41d",
   "metadata": {},
   "source": [
    "Both `island` and `sex` are categotical. We can use `pd.get_dummies` to one-hot-encode them (don't forget to `drop_first`!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f0fdcf-ffc0-4c0c-a37d-dfc44afa2d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the categorical columns\n",
    "cat_cols = ['island', 'sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693b40b-16ab-4ac2-a7d0-81be5b094ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode the categorical columns\n",
    "X_design = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "X_design.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b94b8b1-5c53-4fbd-8972-d828b56616d0",
   "metadata": {},
   "source": [
    "From the remaining columns we can infer that the 'reference' values for our categorical variables are `island = 'Biscoe'`, and `sex = 'Female'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d79a67f-d7f5-4c2e-a2b7-0f7828664388",
   "metadata": {},
   "source": [
    "**Feature Scaling**\n",
    "\n",
    "We should take a closer look at the range of values our predictors take on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b996ae54-a675-4243-9930-748a2d077f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Summary stats of predictors\n",
    "X_design.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a0524c-2aa1-4f6f-b7ba-4af40651bb03",
   "metadata": {},
   "source": [
    "Our features are not on the same scale. Just compare the min/max of `bill_depth_mm` and `body_mass_g` for example.\\\n",
    "This can slow down neural network training for reasons we'll see in an upcoming lecture.\n",
    "\n",
    "Let's make use of `sklearn`'s `StandardScaler` to standardize the data, centering each predictor at 0 and setting their standard deviations to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8660d8-880e-41c1-b645-4a134a41ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3961cf5-f249-4cea-b878-784af9123c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember the column names for later; we'll lose them when we scale\n",
    "X_cols = X_design.columns\n",
    "# Saving the scaler object in a variable allows us to reverse the transformation later\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d715b6c4-9193-4752-978a-f3e8cb36df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scaler was passed a pandas DataFrame but returns a numpy array\n",
    "type(X_scaled), X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee50991-3169-439b-af0a-f38c9b78198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can always add the column names back later if we need to\n",
    "pd.DataFrame(X_scaled, columns=X_cols).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a5ed5e-6e02-4357-abbb-1c9ddd73d83f",
   "metadata": {},
   "source": [
    "**Encoding the Response Variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be56c48c-c59a-42b5-b3dd-113b6bac93b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at our response\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b469e02-215e-42b0-a595-56d0f6c3f901",
   "metadata": {},
   "source": [
    "Our response variable is still a `string`. We need to turn it into some numerical representation for our neural network.\\\n",
    "We could to this ourselves with a few list comprehensions, but `sklearn`'s `LabelEncoder` makes this very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a91968-3489-474f-9c0b-bceed51610a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189936f5-b405-4f9d-8462-a5cb2ea8aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode string labels as integers\n",
    "# LabelEncoder uses the familar fit/transform methods we saw with StandardScaler\n",
    "labenc = LabelEncoder().fit(y)\n",
    "y_enc = labenc.transform(y)\n",
    "y_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7676a0f3-1b85-40d3-8487-94dd379a3cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can recover the class labels from the encoder object later\n",
    "labenc.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52442710-ea06-4b9f-889b-98224698be3f",
   "metadata": {},
   "source": [
    "This gets us part of the way there. But the penguin species are **categorical** not **ordinal**. Keeping the labels as integers implies that species `2` is twice as \"different\" from species `1` as it is from species `0`. We want to perform a conversion here similar to the one-hot encoding above, except will will not 'drop' one of the values. This is where Keras's `to_categorical` utility function comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe68d6f-9e64-4248-86a8-980493af1b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0f8dce-541c-462a-92b0-240f2f47d6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat = to_categorical(y_enc)\n",
    "y_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2848da-50bb-4dd2-9e0f-9091635af006",
   "metadata": {},
   "source": [
    "Perfect!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f21abae-931f-44a4-8675-170a9ede5be6",
   "metadata": {},
   "source": [
    "**Q:** If this is what our array of response variables looks like, what will this mean for the output layer of our neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d51cf2-0fbe-4c2e-a9e5-577e82644aa8",
   "metadata": {},
   "source": [
    "**Train-test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39d32a7-a246-4163-8e36-c93ed2d873d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1fa8d5-f26b-4271-8e58-588b9551103a",
   "metadata": {},
   "source": [
    "You may be familiar with using `train_test_split` to split the `X` and `y` arrays themselves. But here we will using it to create a set of train and test *indices*.\n",
    "\n",
    "We'll see later that being able to determine which rows in the original `X` and `y` ended up in train or test will be helpful.\n",
    "\n",
    "**Q:** But couldn't we just sample integers to get random indices? Why use `train_test_split`?\n",
    "\n",
    "**A:** Because `train_test_split` allows for **stratified** splitting!\n",
    "\n",
    "Here we use a trick to stratify on both the `sex` and `island` variables by concatinating their values together. This gives us a total of 6 possible values (2 sexs x 3 islands). By stratifying on this column we help ensure that each of the 6 possible sex/island combinations is equally represented in both train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e61ded-d730-4bef-99fe-6b9abc51db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate categorical columns; use this for stratified splitting\n",
    "strat_col = penguins['sex'].astype('str') + penguins['island'].astype('str')\n",
    "strat_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cdbaff-55e5-4b9f-be9e-b79dcf7579cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test indices\n",
    "train_idx, test_idx = train_test_split(np.arange(X_scaled.shape[0]),\n",
    "                                                  test_size=0.5,\n",
    "                                                  random_state=109,\n",
    "                                                  stratify=strat_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c72e55-2315-4c2d-92ca-68f2ebd1a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index into X_scaled and y_cat to create the train and test sets\n",
    "X_train = X_scaled[train_idx]\n",
    "y_train = y_cat[train_idx]\n",
    "X_test = X_scaled[test_idx]\n",
    "y_test = y_cat[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e305af-f846-4696-9f8b-1ef31555a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check on the resulting shapes\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8138cf-6376-40da-9e02-7a5b3edf71f3",
   "metadata": {},
   "source": [
    "**Validation Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582ae665-28c3-4be2-bb25-ed21c812bba5",
   "metadata": {},
   "source": [
    "Here is where those indices we saved come in handy.\\\n",
    "We also want to also ensure equal representation across train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b2fb76-21f4-4b1f-b5f9-8d4b305473f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset original stratify column using saved train split indices\n",
    "strat_col2 = strat_col.iloc[train_idx]\n",
    "strat_col2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb99dc2a-d092-46ab-b87a-10e04add704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and validation splits from original train split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=109,\n",
    "                                                    stratify=strat_col2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b20889-919c-4024-bbf6-a229f467c195",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <strong>🏋🏻‍♂️ TEAM ACTIVITY:</strong> Classify Those Penguins!</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa872e-24e8-4fd5-99a8-41457a622e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.metrics import Accuracy, AUC\n",
    "from tensorflow.keras.activations import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58773fb7-b8cc-441c-afa8-5cc0ded35f63",
   "metadata": {},
   "source": [
    "**Build**\n",
    "\n",
    "Construct your NN penguin classifier. You can make use of your `build_NN` function from earlier. What output activation should you use?\n",
    "\n",
    "**Hint:** try to programaticlaly determin the input and output shape from your data rather than hard coding those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7e8164-cd40-475a-99a9-3679060a0f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct your NN and print the model summary\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4ff2b7-9f86-4432-a448-f52b5d5e4e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/sol2_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e0d5fa-af4e-4218-b9f6-e1b965657669",
   "metadata": {},
   "source": [
    "**Compile**\n",
    "\n",
    "Again, let's use `SGD` as our optimizer. You can fiddle with the `learning_rate`.\\\n",
    "What loss and metric(s) do you think are appropriate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81ec729-dae2-4a3e-948c-7f47d0638bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "# youre code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccbea53-4f33-4aee-8458-65e8afa596bc",
   "metadata": {},
   "source": [
    "**Fit**\n",
    "\n",
    "Fit your model and store the results in a variable called `history`.\\\n",
    "Feel free to play with `batch_size` and `epochs`.\n",
    "\n",
    "Don't forget to include the `validation_data`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f327e64-c82d-40ac-b52c-bf38d707aedc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e68e8a-ff6d-4774-acbd-83718b9e18ce",
   "metadata": {},
   "source": [
    "**Plot**\n",
    "\n",
    "Finally, write some code to visualize your loss and metric(s) across the training epochs. You should include both **train** and **validation** scores. This is where a **legend** is very important!\n",
    "\n",
    "**Note:** If you load the solutions they may not run for you unless you have selected the same metric(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2be39b-6296-48f3-af5e-35ca1214f5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47096a09-7947-441a-b492-83e504aba46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load solutions/sol2_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a7c00b-ba6c-4e35-8d7e-efffd28116e8",
   "metadata": {},
   "source": [
    "**End of Team Activity**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0d6820-5afc-4b1b-95b2-27f0b5c2e42b",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>Evaluating the Model</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7f850-c86f-4c77-9522-ac70efb23246",
   "metadata": {},
   "source": [
    "First, let's see how well we could to by simply predicting the majority class in the training data for all observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75e84aa-167e-4aa1-91be-5ad2cd4d8d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_acc = y_train.mean(axis=0).max()\n",
    "print('Naive Accuracy:', naive_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8d9850-cbf6-4ccd-b281-333b2e835e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dc82ee-4a6c-42b3-9048-a5203dae1cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1b4022-22b4-4e56-a3ae-e0579ac81e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a292793-7ac0-40af-8834-ff23229c49a5",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>Black Box Interpretation</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b73b44-a5ca-4091-8454-7c1540737d59",
   "metadata": {},
   "source": [
    "**Proxy Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c342da-f412-4f05-9462-ee2e6657173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd292be-2697-432d-b3f5-2aeea475c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train & test response variables for proxy model\n",
    "y_train_bb = model.predict(X_train).argmax(-1)\n",
    "y_test_bb = model.predict(X_test).argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f607ad96-ac43-46c1-b380-7b3a9f24102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross-validation to tune proxy's hyperparameters\n",
    "parameters = {'max_depth':range(1,10),\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "grid = GridSearchCV(clf, parameters, cv=5)\n",
    "\n",
    "# fit using same train but NN's predictions as response\n",
    "grid.fit(X_train, y_train_bb)\n",
    "print('Best Score:', grid.best_score_)\n",
    "print('Best Params:', grid.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258cb61a-cdb6-468a-ad6a-21408caa444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve best estimator from the grid object\n",
    "proxy = grid.best_estimator_\n",
    "\n",
    "bb_test_score =  sum(y_test_bb == y_test.argmax(-1))/len(y_test_bb)\n",
    "proxy_test_score = proxy.score(X_test, y_test_bb)\n",
    "print('Black Box Test Score:', bb_test_score)\n",
    "print('Proxy Model Test Score:', proxy_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dc272c-282c-43b8-b373-0978130d1d98",
   "metadata": {},
   "source": [
    "**Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5e1d3c-189f-4504-8c0e-20bfa725e84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = proxy.feature_importances_\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f1c81b-0aac-4de6-8c75-c23354a5af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_idx = np.argsort(feature_importances)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a33e422-8b9e-47a5-a018-1f7e86d3f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=feature_importances[sort_idx], y=X_cols[sort_idx], color='purple', orient='h')\n",
    "for index, val in enumerate(feature_importances[sort_idx]):\n",
    "    ax.text(val/3, index, round(val, 2),color='white', weight='bold', va='center')\n",
    "ax.set_title('NN Feature Importance According to DTree Proxy')\n",
    "sns.despine(right=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0802a3-141f-42dc-ad0f-b46ea1d87c68",
   "metadata": {},
   "source": [
    "**Fixing All But One Predictor**\n",
    "\n",
    "We can alo try to see how a predictor affects the NN's output by fixing all the other to some \"reasonable\" values (e.g., mean, mode) and then only varying the predictor of interest.\n",
    "\n",
    "Based on the results above, let's explore how `bill_length_mm` effects the NN's output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f5ae6d-8f3c-4ee7-9370-cf8426f23e6a",
   "metadata": {},
   "source": [
    "**Construct 'Average' Observation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbff7acb-9621-4153-8b9a-ca01e4527bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review data types\n",
    "X_design.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8851c92-3d32-4f64-9009-dc153481eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take means for continous\n",
    "means = X_scaled[:,:4].mean(axis=0)\n",
    "\n",
    "# And modes for catgoricals\n",
    "modes = pd.DataFrame(X_scaled[:,4:]).mode().values.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a522f9-0d7c-4bf6-a5c6-a52d4d1261aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape Sanity Check\n",
    "means.shape, modes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02e6ffc-70aa-408c-a2cf-3247d946fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate these two back together\n",
    "avg_obs = np.concatenate([means, modes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58720d6-eecc-4cf0-88e4-4dae18e737f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And stick it back in a DataFrame\n",
    "avg_obs = pd.DataFrame(avg_obs).transpose()\n",
    "avg_obs.columns = X_design.columns\n",
    "avg_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c25ce-50f4-459c-ba55-203a240342ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify column in our array that corresponds to bill length\n",
    "bill_col = np.argmax(X_design.columns == 'bill_length_mm')\n",
    "\n",
    "# Find the min and max bill length stdevs in the data set\n",
    "bill_min_std = np.min(X_scaled[:,bill_col])\n",
    "bill_max_std = np.max(X_scaled[:, bill_col])\n",
    "\n",
    "# Create 100 evenly spaced values within that range\n",
    "bill_lengths = np.linspace(bill_min_std, bill_max_std, 100)\n",
    "\n",
    "# Create 100 duplicates of the average observation\n",
    "avg_df  = pd.concat([avg_obs]*bill_lengths.size,ignore_index=True)\n",
    "\n",
    "# Set the bill length column to then linspace we just created\n",
    "avg_df['bill_length_mm'] = bill_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e658a4-930d-4d7e-83ec-60d339076be0",
   "metadata": {},
   "source": [
    "Notice now that all rows are identical except for `bill_length_mm` which slowly covers the entire range of values observed in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6722c597-fb21-4337-9097-decd2859c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b98e37-26fa-4e7e-80c9-a9a63e34e0b3",
   "metadata": {},
   "source": [
    "**Return Predictor to Original Scale**\n",
    "\n",
    "When we visualize our results we'll want to do so back in the original scale for better interpretability.\n",
    "\n",
    "Here we make use of our scaler object from way back when as it stores the means and standard deviations of the original, unscaled predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef249c-4589-47d2-8a5b-57a428b70ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover the feature of interest on the original scale\n",
    "bill_std = np.sqrt(scaler.var_[bill_col])\n",
    "bill_mean = scaler.mean_[bill_col]\n",
    "bill_lengths_original = (bill_std*bill_lengths)+bill_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600515ef-98dc-4fc0-9d66-cf9a1988bb27",
   "metadata": {},
   "source": [
    "We can sanity check out inverse transformation by confirming we recovereed the same min and max bill length from our very first DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b5e7b7-6c90-49fd-8195-d0661fc0366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min Sanity Check\n",
    "bill_lengths_original.min(), penguins.bill_length_mm.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c537236-565e-4201-8219-8355676dd3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Sanity Check\n",
    "bill_lengths_original.max(), penguins.bill_length_mm.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4ce62e-b188-4301-a74d-0f4307988831",
   "metadata": {},
   "source": [
    "Now we are ready to plot an approximation of how `bill_length_mm` affects the NN's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b5e314-3e1d-4d99-ae56-050d0ad20ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted class probabilities as a function of bill length (approx)\n",
    "avg_pred = model.predict(avg_df)\n",
    "fig, ax = plt.subplots()\n",
    "for idx, species in enumerate(labenc.classes_): \n",
    "    plt.plot(bill_lengths_original, avg_pred[:,idx], label=species)\n",
    "ax.set_ylabel('predicted probability')\n",
    "ax.set_xlabel('bill length in mm')\n",
    "ax.set_title('NN Predictions varying only bill length, holding all other predictors at mean/mode')\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dec89b-a1f8-420a-a48d-6749d2798259",
   "metadata": {},
   "source": [
    "If you know your penguins this should be too surprising. Gentoo penguins are the 3rd largest species after the emperor and king penguins (not represented in our dataset).\n",
    "\n",
    "**Q:** Why is this only an *approximation* of how `bill_length_mm` affects the NN's predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cb12d4-a20c-4e01-a963-f59fafa11e7a",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>Bagging</div>\n",
    "\n",
    "You'll be using bagging (\"bootstrap aggregating\") in your HW so let's take a minute to review the idea and see how it would work with a Kerass model.\n",
    "    \n",
    "The idea is to similuate multiple datasets by sampling our current one with replacement and fitting a model on this sample. The process is repeated multiple times until we have an *ensemble* of fitted models, all trained on slightly different datasets. \n",
    "    \n",
    "We can then treat the ensemble as a singled 'bagged' model. When it is time to predict, each model in the ensemble makes its own predictions. These predictions can then be *aggregated* across models, for example, by taking the average or through majority voting.\n",
    "    \n",
    "We may also be interested in looking at the distribution of the predictions for a given observation as this may help us quanity our uncertainty in a way in which we could not with a single model's predictions (even if that model outputs a probability!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d342e5-e665-4949-b49e-075bc40536ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sup parameters for the bagging process\n",
    "learning_rate=1e-1\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "n_boot = 30\n",
    "bagged_model = []\n",
    "np.random.seed(109)\n",
    "\n",
    "for n in range(n_boot):\n",
    "    # Bootstrap\n",
    "    boot_idx = np.random.choice(X_train.shape[0], size=X_train.shape[0], replace=True)\n",
    "    X_train_boot = X_train[boot_idx]\n",
    "    y_train_boot = y_train[boot_idx]\n",
    "\n",
    "    # Build\n",
    "    boot_model= build_NN(name=f'penguins_{n}',\n",
    "                        input_shape=(X_train_boot.shape[1],),\n",
    "                        hidden_dims=[8,16,32],\n",
    "                        hidden_act='relu',\n",
    "                        out_dim=3,\n",
    "                        out_act='softmax')\n",
    "    # Compile\n",
    "    boot_model.compile(optimizer=SGD(learning_rate=learning_rate),\n",
    "                       loss='categorical_crossentropy',\n",
    "                       metrics=['acc', 'AUC'])\n",
    "\n",
    "    # Fit\n",
    "    boot_model.fit(X_train_boot,\n",
    "                   y_train_boot,\n",
    "                   batch_size=batch_size,\n",
    "                   epochs=epochs,\n",
    "                   verbose=0)\n",
    "    \n",
    "    # Store bootstrapped model's probability predictions\n",
    "    bagged_model.append(boot_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b196d3db-4ef8-41e0-a243-78d987805623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice we can programatically recover the shape of a model's output layer\n",
    "m = bagged_model[0]\n",
    "out_dim = m.layers[-1].output_shape[-1]\n",
    "print(out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8b2711-a669-4db7-b046-c0dcfa7a1fff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bagged_pred(bagged_model, X): \n",
    "    # Number of observations\n",
    "    n_obs = X.shape[0]\n",
    "    # Prediction dimensions (here, number of classes)\n",
    "    pred_dim = bagged_model[0].layers[-1].output_shape[-1]\n",
    "    # Number of models in the bagged ensemble\n",
    "    n_models = len(bagged_model)\n",
    "    # 3D tensor to store predictions from each bootstrapped model\n",
    "    # n_observations x n_classes x n_models\n",
    "    boot_preds = np.zeros((n_obs, pred_dim, n_models))\n",
    "    # Store all predictions in the tensor\n",
    "    for i, model in enumerate(bagged_model):\n",
    "        boot_preds[:,:,i] = model.predict(X)\n",
    "    # Average the predictions across models\n",
    "    bag_pred = boot_preds.mean(axis=-1)\n",
    "    return bag_pred, boot_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c17bdf8-59c4-4dcd-9024-692eed1a273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get aggregated and unaggregated ensemble predictions\n",
    "bag_pred, boot_preds = get_bagged_pred(bagged_model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e34818e-b600-4e80-a168-ecc43981ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of aggregated predictions\n",
    "bag_pred[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791efa33-8042-4f23-a033-c15722d9144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of unaggregated ensemble predictions tensor\n",
    "boot_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc847195-fd1a-4972-ae37-1647ca7b8f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bagged accuracy\n",
    "bag_acc = sum(bag_pred.argmax(axis=-1) == y_test.argmax(axis=-1))/bag_pred.shape[0]\n",
    "print('Bagged Acc:', bag_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a72ea2-ddc3-419e-975f-1c774e388746",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <strong>🏋🏻‍♂️ Optional Take-home Challenges</strong></div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e839a837-7855-4a70-a234-3f9f0ca3a72e",
   "metadata": {},
   "source": [
    "**Bagged NN Custom Python Class**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b49c56a-29dd-4c2d-a0ae-99b43e7486ea",
   "metadata": {},
   "source": [
    "It would be nice if we could interact with our bagged model like any other keras model, passing Create a custom `Bagged_NN` class with its own `build`, `compile`, `fit`, `eval`, and `predict` methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44053d79-4dc8-4a75-aebb-59118c787f77",
   "metadata": {},
   "source": [
    "**Use Bootstraped Predictions To Quantify Uncertainty**\n",
    "\n",
    "In your HW you'll use bootstrapping to quantify uncertainty on predictions of a *binary* variable using Posterior Predictive Ratio (PPR). How might you do something similar with *categorical* bootstrapped predictions like we have here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c8fba-921d-4418-ba02-76cb6994c935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might something like entropy be useful?\n",
    "from scipy.stats import entropy\n",
    "entropy([0.25,0.25], base=2), entropy([0.8,0.2], base=2), entropy([1,0,0,0], base=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8257a827-48fb-45d6-b36a-62317d01dd5e",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>An Image Classification Example</div>\n",
    "\n",
    "The 2nd half of your HW asks you to classifying images. Let's try soemthing similar now using the famouse MNIST dataset of handwritten digits.\\\n",
    "\n",
    "We can load the dataset directly from Tensorflow/Keras! You can read more about TensorFlow's datasets [here](https://www.tensorflow.org/api_docs/python/tf/keras/datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01371c2-e5bb-450d-b0d5-4e1c95b142a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394c7c2d-c3c1-4b4c-8ed0-7d404cc794b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique response variable values\n",
    "set(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cde5d0-3bd1-4dbe-b0f9-71e3a5657a92",
   "metadata": {},
   "source": [
    "Each observation is an 28x28 pixel image.\\\n",
    "There are 60,000 training examples and 10,000 test images.\\\n",
    "The $y$ values corresponde to which of the digit the image represents, 0-9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539cbf0c-0bc4-48ab-8276-d88d2b6b5b70",
   "metadata": {},
   "source": [
    "This is how each image is represented numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48854ce3-a1d8-4c95-bf67-250d194c8898",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(edgeitems=30, linewidth=100000, formatter=dict(float=lambda x: \"%.3g\" % x))\n",
    "x_train[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f25af1-9002-47b7-8065-78b0255e0408",
   "metadata": {},
   "source": [
    "The values represent pixel intensity and range from 0-255.\\\n",
    "We can use `plt.imshow` or `ax.imshow` to display it as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22921b59-98b9-4768-bfb0-592b0bb94a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display and example observation as an image\n",
    "print('This picture belongs to the class for number', y_train[10])\n",
    "ax = plt.gca()\n",
    "ax.grid('off')\n",
    "ax.imshow(x_train[10], cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b02d75f-4d0a-4c53-a60e-3f2964359f47",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>(Just a Little) Preprocessing</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec773717-93f7-4cc9-bf0d-3d8930e90493",
   "metadata": {},
   "source": [
    "**Flattening**\n",
    "\n",
    "We don't know how to feed a 2D input into our neural networks (yet!). So we will simply flatted each image to a length 28x28 = 784 array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c14b17-c472-493f-9656-521c63cb65e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten image data\n",
    "x_train = x_train.reshape(x_train.shape[0], 784)\n",
    "x_test = x_test.reshape(x_test.shape[0], 784)\n",
    "\n",
    "# check if the shapes are ok\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fd4a96-fc05-428c-9849-0e63498d763f",
   "metadata": {},
   "source": [
    "**Normalizing**\n",
    "\n",
    "Let's confirm what we said about pixel values ranging from 0-255 and then normalize them to the range [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e773e660-ea7d-4226-a46e-b4a4fa96f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the min and max of x_train and x_test\n",
    "print(x_train.min(), x_train.max(), x_test.min(), x_test.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f40d6c2-e3dd-4a2b-a4d7-22e000edd7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "\n",
    "print(x_train.min(), x_train.max(), x_test.min(), x_test.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93b2a2-1903-48c7-8306-7deb86a94c33",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>Build & Compile</div>\n",
    "\n",
    "Here we use a little trick with the `'sparse_categorical_crossentropy'` loss. Basically, the saves us from having to turn our response variable into a categorical one! We can just leave them as integers. We'll also cheat a bit here and use the `Adam` optimizer. We'll learn more about this and other optimizers in the coming lectures and advanced section.\n",
    "    \n",
    "Notice too how a sequential Keras model can also be defined as a list passed to the `Sequential` constructor rather than by repeatedly using the `add` method. In future labs, we'll look at the *functional* Keras API, which is an alternative approach to sequential which is more flexible, allowing for more complex architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb87171-8a10-4e86-a8ff-09263ecf7381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9c3ac7-ee86-42f0-9659-a4e89fdf269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build MNIST model\n",
    "model_mnist = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Input(shape = (784,)),\n",
    "  tf.keras.layers.Dense(128,activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_mnist.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98caa4c8-7d09-41ed-81da-aa50c2cd2975",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>Fit</b></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12cf155-363e-477e-a1b6-c68f75fd7edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the MNIST model\n",
    "trained_mnist = model_mnist.fit(x_train,\n",
    "                                y_train, \n",
    "                                epochs=6,\n",
    "                                batch_size=128,\n",
    "                                validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9c448-126e-454d-b3f9-de01bee6c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for plotting training history\n",
    "def plot_accuracy_loss(model_history):\n",
    "    plt.figure(figsize=[12,4])\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.semilogx(model_history.history['accuracy'], label = 'train_acc', linewidth=4)\n",
    "    plt.semilogx(model_history.history['val_accuracy'], label = 'val_acc', linewidth=4, alpha=.7)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.loglog(model_history.history['loss'], label = 'train_loss', linewidth=4)\n",
    "    plt.loglog(model_history.history['val_loss'], label = 'val_loss', linewidth=4, alpha=.7)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15bec7a-f409-4c5d-9c6a-2b4f54e004f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MNIST training history\n",
    "plot_accuracy_loss(trained_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade359f-07fe-4977-a0d3-290c9f1b6b94",
   "metadata": {},
   "source": [
    "Not bad! But do see some overfitting as the validation accuracy starts to diverge from the training accuracy in later epochs. The same general trend can also be seen in the plot of the losses. In the next lecture we'll look at methods for dealing with overfitting in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37136162-906c-4ddf-ae3d-ff630aae7a86",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>Visually Inspecting Model Performance</b></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e24595b-547c-42c9-abe6-2d18f6f617eb",
   "metadata": {},
   "source": [
    "A great benefit of working with image date is that you can often (but not always) simply look at an observation to see if your model's prediction make sense or not. Let's try that now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be49990b-8c46-4d5e-a125-887573d78edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a single prediction and validate it\n",
    "def example_NN_prediction(dataset = x_test, \n",
    "                          model_ = model_mnist):\n",
    "    \"\"\"\n",
    "    This tests our MNist FFNN by examining a single prediction on the test set and \n",
    "    checking if it matches the real label.\n",
    "    \n",
    "    Arguments:\n",
    "        n: if you select n then you will choose the nth test set\n",
    "    \"\"\"\n",
    "    mnist_preds = model_mnist.predict(x_test)\n",
    "    all_predictions = np.argmax(mnist_preds, axis = 1)\n",
    "    \n",
    "    n = np.random.choice(784)\n",
    "    digit = x_test[n,:]\n",
    "    actual_label = y_test[n]\n",
    "    \n",
    "    plt.imshow(digit.reshape(-1, 28))\n",
    "    prediction_array = model_.predict(digit.reshape(1,-1))\n",
    "    prediction = np.argmax(prediction_array)\n",
    "    if prediction == y_test[n]:\n",
    "        print(\"The Mnist model correctly predicted:\", prediction)\n",
    "    else:\n",
    "        print(\"The true label was\", actual_label)\n",
    "        print(\"The Mnist model incorrectly predicted:\", prediction)\n",
    "\n",
    "####################################################        \n",
    "# Make a many predictions and validate them\n",
    "###################################################\n",
    "def example_NN_predictions(model_,\n",
    "                          dataset_ = x_test,\n",
    "                          response_ = y_test,\n",
    "                          get_incorrect = False):\n",
    "    \"\"\"\n",
    "    This tests our MNist FFNN by examining 3 images and checking if our nueral network\n",
    "    can correctly classify them.\n",
    "    \n",
    "    Arguments:\n",
    "        model_ : the mnist model you want to check predictions for.\n",
    "        get_incorrect (boolean): if True, the model will find 3 examples \n",
    "                        where the model made a mistake. Otherwise it just select randomly.\n",
    "    \"\"\"\n",
    "    dataset  = dataset_.copy()\n",
    "    response = response_.copy()\n",
    "    \n",
    "    # If get_incorrect is True, then get an example of incorrect predictions.\n",
    "    # Otherwise get random predictions.\n",
    "    if not get_incorrect:\n",
    "        n = np.random.choice(dataset.shape[0], size = 3)\n",
    "        digits = dataset[n,:]\n",
    "        actual_label = response[n]\n",
    "    else:\n",
    "        # Determine where the model is making mistakes:\n",
    "        mnist_preds = model_mnist.predict(dataset)\n",
    "        all_predictions = np.argmax(mnist_preds, axis = 1)\n",
    "        incorrect_index = all_predictions != response\n",
    "        incorrect = x_test[incorrect_index, :]\n",
    "        \n",
    "        # Randomly select a mistake to show:\n",
    "        n = np.random.choice(incorrect.shape[0], size = 3)\n",
    "        digits  = incorrect[n,:]\n",
    "        \n",
    "        # determine the correct label\n",
    "        labels = response[incorrect_index]\n",
    "        actual_label = labels[n]\n",
    "      \n",
    "    #get the predictions and make the plot:\n",
    "    fig, ax = plt.subplots(1,3, figsize = (12, 4))\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    for i in range(3):\n",
    "        #show the digit:\n",
    "        digit = digits[i,:]\n",
    "        ax[i].imshow(digit.reshape(28,-1)) #reshape the image to 28 by 28 for viewing\n",
    "        \n",
    "        # reshape the input correctly and get the prediction:\n",
    "        prediction_array = model_.predict(digit.reshape(1,-1))\n",
    "        prediction = np.argmax(prediction_array)\n",
    "        \n",
    "        #Properly label the prediction (correct vs incorrect):\n",
    "        if prediction == actual_label[i]:\n",
    "            ax[i].set_title(\"Correct Prediction: \" + str(prediction))\n",
    "        else:\n",
    "            ax[i].set_title('Incorrect Prediction: {} (True label: {})'.format(\n",
    "                prediction, actual_label[i]))\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1651b5b3-fba1-4a8a-8b97-945dbc1561ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a random prediction example\n",
    "example_NN_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b76aea7-d315-4d28-9149-d8662f66cfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct predictions\n",
    "example_NN_predictions(model_ = model_mnist, get_incorrect = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a11aa5-ebbb-4d02-95ef-7db67f061f2e",
   "metadata": {},
   "source": [
    "Let's see some examples where the network makes the wrong prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514bcadc-c0e7-446d-8bf8-f2a518d469ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorrect Predictions\n",
    "example_NN_predictions(model_ = model_mnist, get_incorrect = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b91da0-b9f2-45fb-bf45-5a462d7c1c7e",
   "metadata": {},
   "source": [
    "Oh my. That is some bad handwriting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60d862a-e84f-4fb5-a60a-369df13f57ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
