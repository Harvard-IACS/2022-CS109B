{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46da1694",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109B Introduction to Data Science\n",
    "\n",
    "## Lab 8: Recurrent Neural Networks and Introduction to Natural Language Processing\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Spring 2022**<br/>\n",
    "**Instructors**: Mark Glickman & Pavlos Protopapas<br/>\n",
    "**Lab Leaders**: Marios Mattheakis & Chris Gumb\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beebf68",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this Lab, you should understand how to:\n",
    "- use `keras` for constructing a simple RNN for time-series prediction\n",
    "- perform basic preprocessing on text data (stemming, tokenization, padding, one-hot encoding)\n",
    "- Feed Forward NNs for NLP tasks \n",
    "- add embedding layers to improve the performance \n",
    "- use `keras` simple RNNs for NLP \n",
    "- inspect the embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24fb978",
   "metadata": {},
   "source": [
    "<a id=\"contents\"></a>\n",
    "\n",
    "## Notebook Contents\n",
    "\n",
    "- [**Simple RNNs**](#rnn_intro)\n",
    "    - [Time-series prediction](#timeSeries)\n",
    "    - [Activity 1: Forecasting timeseries](#act1)\n",
    "- [**Introduction to NLP**](#NLP_intro)\n",
    "    - [Case Study: IMDB Review Dataset](#imdb)\n",
    "- [**Preprocessing Text Data**](#prep)\n",
    "    - [Tokenization](#token)\n",
    "    - [Stemming](#stem)\n",
    "    - [Padding](#pad)\n",
    "    - [Numerical Encoding](#encode)    \n",
    "- [**Neural Networks for NLP**](#NN)\n",
    "    - [Feed Forward Neural Networks](#FFNN)\n",
    "    - [Embedding layer](#embedding)    \n",
    "    - [Activity 2: Recurrent Neural Networks with embeddings](#act2)\n",
    "- [**Extra Material: Inspecting the embedding space**](#SM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29c871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, SimpleRNN, Flatten #GRU, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_datasets\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(109)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddebadfb",
   "metadata": {},
   "source": [
    "# Simple Recurrent Neural Networks (RNNs) <div id='rnn_intro'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2591303",
   "metadata": {},
   "source": [
    "An RNN is similar to a FFNN in that there is an input layer, a hidden layer, and an output layer. The input layer is fully connected to the hidden layer, and the hidden layer is fully connected to the output layer. However, the crux of what makes it a **recurrent** neural network is that the hidden layer for a given time _t_ is not only based on the input layer at time _t_ but also the hidden layer from time _t-1_.\n",
    "\n",
    "Here's a popular blog post on [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n",
    "\n",
    "\n",
    "In Keras, the vanilla RNN unit is implemented the`SimpleRNN` layer:\n",
    "```\n",
    "tf.keras.layers.SimpleRNN(\n",
    "    units, activation='tanh', use_bias=True,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    recurrent_initializer='orthogonal',\n",
    "    bias_initializer='zeros', kernel_regularizer=None,\n",
    "    recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "    kernel_constraint=None, recurrent_constraint=None, bias_constraint=None,\n",
    "    dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False,\n",
    "    go_backwards=False, stateful=False, unroll=False, **kwargs\n",
    ")\n",
    "```\n",
    "For more details check Keras' documention https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN.\n",
    "\n",
    "As you can see, recurrent layers in Keras take many arguments. We only need to be concerned with `units`, which specifies the size of the hidden state. \n",
    "\n",
    "**REMOVE**, and `return_sequences`, which will be discussed shortly. For the moment is it fine to leave this set to the default of `False`.\n",
    "\n",
    "As you will see next week simple RNNs have some serious problems and limitations, like the gradient vanishing/exploding issue.  Due to these limitations, simple RNN unit  tends not to be used much in practice. For this reason it seems that the Keras developers neglected to implement GPU acceleration for this layer! Later in the Lab, you will notice that training an RNN is slower the training an FFNN even when the RNN has fewer parameters. \n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765fe052",
   "metadata": {},
   "source": [
    "## Time-series prediction <div id = 'timeSeries'>\n",
    "    \n",
    "RNNs become effective in learning from sequential data like time series and text. Let's start this journey in RNNs by predicting a noisy time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a466b9",
   "metadata": {},
   "source": [
    "Generate some synthetic sequential noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85d640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N =  1000    \n",
    "Tp = 800    \n",
    "\n",
    "t=np.arange(0,N)\n",
    "\n",
    "x=np.sin(0.02*t)* 1*np.sin(0.05*t) + 2*np.exp(-(t-500)**2/1000)\n",
    "#Add gaussian (white) noise\n",
    "x += np.random.rand(N)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(x)\n",
    "\n",
    "\n",
    "plt.plot(t, x,'k')\n",
    "plt.xlabel('t'); plt.xlabel('x'); \n",
    "plt.xlabel('Time'); plt.ylabel('Series')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f11e889",
   "metadata": {},
   "source": [
    "#### Split data into training and testing sets\n",
    "Note, this is forecasting, so we do not know the future "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fdb890",
   "metadata": {},
   "outputs": [],
   "source": [
    "values=df.values\n",
    "train,test = values[0:Tp,:], values[Tp:N,:]\n",
    "\n",
    "plt.plot(df[0:Tp], 'b', label='training')\n",
    "plt.plot(df[Tp:N], 'g', label='testing')\n",
    "\n",
    "plt.axvline(df.index[Tp], c=\"r\")\n",
    "plt.xlabel('Time'); plt.ylabel('Series')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95887e7",
   "metadata": {},
   "source": [
    "#### Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa74527b",
   "metadata": {},
   "source": [
    "RNNs  require a step value that contains `n` number of elements as an input sequence. Here, we define it as a `step`. \n",
    "Let's understand this concept through  two simple cases. Cosidere the input `x` and the output `y`:\n",
    "- For step=1: \n",
    "   - x=[1,2,3,4,5]\n",
    "   - y=[2,3,4,5,6]\n",
    "- For step=2: \n",
    "   - x=[ (1,2), (2,3), (3,4) (4,5) ]\n",
    "   - y=[3,4,5,6]\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dc9ab2",
   "metadata": {},
   "source": [
    "The sizes of `x`  and `y` are  different. We can  fix this by adding step size into the training and test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54065d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape, test.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e4debe",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 4\n",
    "# add step elements into train and test\n",
    "test = np.append(test,np.repeat(test[-1,],step))\n",
    "train = np.append(train,np.repeat(train[-1,],step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b17f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape, test.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0bb576",
   "metadata": {},
   "source": [
    "Convert the datasets into the matrix with step value as it has shown above explation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToMatrix(data, step):\n",
    "    X, Y =[], []\n",
    "    for i in range(len(data)-step):\n",
    "        d=i+step  \n",
    "        X.append(data[i:d,])\n",
    "        Y.append(data[d,])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "trainX, trainY =convertToMatrix(train,step)\n",
    "testX,  testY =convertToMatrix(test,step)\n",
    "\n",
    "print('Shapes of the training dataset for (x,y): ', trainX.shape, trainY.shape)\n",
    "print('Shapes of the testing dataset for (x,y) : ', testX.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7928378",
   "metadata": {},
   "source": [
    "Finally, we reshape `trainX` and `testX` to fit with the Keras RNN model that  requires three-dimensional input data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea673f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "print(trainX.shape, testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0339319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Here, we add the RNN unit. Keras makes it easy for us \n",
    "model.add(SimpleRNN(units=32, input_shape=(1,step), activation=\"relu\"))\n",
    "#\n",
    "model.add(Dense(8, activation=\"relu\")) \n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer= 'adam' )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21988f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(trainX,trainY, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict  = model.predict(testX)\n",
    "\n",
    "# concate train and test predictions for plotting purposes\n",
    "predicted = np.concatenate((trainPredict,testPredict),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c63dbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainScore = model.evaluate(trainX, trainY, verbose=0)\n",
    "testScore = model.evaluate(testX, testY, verbose=0)\n",
    "print('Train score: ', trainScore)\n",
    "print('Test score: ', testScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad4adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = df.index.values\n",
    "plt.plot(df[0:Tp], 'b', label='training')\n",
    "plt.plot(df[Tp:N], 'g', label='testing')\n",
    "# plt.plot(index,predicted)\n",
    "plt.plot(predicted, 'm', label='network')\n",
    "plt.axvline(df.index[Tp], c=\"r\")\n",
    "plt.xlabel('Time'); plt.ylabel('Series')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcdfd9a",
   "metadata": {},
   "source": [
    "# Activity 1 <div id='act1'></div>\n",
    "- Repeat the above experiment for different steps in the range [1, 10, 100].\n",
    "- Does the step affect the performance? Make some comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83dc916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682e6833",
   "metadata": {},
   "source": [
    "# Introduction to NLP <div id = 'LP_intro'></div>\n",
    "    \n",
    "## Case Study: IMDB Review Classifier <div id='imdb'></div>\n",
    "<!-- <img src='fig/manyto1.png' width='300px'> -->\n",
    "\n",
    "Let's frame our introduction to NLP  around the example of a text classifier. Specifically, we'll build and evaluate various models that all attempt to descriminate between positive and negative reviews through the Internet Movie Database (IMDB). The dataset is again made available to us through the tensorflow datasets API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298df6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test), info = tensorflow_datasets.load('imdb_reviews', split=['train', 'test'], with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-course",
   "metadata": {},
   "source": [
    "The helpful `info` object provides details about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3bc7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-perfume",
   "metadata": {},
   "source": [
    "We see that the dataset consists of text reviews and binary good/bad labels. Here are two examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143ad23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0: 'bad', 1: 'good'}\n",
    "seen = {'bad': False, 'good': False}\n",
    "for review in train:\n",
    "    label = review['label'].numpy()\n",
    "    if not seen[labels[label]]:\n",
    "        print(f\"text:\\n{review['text'].numpy().decode()}\\n\")\n",
    "        print(f\"label: {labels[label]}\\n\")\n",
    "        seen[labels[label]] = True\n",
    "    if all(val == True for val in seen.values()):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad90142",
   "metadata": {},
   "source": [
    "# Preprocessing Text Data <div id='prep'></div>\n",
    "\n",
    "Computers have no built-in knowledge of language and cannot understand text data in any rich way that humans do -- at least not without some help! The first crucial step in natural language processing is to clean and preprocess your data so that your algorithms and models can make use of it.\n",
    "    \n",
    "We'll look at a few preprocess steps:\n",
    "- Tokenization\n",
    "- Stemming \n",
    "- Padding\n",
    "- Numerical encoding\n",
    "        \n",
    "Depending on your NLP task, you may (or may not) want to take additional preprocessing steps which we will not cover here. These can include:\n",
    "- converting all characters to lowercase\n",
    "- treating each punctuation mark as a token (e.g., , . ! ? are each separate tokens)\n",
    "- removing punctuation altogether\n",
    "- separating each sentence with a unique symbol (e.g., <S> and </S>)\n",
    "- removing words that are incredibly common (e.g., function words, (in)definite articles). These are referred to as 'stopwords').\n",
    "- Lemmatizing (replacing words with their 'dictionary entry form')\n",
    "    \n",
    "Useful NLP Python libraries such as [NLTK](https://www.nltk.org/) and [spaCy](https://spacy.io/) provide built in methods for many of these preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef97d1",
   "metadata": {},
   "source": [
    "<!-- <div class='exercise' id='token'><b>Tokenization</b></div></br> -->\n",
    "## Tokenization  <div id='token'></div>\n",
    "\n",
    "**Tokenization**   is the process of breaking a document down into words, punctuation marks, numeric digits, etc.\n",
    "\n",
    "**Tokens** are the atomic units of meaning which our model will be working with. What should these units be? These could be characters, words, or even sentences. For our movie review classifier we will be working at the word level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-secret",
   "metadata": {},
   "source": [
    "For this example we will process just a subset of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33069976",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 10 # # of the reviews to be considered\n",
    "subset = list(train.take(SAMPLE_SIZE))\n",
    "subset[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-greenhouse",
   "metadata": {},
   "source": [
    "The TFDS format process datasets into a standard format and therefore, allows for the construction of efficient preprocessing pipelines. But for our own preprocessing example we will be primarily working with Python `list` objects. This gives us a chance to practice the Python **list comprehension** which is a powerful tool to have at your disposal. It will serve you well when processing arbitrary text which may not already be in a nice TFDS format (such as in the HW 😉).\n",
    "\n",
    "We'll convert our data subset into X and y lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2094f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [x['text'].numpy().decode() for x in subset]\n",
    "y = [x['label'].numpy() for x in subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c9d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X has {len(X)} reviews')\n",
    "print(f'y has {len(y)} labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc65379",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CHARS = 20\n",
    "print(f'First {N_CHARS} characters of all reviews:\\n{[x[:20]+\"...\" for x in X]}\\n')\n",
    "print(f'All labels:\\n{y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-design",
   "metadata": {},
   "source": [
    "Each observation in `X` is a review. A review is a `str` object which we can think of as a sequence of characters. This is indeed how Python treats strings as made clear by how we are printing 'slices' of each review in the code cell above.<br>\n",
    "\n",
    "In this example, we will work  at a word level.  This means that our observations should be organized as **sequences of words** rather than sequences of characters. In general, we can prepare our data in different ways like at a character level.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d4fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list comprehensions again to the rescue!\n",
    "X_ = [x.split() for x in X]   # keep this temporal object for a comparison purpose, will see shortly\n",
    "X = [x.split() for x in X]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-detective",
   "metadata": {},
   "source": [
    "Now let's look at the first 10 **tokens** in the first 2 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c812c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Review 1: ', X[0][:10])\n",
    "print('Review 2: ', X[1][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-huntington",
   "metadata": {},
   "source": [
    "## Stemming <div id='stem'></div>\n",
    "**Stemming**  is the process of producing morphological variants of a root/base word. For example, a stemming algorithm reduces the words \"chocolates\", \"chocolatey\", \"choco\" to the root word, \"chocolate\" or the words \"likes\", \"liked\", \"likely\", \"liking\" to \"like\".\n",
    "\n",
    "Stemming is desirable as it may reduce redundancy as most of the time the word stem and their inflected/derived words mean the same.\n",
    "\n",
    "Here, we use the package **Natural Language Tool Kit (NLTK)** for more information check [here](https://www.nltk.org/api/nltk.stem.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32972fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# object for steamming \n",
    "ps = PorterStemmer()\n",
    "# perform stemming in all the sentences\n",
    "X = [[ps.stem(w) for w in x] for x in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0553f0",
   "metadata": {},
   "source": [
    "Inspect the above nested list comprehension: \n",
    "\n",
    "```\n",
    "i=0\n",
    "for words in X:\n",
    "    j=0\n",
    "    for w in words:\n",
    "        X[i][j]=ps.stem(w)\n",
    "        j+=1\n",
    "    i+=1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2168cd61",
   "metadata": {},
   "source": [
    "Let's compare the words before and after stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca33b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print(X_[0][i], ' --> ', X[0][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2831e0",
   "metadata": {},
   "source": [
    "<div  style=\"background-color:#b3e6ff\">\n",
    "<b>Q</b>: Should we always use stemming?\n",
    "</div>\n",
    "\n",
    "In classification tasks (like sentiment analysis) stemming is fine. But what about in a text generation task?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-hello",
   "metadata": {},
   "source": [
    "## Padding <div id='pad'></div>\n",
    "\n",
    "Let's take a look at the lengths of the reviews in our subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a82397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(x) for x in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-mexico",
   "metadata": {},
   "source": [
    "If we were training our RNN one sentence at a time, it would be okay to have sentences of varying lengths. However, as with any neural network, it can be sometimes be advantageous to train inputs in batches. When doing so with RNNs, our input tensors need to be of the same length/dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-venice",
   "metadata": {},
   "source": [
    "Here are two examples of tokenized reviews padded to have a length of 5.\n",
    "```\n",
    "['I', 'loved', 'it', '<PAD>', '<PAD>']\n",
    "['It', 'stinks', '<PAD>', '<PAD>', '<PAD>']\n",
    "```\n",
    "Now let's pad our own examples. Note that 'padding' in this context also means truncating sequences that are longer than our specified max length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3641cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 500\n",
    "PAD = '<PAD>'\n",
    "# truncate\n",
    "X = [x[:MAX_LEN] for x in X]\n",
    "# pad\n",
    "for x in X:\n",
    "    while len(x) < MAX_LEN:\n",
    "        x.append(PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3d44fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(x) for x in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-barcelona",
   "metadata": {},
   "source": [
    "Now all reviews have the same length!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-worse",
   "metadata": {},
   "source": [
    "## Numerical Encoding <div id='encode'></div>\n",
    "\n",
    "If each review in our dataset is an observation, then the features of each observation are the tokens, in this case, words. But these words are still **strings**. Our machine learning methods require us to be able to multiple our features by weights. If we want to use these words as inputs for a neural network we'll have to convert them into some **numerical representation**.\n",
    "\n",
    "One solution is to create a **one-to-one mapping** between unique words and integers.\n",
    "\n",
    "If the five sentences below were our entire corpus, our conversion would look this:\n",
    "\n",
    "1. i have books - [1, 4, 2]\n",
    "2. interesting books are useful [11,2,9,8]\n",
    "3. i have computers [1,4,3]\n",
    "4. computers are interesting and useful [3,5,11,10,8]\n",
    "5. books and computers are both valuable. [2,10,3,9,13,12]\n",
    "6. bye bye [7,7]\n",
    "\n",
    "I-1, books-2, computers-3, have-4, are-5, computers-6,bye-7, useful-8, are-9, and-10,interesting-11, valuable-12, both-13\n",
    "\n",
    "To accomplish this we'll first need to know what all the unique words are in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a1e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = [word for review in X for word in review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cd81d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "len(all_tokens), sum([len(x) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-information",
   "metadata": {},
   "source": [
    "Casting our `list` of words into a `set` is a great way to get all the *unique* words in the data. Hence, we build our **vocabulary**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0177ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(all_tokens))\n",
    "print('Unique Words in our vocabulary:', len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-preliminary",
   "metadata": {},
   "source": [
    "You can easily check that the vocabulary will be larger if stemming is not applied. Check it by yourself.\n",
    "\n",
    "Now we need to create a mapping from words to integers. For this we will perform a **dictionary comprehension**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed64a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word: idx for idx, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608f9a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-might",
   "metadata": {},
   "source": [
    "We repeat the process, this time mapping integers to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727e133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {idx: word for idx, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f31e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-irish",
   "metadata": {},
   "source": [
    "Now, perform the mapping to encode the observations in our subset. One more  ***nested list comprehensions***!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558e111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_proc = [[word2idx[word] for word in review] for review in X]\n",
    "X_proc[0][:10], X_proc[1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9254ad",
   "metadata": {},
   "source": [
    "\n",
    "# Neural Networks for NLP <div id='NN'></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-action",
   "metadata": {},
   "source": [
    "`X_proc` is a list of lists but if we are going to feed it into a `keras` model we should convert both it and `y` into `numpy` arrays.\n",
    "\n",
    "Just a reminder that `y` is the response variable: \n",
    "```\n",
    "X = [x['text'].numpy().decode() for x in subset]\n",
    "y = [x['label'].numpy() for x in subset]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_proc = np.hstack(X_proc).reshape(-1, MAX_LEN)\n",
    "y = np.array(y)\n",
    "print(X_proc.shape, y.shape)\n",
    "X_proc, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0628d9af",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network  <div id='NN'></div>\n",
    "\n",
    "Now, just to show that we've successfully processed the data, we perform a test train split and feed it into an FFNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f77091",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_proc, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59498ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(250, activation='relu',input_dim=MAX_LEN))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=2, verbose=2)\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-invalid",
   "metadata": {},
   "source": [
    "It worked! \n",
    "Is this a good performance? \n",
    "\n",
    "Well,  our subset is balanced and very small. So we shouldn't get excited about this results. \n",
    "Note that  adding more layers or neurons does not improve the performance, check it by your own! <br> \n",
    "\n",
    "### Load more clean data\n",
    "The IMDB dataset is very popular so `keras` also includes an alternative method for loading the data. This method can save us a lot of time for many reasons:\n",
    "- Cleaned text with less meaningless punctuation\n",
    "- Pre-tokenized and numerically encoded\n",
    "- Allows us to specify maximum vocabulary size\n",
    "- more ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9693eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# We want to have a finite vocabulary to make sure that our word matrices are not arbitrarily small\n",
    "MAX_VOCAB = 10000\n",
    "INDEX_FROM = 3   # word index offset \n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=MAX_VOCAB, index_from=INDEX_FROM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-aurora",
   "metadata": {},
   "source": [
    "`get_word_index` will load a json object we can store in a dictionary. This gives us the word-to-integer mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c40e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = imdb.get_word_index(path='imdb_word_index.json')\n",
    "word2idx = {k:(v + INDEX_FROM) for k,v in word2idx.items()}\n",
    "word2idx[\"<PAD>\"] = 0\n",
    "word2idx[\"<START>\"] = 1\n",
    "word2idx[\"<UNK>\"] = 2\n",
    "word2idx[\"<UNUSED>\"] = 3\n",
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebc9a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {v: k for k,v in word2idx.items()}\n",
    "idx2word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-plaintiff",
   "metadata": {},
   "source": [
    "We can see that the text data is already preprocessed for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a51974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of reviews', len(X_train))\n",
    "print('Length of first and fifth review before padding', len(X_train[0]) ,len(X_train[4]),'\\n')\n",
    "print('First review: ', X_train[0],'\\n')\n",
    "print('First label: ', y_train[0],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-fiber",
   "metadata": {},
   "source": [
    "Here is an example review using the index-to-word mapping we created from the loaded JSON file to view the a review in its original form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e86baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_review(x):\n",
    "    review = ' '.join([idx2word[idx] for idx in x])\n",
    "    print(review)\n",
    "\n",
    "show_review(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-episode",
   "metadata": {},
   "source": [
    "NOTE: This text is not comming with **padding** and **stemming**.\n",
    "\n",
    "Looking at the distribution of lengths will help us determine what a reasonable length to pad to will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255c8a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([len(x) for x in X_train])\n",
    "plt.title('review lengths');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-wound",
   "metadata": {},
   "source": [
    "We saw one way of doing this earlier, but Keras actually has a built in `pad_sequences` helper function. This handles both padding and truncating. By default padding is added to the *beginning* of a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-guide",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"  style=\"background-color:#b3e6ff\">\n",
    "<b>Q</b>: Why might we want to truncate? Why might we want to pad from the beginning of a sequence (sentence in this case)?\n",
    "</div>\n",
    "\n",
    "- Unless we truncate we need to pad every sentence according to the longest sentence. That will require too much padding providing a lot of useless information and long vectors which might be computationally costly. \n",
    "- Padding in the beginning of a sentence, retain the most important information in the end of sequence that sometime ehnances the performance since it keeps the 'short' memory more informative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4035486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a30c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 500\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_LEN, padding='pre') #padding='post' will pad in the end of a sequence\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_LEN, padding='pre')\n",
    "print('Length of first and fifth review after padding', len(X_train[0]) ,len(X_train[4]))\n",
    "print(\"Note that earlier the lenghts were 218 and 147.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41f1136",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((X_train.shape))\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-advice",
   "metadata": {},
   "source": [
    "## Model 1: Naive Feed-Forward Network <div id='FFNN'></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc00ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(name='Naive_FFNN')\n",
    "model.add(Dense(250, activation='relu',input_dim=MAX_LEN))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128, verbose=2)\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-doctor",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"  style=\"background-color:#b3e6ff\">\n",
    "<b>Q</b>: Why was the performance so poor? How could we improve our encoding?\n",
    "    \n",
    "<b>A</b>: The 'magic' Embedding Layer\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-ownership",
   "metadata": {},
   "source": [
    "## Model 2: Feed-Forward Network with Embeddings <div id='embedding'></div>\n",
    "<img src='wordembedding2.png' width=450px>\n",
    "\n",
    "    \n",
    "    \n",
    "Embedding process is  a linear projection from one vector space to another. For NLP, we usually use embeddings to project the **sparse one-hot encodings** of words on to **a more compact lower-dimensional** continuous space.\n",
    "We can view this embedding layer process as  a transformation from $\\mathbb{R}^\\text{inp} \\rightarrow$ $\\mathbb{R}^\\text{emb}$\n",
    "\n",
    "This **not only reduces dimensionality** but also **allows semantic similarities** between tokens to be captured by 'similiarities' between the embedding vectors. This was not possible with one-hot encoding as all vectors there were orthogonal to one another. \n",
    "\n",
    "<img src='wordembedding.png' width=450px>\n",
    "\n",
    "It is also possible to load pretrained embeddings that were learned from giant corpora. This would be an instance of transfer learning.\n",
    "\n",
    "If you are interested in learning more, start with the astromonically impactful papers of [word2vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) and [GloVe](https://www.aclweb.org/anthology/D14-1162.pdf).\n",
    "\n",
    "Next **Advanced Section** will focus on  *word2vec*. \n",
    "\n",
    "In Keras we use the [`Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer:\n",
    "```\n",
    "tf.keras.layers.Embedding(\n",
    "    input_dim, output_dim, embeddings_initializer='uniform',\n",
    "    embeddings_regularizer=None, activity_regularizer=None,\n",
    "    embeddings_constraint=None, mask_zero=False, input_length=None, **kwargs\n",
    ")\n",
    "```\n",
    "We'll need to specify the `input_dim` and `output_dim`. Since we are working with sequences we  also need to set the `input_length`.\n",
    "\n",
    "Let's implement this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb29ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ac0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 2\n",
    "\n",
    "model.reset_states()\n",
    "\n",
    "model = Sequential(name='embedding_FFNN')\n",
    "## EMBEDDING AND FLATTEN LAYERs  \n",
    "model.add(Embedding(MAX_VOCAB, EMBED_DIM, input_length=MAX_LEN))\n",
    "model.add(Flatten())\n",
    "#-\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128, verbose=2)\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4529ac",
   "metadata": {},
   "source": [
    "WoW! Notice the huge improvement in the performance. Embedding layer really helps! \n",
    "\n",
    "NOTE: We need a flatten layer to correct the dimensions. The embedding layer returns a matrix where each column corresponds to a word encoding. However, the next `Dense` layer is expecting a vector instead of a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994e1642",
   "metadata": {},
   "source": [
    "# Activity 2: RNN with embedding for NLP <div id='act2'></div>\n",
    "\n",
    "<img src='simplernn.png' width=300px>\n",
    "\n",
    "\n",
    "- Construct a network architecture with: \n",
    "    - an embedding layer\n",
    "    - SimpleRNN unit of 250 neurons\n",
    "    - Dense layer \n",
    "- Train this network on the data used in the previous example, namely `X_train`, `y_train`\n",
    "    - Train for 3 epochs and for a batch_size=128. It is slow because it is not run on GPUs.\n",
    "- Accordingly, evaluate on `X_test`, `y_test` datasets\n",
    "- Report the accuracy score on the testing set\n",
    "- Can you see any improvement comparing to the FFNN model? Make some comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c836d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91845687",
   "metadata": {},
   "source": [
    "Notice that we do not get any improvement comparing to FFNNs. What is going on here??? Why does FFNN perform better that RNNs? \n",
    "\n",
    "It is because this task is extremely easy and the network does not  really need memory to make a good prediction. Just some key words appearing in the text like \"terrible\" or \"amazing\" can determine the prediction. \n",
    "\n",
    "In more challenging tasks, like mult-categorical classification and text generation,  memory is crucial and  recurrency is a way to make it. \n",
    "\n",
    "\n",
    "Next week you will see some more efficient RNN architectures like **LSTM** and **GRU**. These are much more efficient RNNs and can also be implemented on GPUs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4b4ece",
   "metadata": {},
   "source": [
    "# Extra Material <div id='SM'></div>\n",
    "\n",
    "\n",
    "## Inpsecting the embedding space\n",
    "\n",
    "Let's train again the FFNN with the embeddings layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698dae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 2\n",
    "\n",
    "model.reset_states()\n",
    "\n",
    "model = Sequential(name='embedding_FFNN')\n",
    "model.add(Embedding(MAX_VOCAB, EMBED_DIM, input_length=MAX_LEN))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128, verbose=0)\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f002a2ac",
   "metadata": {},
   "source": [
    "#### Get access to the embeddings or embedding space or latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6c952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7fd868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with a Sequential model\n",
    "get_embed_out = backend.function(\n",
    "    [model.layers[0].input],\n",
    "    [model.layers[1].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee64dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output = get_embed_out([X_test[0]])\n",
    "# layer_output = get_embed_out([X_train[0]])\n",
    "\n",
    "print(type(layer_output), len(layer_output), layer_output[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662eee59",
   "metadata": {},
   "source": [
    "#### Create a list of some representative words and check where they live in the embedding space. \n",
    "Can you see any meaningful patern? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196caf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = layer_output[0]\n",
    "plt.plot(words[:,0], words[:,1],'bo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07e3ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = ['great',   'pleasure', 'good', 'awesome',\n",
    "          'movie', 'and', 'was' ,\n",
    "          'bad', 'boring' , 'crap']\n",
    "\n",
    "enc_review = tf.constant([word2idx[word] for word in review])\n",
    "enc_review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f2e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words = get_embed_out([enc_review])[0]\n",
    "\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.plot(words[:,0], words[:,1], 'ob')\n",
    "for i, txt in enumerate(review):\n",
    "    plt.annotate(txt, (words[i,0], words[i,1]),  size=18)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (default - all except pytorch)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
