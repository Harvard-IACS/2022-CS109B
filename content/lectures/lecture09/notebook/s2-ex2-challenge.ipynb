{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title :\n",
    "Backpropagation by hand\n",
    "\n",
    "## Description :\n",
    "The aim of this exercise is to perform back-propagation to update the weights of a simple neural network shown below. \n",
    "\n",
    "<img src=\"../fig/fig1.png\" style=\"width: 500px;\">\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "- Get the predictor and response variables from the file `backprop.csv` and assign them to variables `x` and `y`.\n",
    "- Build a forward pass of the above neural network with one hidden layer. You will build this neural network using **numpy** (no deep learning package allowed).\n",
    "- Initialize the weights randomly with the random seed as 310, and make a prediction.\n",
    "- Plot your neural net predictions with the true value.\n",
    "\n",
    "<img src=\"../fig/fig2.png\" style=\"width: 500px;\">\n",
    "\n",
    "- Compute the `mean_squared_error` of your predictions with the actual values.\n",
    "- Find the derivative of the loss function with respect to $w_1$.\n",
    "- Find the derivative of the loss function with respect to $w_2$.\n",
    "- Use the derivatives to update $w_1$ and $w_2$.\n",
    "- Use the updated weights to make a forward pass and compute new predictions.\n",
    "- Plot the new predictions with the actual data. This will look similar to the one given above.\n",
    "- Calculate your `MSE` and compare with the earlier value.\n",
    "\n",
    "## Hints: \n",
    "\n",
    "- Loss function: \n",
    "\n",
    "$$L\\ =\\ \\frac{1}{n}\\sum_1^n\\left(y_{pred}-y_{true}\\right)^2$$\n",
    "\n",
    "- Activation function: \n",
    "\n",
    "$$f\\left(x\\right)=\\sin x$$\n",
    "\n",
    "<a href=\"https://matplotlib.org/3.3.1/api/_as_gen/matplotlib.pyplot.scatter.html\" target=\"_blank\">ax.plot()</a>\n",
    "A scatter plot of y vs. x with varying marker size and/or colour.\n",
    "\n",
    "<a href=\"https://numpy.org/doc/stable/reference/generated/numpy.exp.html\" target=\"_blank\">np.exp()</a>\n",
    "Calculates the exponential of all elements in the input array.\n",
    "\n",
    "<a href=\"https://numpy.org/doc/stable/reference/generated/numpy.dot.html\" target=\"_blank\">np.dot()</a>\n",
    "Computes the dot product of the vectors.\n",
    "\n",
    "<a href=\"https://matplotlib.org/3.2.2/api/_as_gen/matplotlib.pyplot.xlabel.html\" target=\"_blank\">plt.xlabel()</a>\n",
    "This is used to specify the text to be displayed as the label for the x-axis.\n",
    "\n",
    "<a href=\"https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.ylabel.html\" target=\"_blank\">plt.ylabel()</a>\n",
    "This is used to specify the text to be displayed as the label for the y-axis.\n",
    "\n",
    "**Note: This exercise is auto-graded and you can try multiple attempts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data from the file `backprop.csv`\n",
    "df = pd.read_csv('backprop.csv')\n",
    "\n",
    "# Get the predictor and response variables\n",
    "x = df.x.values.reshape(-1,1)\n",
    "y = df.y.values.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights, but keep the random seed as 310 for reproducible results\n",
    "np.random.seed(310)\n",
    "\n",
    "# W is a list that contains both w1 and w2\n",
    "# W = [w1,w2]\n",
    "W = np.array([np.random.randn(1, 1), np.random.randn(1, 1)]).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⏸ What is the dimension of W?\n",
    "\n",
    "#### A. 2x1\n",
    "#### B. 1x2\n",
    "#### C. 3x3\n",
    "#### D. 3x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow1) ###\n",
    "# Submit an answer choice as a string below \n",
    "# (eg. if you choose option C, put 'C')\n",
    "answer1 = '___'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to define the neural network\n",
    "def neural_network(W, x):\n",
    "\n",
    "    # W is a list of the two weights (w1,w2) of your neural network\n",
    "    # x is the input to the neural network\n",
    "    w1 = W[0].reshape(-1,1)\n",
    "    w2 = W[1].reshape(-1,1)\n",
    "\n",
    "    '''\n",
    "    Compute h1, h2 and y\n",
    "    h1 is the matrix product of the input and w1 using np.dot()\n",
    "    To compute h2, first use the activation function on h1, then multiply by w2\n",
    "    Finally, use the activation function on h2 to compute y\n",
    "    Return all three values which you will use to compute derivatives later\n",
    "    '''\n",
    "\n",
    "    # Remember to keep track of the order in which the dot product \n",
    "    # is computed\n",
    "    h1 = ___\n",
    "    h2 = ___\n",
    "    y = ___\n",
    "    \n",
    "    # Remember that we return all activations and the output,\n",
    "    return h1,h2,y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True function vs neural network predictions\n",
    "Here we plot our neural network predictions (with random initializations) along with the true function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(8,6))\n",
    "\n",
    "# Plot the true x and y values\n",
    "ax.plot(x,y,label = 'True function',color='darkblue',linewidth=2)\n",
    "\n",
    "# Plot the x values with the network predictions\n",
    "h1,h2,y_pred = neural_network(W,x)\n",
    "ax.plot(x,y_pred,label = 'Neural net predictions',color='#9FC131FF',linewidth=2)\n",
    "\n",
    "# Set the x and y labels\n",
    "ax.set_xlabel('$x$',fontsize=14)\n",
    "ax.set_ylabel('$y$',fontsize=14)\n",
    "ax.legend(fontsize=14);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_nn_mse) ###\n",
    "\n",
    "# You can use the mean_squared_error function to find the MSE \n",
    "# of your predictions with true function values\n",
    "h1,h2, y_pred = ___\n",
    "mse = ___\n",
    "print(f'The MSE of the neural network predictions wrt true function is {mse:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⏸ What is $\\frac{\\partial L}{\\partial w_2}$ from the options below: \n",
    "\n",
    "#### A. $\\frac{1}{n}\\sum_i^n2\\left(y_{pred}\\ -y\\right)\\cos\\left(h_2\\right)\\sin\\left(h_1\\right)$\n",
    "#### B. $\\frac{1}{n}\\sum_i^n2\\left(y-y_{pred}\\ \\right)\\cos\\left(a_2\\right)\\sin\\left(a_1\\right)$\n",
    "#### C. $\\frac{1}{n}\\sum_i^n2\\left(y-y_{pred}\\ \\right)\\sin\\left(h_2\\right)\\sin\\left(h_1\\right)$\n",
    "#### D. $\\frac{1}{n}\\sum_i^n2\\left(y_{pred}\\ -y\\right)\\cos\\left(a_2\\right)\\sin\\left(a_1\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow2) ###\n",
    "# Submit an answer choice as a string below \n",
    "# (eg. if you choose option C, put 'C')\n",
    "answer2 = '___'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will update the weights only once\n",
    "\n",
    "# Get the predicted response, and the two affine transformations of \n",
    "# the network defined above\n",
    "h1,h2,y_pred = ___\n",
    "\n",
    "# Compute the gradient of the loss function with respect to weight 2\n",
    "dldw2 = ___\n",
    "\n",
    "# Now compute the gradient of the loss function with respect to weight 1\n",
    "dldw1 = ___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the update step, make sure to update the weights with their gradients\n",
    "# Here we take a learning rate of 1\n",
    "lr = 1\n",
    "W[0] = W[0] - lr*dldw1\n",
    "W[1] = W[1] - lr*dldw2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(8,6))\n",
    "\n",
    "# Plot the true x and y values\n",
    "ax.plot(x,y,label = 'True function',color='darkblue',linewidth=2)\n",
    "\n",
    "# Plot the x values with the network predictions\n",
    "h1,h2,y_pred = neural_network(W,x)\n",
    "ax.plot(x,y_pred,label = 'Neural net predictions',color='#9FC131FF',linewidth=2)\n",
    "\n",
    "# Set the x and y labels\n",
    "ax.set_xlabel('$x$',fontsize=14)\n",
    "ax.set_ylabel('$y$',fontsize=14)\n",
    "ax.legend(fontsize=14);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_one_update_mse) ###\n",
    "\n",
    "# Compute the new MSE after one update and print it\n",
    "h1,h2,y_pred = neural_network(W,x)\n",
    "\n",
    "# Calculate the mse using the new predicted y values \n",
    "mse_update = ___\n",
    "print(f'The MSE of the new neural network predictions with true function is {mse_update:.2f} as compared to {mse:.2f} from before ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Several updates\n",
    "\n",
    "In principle, only a single update will never be sufficient to improve model predictions.\n",
    "In the below segment, use the method from above, and update the weight 300 times before plotting predictions.\n",
    "\n",
    "Does your MSE decrease?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize the weights to start again \n",
    "np.random.seed(310)\n",
    "W = [np.random.randn(1, 1), np.random.randn(1, 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlike the previous step, this time we will set a learning rate of \n",
    "# 0.01 to avoid drastic updates and run the above code for 300 loops\n",
    "\n",
    "lmb = 0.01\n",
    "for i in range(300):\n",
    "    h1,h2,y_pred = neural_network(W,x)\n",
    "\n",
    "    # Remember to use np.mean\n",
    "    dldw2 = ___\n",
    "    dldw1 = ___\n",
    "    \n",
    "    W[0] = W[0] - lmb * dldw1\n",
    "    W[1] = W[1] - lmb * dldw2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(8,6))\n",
    "\n",
    "# Plot the true x and y values\n",
    "ax.plot(x,y,label = 'True function',color='darkblue',linewidth=2)\n",
    "\n",
    "# Plot the x values with the network predictions\n",
    "h1,h2,y_pred = neural_network(W,x)\n",
    "ax.plot(x,y_pred,label = 'Neural net predictions',color='#9FC131FF',linewidth=2)\n",
    "\n",
    "# Set the x and y labels\n",
    "ax.set_xlabel('$x$',fontsize=14)\n",
    "ax.set_ylabel('$y$',fontsize=14)\n",
    "ax.legend(fontsize=14);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_mse) ###\n",
    "\n",
    "# We again compute the MSE and compare it with the original predictions\n",
    "h1,h2,y_pred = neural_network(W,x)\n",
    "mse_final = mean_squared_error(y,y_pred)\n",
    "\n",
    "print(f'The final MSE is {mse_final:.2f} as compared to {mse:.2f} from before ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mindchow 🍲\n",
    "\n",
    "If you notice, your predicted values are off by approximately 0.5, from the actual values.\n",
    "After marking, go back to your neural network and add a bias correction to your predictions of 0.5.\n",
    "i.e `y = np.sin(h2) + 0.5` and rerun your code.\n",
    "\n",
    "Does your code fit better? And does your $MSE$ reduce?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow3) ###\n",
    "# Type your answer within in the quotes given\n",
    "answer3 = '___'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
