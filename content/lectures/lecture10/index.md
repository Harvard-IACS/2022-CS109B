Title: Lecture 10: Optimizers
Category: lectures
Date: 2022-02-28
Author: Pavlos Protopapas 
Slug: lecture10
Tags: Optimizers, Adam, SGD, Momentum, RMSProp Adagrad, Gradient Clipping, Learning Rate 

## Slides
- [Lecture 10 : Optimizers (PDF)]({attach}presentation/Optimizers.pdf)
- [Lecture 10 : Bias Correction (PDF)]({attach}presentation/BiasCorrections.pdf)

## Exercises
- [Lecture 10: Exercise: Gradient Clipping [Notebook]]({filename}notebook/decay_clipping_scaffold.ipynb)
- [Lecture 10: Exercise: RMSProp vs Learning Rate Decay [Notebook]]({filename}notebook/adaptivelr-challenge.ipynb)
