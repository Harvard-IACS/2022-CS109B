{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title :\n",
    "Gradient Clipping\n",
    "\n",
    "## Description :\n",
    "The aim of this exercise is to understand gradient clipping. \n",
    "\n",
    "<img src=\"../fig/fig1.png\" style=\"width: 500px;\">\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "- Generate predictor and response data using the function `f`.\n",
    "- Visualise the generated data using the helper code. The graph will look similar to the one given above.\n",
    "- Implement the `clip` function to return the gradient based on the `clip_threshold`. Consider the image shown below for implementation.\n",
    "- Set `learning_rate` as mentioned in the scaffold.\n",
    "- In the gradient descent algorithm incorporate gradient clipping. Follow the step by step instructions given in the scaffold.\n",
    "\n",
    "<img src=\"../fig/fig2.png\" style=\"width: 500px;\">\n",
    "\n",
    "## Hints: \n",
    "\n",
    "If $\\text { if }|g|>u$ then $g \\leftarrow \\frac{g u}{|g|}$\n",
    "\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical\" target=\"_blank\">numpy.abs(x)</a>\n",
    "Returns the absolute value of x\n",
    "\n",
    "<a href=\"https://numpy.org/doc/stable/reference/generated/numpy.linspace.html\" target=\"_blank\">np.linspace()</a>\n",
    "Return evenly spaced numbers over a specified interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to compute response given predictor\n",
    "def f(x):\n",
    "    return np.cos(3*np.pi*x)/x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to compute the derivative\n",
    "def der_f(x):\n",
    "    return -(3*np.pi*x*np.sin(3*np.pi*x)+np.cos(3*np.pi*x))/x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to get tangent points\n",
    "def get_tangent_line(x, x_range=.5):\n",
    "    y = f(x)\n",
    "    m = der_f(x)\n",
    "    x1, y1 = x, y\n",
    "    x = np.linspace(x1-x_range/2, x1+x_range/2, 50)\n",
    "    y = m*(x-x1)+y1\n",
    "    return x, y, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to plot the data\n",
    "def plot_it(cur_x, title='', ax=plt):\n",
    "    y = f(x)\n",
    "    ax.plot(x,y)\n",
    "    ax.scatter(cur_x, f(cur_x), c='r', s=80, alpha=1);\n",
    "    x_tan, y_tan, der = get_tangent_line(cur_x)\n",
    "    ax.plot(x_tan, y_tan, ls='--', c='r')\n",
    "    # Indicate when if our location is outside the x range\n",
    "    if cur_x > x.max():\n",
    "        ax.axvline(x.max(), c='r', lw=3)\n",
    "        ax.arrow(x.max()/1.6, y.max()/2, x.max()/5, 0, color='r', head_width=.25)\n",
    "    if cur_x < x.min():\n",
    "        ax.axvline(x.min(), c='r', lw=3)\n",
    "        ax.arrow(x.max()/2.5, y.max()/2, -x.max()/5, 0, color='r', head_width=.25)\n",
    "    ax.set_xlim(x.min(), x.max())\n",
    "    ax.set_ylim(-3.5, 3.5)\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 200 predictor data points between 0.07 and 3 using np.linspace\n",
    "x = ___\n",
    "\n",
    "# Get the response data from the predictor data by calling the function f \n",
    "y = ___\n",
    "\n",
    "# Helper code to plot the generated data\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "plt.plot(x,y, linewidth=3, color='#F5B7B1')\n",
    "plt.xlim(x.min(), x.max());\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('$x$', fontsize=16)\n",
    "plt.ylabel('$y$', fontsize=16)\n",
    "plt.grid(alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### edTest(test_clipping) ###\n",
    "\n",
    "# Function to perform clipping\n",
    "# This function should return the gradient with a magnitude<=clip_threshold\n",
    "def clip(g, use_clip=0, clip_threshold=8):\n",
    "    \n",
    "    # Compare the absolute value of the gradient with the clip_threshold\n",
    "    if ___ and use_clip==1:\n",
    "        \n",
    "        # Compute the gradient based on the equation given\n",
    "        ___\n",
    "        \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform gradient descent with clipping and plot it\n",
    "def gradient_descent(cur_x, learning_rate, epsilon, num_iter, clip_threshold, use_clip=0):\n",
    "    \n",
    "    # Plotting one panel per gradient descent iteration\n",
    "    fig, axs = plt.subplots(num_iter//3, 3, figsize=(15,6), sharey=True)\n",
    "\n",
    "    # To create sub-panels\n",
    "    for i, ax in enumerate(axs.ravel()):\n",
    "        plot_it(cur_x, title=f\"{i} step{'' if i == 1 else 's'}\", ax=ax)\n",
    "\n",
    "        # Store the current x value before change in a separate variable\n",
    "        prev_x = ___\n",
    "\n",
    "        # Get the derivative of cur_x using the function der_f\n",
    "        der_cur_x = ___\n",
    "\n",
    "        # Clip the gradient of the derivative of x by calling the \n",
    "        # clip function with use_clip and the clip_threshold as in \n",
    "        # the function definition \n",
    "        g = ___\n",
    "\n",
    "        # Update the x-value using the learning rate\n",
    "        cur_x = ___\n",
    "\n",
    "        # Stop algorithm if magnitude of change goes below epsilon\n",
    "        if np.abs(cur_x - prev_x) <= epsilon: \n",
    "\n",
    "            # Hide unused subplots on convergence\n",
    "            for ax in axs.ravel()[i+1:]:\n",
    "                ax.axis('off') \n",
    "            break\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if i == len(axs.ravel())-1:\n",
    "        print('Did not converge!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⏸ Set the intial x value to 0.07, learning rate to 0.01, and epsilon to 0.025. With use_clip value as 0 perform gradient descent. What do you observe and why?\n",
    "\n",
    "#### A. There is no convergence visible because of high learning rate.\n",
    "#### B. There are 2 convergence points due to small decay rate.\n",
    "#### C. The first gradient is very large and hence takes us away from the region of interest.\n",
    "#### D. There is no convergence visible because the epsilon is extremely low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow1) ###\n",
    "# Submit an answer choice as a string below (eg. if you choose option A, put 'A')\n",
    "answer1 = '___'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⏸ Now set the `use_clip` to 1 and set a reasonable threshold, what do you observe?\n",
    "\n",
    "#### A. It still jumps away from the area of interest\n",
    "#### B. Solution stays in the area of interest and converges\n",
    "#### C. The solution doesn't not change\n",
    "#### D. The solution doesn't jump away from the area of interest but it does not converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow2) ###\n",
    "# Submit an answer choice as a string below (eg. if you choose option A, put 'A')\n",
    "answer2 = '___'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⏸ What would you have to do to make the previous solution converge?\n",
    "\n",
    "#### A. Increase the clipping threshold.\n",
    "#### B. Reduce the learning rate.\n",
    "#### C. Increase epsilon i.e. the convergence criteria.\n",
    "#### D. B or C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow3) ###\n",
    "# Submit an answer choice as a string below (eg. if you choose option A, put 'A')\n",
    "answer3 = '___'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mindchow 🍲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After marking your exercise, try tweaking learning_rate, and perhaps the default value of clip_threshold. Can you anticipate how it will affect your results? \n",
    "\n",
    "See if you can find a combination that will converge to the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow4) ###\n",
    "# Type your answer within in the quotes given\n",
    "answer4 = '___'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
