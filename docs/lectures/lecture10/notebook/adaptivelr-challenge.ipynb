{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title :\n",
    "RMSProp vs Learning Rate Decay\n",
    "\n",
    "## Description :\n",
    "The aim of this exercise is to visualize the learning rate variance for various scheduling strategies such as learning rate decay, AdaGrad and RMS Prop.\n",
    "\n",
    "On completing the exercise, you should be able to see graphs similar to the one given below.\n",
    "\n",
    "<img src=\"../fig/fig1.png\" style=\"width: 500px;\">\n",
    "\n",
    "NOTE - This graph is only a sample. The transition, start and end changes with different weight initialisation.\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "- Begin by generating the 500 data points between -5 and 5 and then compute the corresponding values using the `get_response_data` function.\n",
    "- Visualize the generated data using the helper code. \n",
    "- Make a choice for the learning rate, decay rate and starting point for the weight.\n",
    "- Use the helper function to visualize how the descent works to get graphs similar to the one given above.\n",
    "\n",
    "## Hints: \n",
    "\n",
    "Learning rate decay:\n",
    "\n",
    "$$\\eta=\\eta\\left(1-\\ \\phi\\right)$$\n",
    "\n",
    "Where $\\phi$ is the learning rate decay parameter.  \n",
    "\n",
    "<a href=\"https://numpy.org/doc/stable/reference/generated/numpy.sqrt.html\" target=\"_blank\">np.sqrt()</a>\n",
    "Return the non-negative square-root of an array, element-wise.\n",
    "\n",
    "<a href=\"https://numpy.org/doc/stable/reference/generated/numpy.linspace.html\" target=\"_blank\">np.linspace()</a>\n",
    "Return evenly spaced numbers over a specified interval.\n",
    "\n",
    "<a href=\"https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html\" target=\"_blank\">np.random.uniform()</a>\n",
    "Draws samples from a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessay libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helper import get_response_data, derivative, lr_decay, rms_prop, adagrad, comparison_plot, landscapes_plot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data for performing the gradient descent with Adam\n",
    "\n",
    "# Get the 500 predictor data points from -5 to 5\n",
    "x = np.linspace(-5,5,500)\n",
    "\n",
    "# Generate the response data from predictor data using the helper function get_response_data\n",
    "y = get_response_data(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the sample loss landscape\n",
    "\n",
    "Here we assume that $Y$ is the loss, and $X$ is the weight to be updated.\n",
    "\n",
    "${\\partial L}/{\\partial w} = {\\partial y}/{\\partial x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the helper code below to plot the generated data\n",
    "plt.plot(x, y, linewidth=3, color='#F5B7B1')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('$w$', fontsize=16)\n",
    "plt.ylabel('$L$', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effective learning rate over epochs\n",
    "\n",
    "1. RMS Prop\n",
    "2. Exponential Decay\n",
    "3. AdaGrad\n",
    "\n",
    "\n",
    "### Here $\\epsilon$ is the initial learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the initial learning rate\n",
    "epsilon = ___\n",
    "\n",
    "# Set the initial starting value for the weight to a value from -5 to 5\n",
    "W = ___\n",
    "\n",
    "# Set the decay rate for the lr_decay function\n",
    "decay_rate = ___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⏸  What is likely to happen if you choose a very high decay rate for **Learning Rate Decay**?\n",
    "\n",
    "#### A. The model will converge to the local minima in fewer iterations.\n",
    "#### B. The learning rate will first decrease and then increase. \n",
    "#### C. The model will take more iterations to converge to the local minima.\n",
    "#### D. The learning rate will first increase then decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow1) ###\n",
    "# Type your answer within in the quotes given\n",
    "\n",
    "answer1 = '___'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the helper code below to visualize the variation \n",
    "# of the learning rate over the duration of the train\n",
    "Ws_rms,rms = rms_prop(W, epsilon=epsilon)\n",
    "Ws_ada,ada = adagrad(W, epsilon=epsilon)\n",
    "Ws_lrs, lrs = lr_decay(W,epsilon=epsilon,decay_rate=decay_rate)\n",
    "\n",
    "# To plot the adaptive and scheduled learning rates of the 3 algorithms\n",
    "# The loss landscape plot is for the learning rate decay algorithm\n",
    "comparison_plot(rms=rms,ada=ada,lrs=lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot the landscape of the 3 algorithms\n",
    "landscapes_plot(x,y,Ws_lrs,Ws_ada,Ws_rms,rms=rms,ada=ada,lrs=lrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⏸ Why does the learning rate for **RMS Prop** first decrease and then increase?\n",
    "\n",
    "#### A. Because of the momentum parameter in RMS prop.\n",
    "#### B. Because of the decreasing gradient as we approach the minima.\n",
    "#### C. Because the second moment is a weighted moving average.\n",
    "#### D. Because of learning rate decay parameter $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow2) ###\n",
    "# Type your answer within in the quotes given\n",
    "\n",
    "answer2 = '___'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
