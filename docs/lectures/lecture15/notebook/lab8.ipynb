{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title :\n",
    "Grad-CAM from scratch\n",
    "\n",
    "## Description :\n",
    "The goal of this exercise is to make a saliency map using Grad-CAM.\n",
    "\n",
    "Your final image may resemble the one below:\n",
    "\n",
    "<img src=\"../fig/fig1.png\" style=\"width: 500px;\">\n",
    "\n",
    "For this exercise, we will use the MobileNetV2 pre-trained model. You will apply Grad-CAM to the input cat image using what we learnt from lecture:\n",
    "\n",
    "<img src=\"../fig/fig2.png\" style=\"width: 500px;\">\n",
    "\n",
    "<img src=\"../fig/fig3.png\" style=\"width: 500px;\">\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "- Load the pre-trained model and pre-process the given image to make a prediction.\n",
    "- Find the predicted class of the image. It should be an **Egyptian cat**.\n",
    "- Using the tf.keras Functional API, build a model that gives the model predictions and the feature maps after the last convolution in the pre-trained network.\n",
    "- Using <a href=\"https://www.tensorflow.org/api_docs/python/tf/GradientTape\" target=\"_blank\">tf.GradientTape()</a> find the gradients of the output with respect to the activations.\n",
    "- As per the Grad-CAM implementation, pool the gradients and find the heatmap.\n",
    "- Upsample the heatmap using the helper function and superimpose it on the original image to get the output like the one shown above.\n",
    "\n",
    "## Hints: \n",
    "\n",
    "<a href=\"https://keras.io/guides/sequential_model/\" target=\"_blank\">model.layers</a>\n",
    "Accesses layers of the model\n",
    "\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/activations/linear\" target=\"_blank\">tf.keras.activations.linear</a>\n",
    "Linear activation function\n",
    "\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/Model\" target=\"_blank\">model.predict()</a>\n",
    "Used to predict the values given the model\n",
    "\n",
    "<a href=\"https://keras.io/api/applications/mobilenet/#mobilenetv2-function\" target=\"_blank\">tf.keras.applications.mobilenet_v2.MobileNetV2</a>\n",
    "Instantiates the MobileNet v2 architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MobileNet V2 pre-trained model\n",
    "# Rather than training a model from scratch we can use a pre trained\n",
    "# model that has already been trained in the imagenet dataset\n",
    "# MobileNetV2 is a SOTA model for image classification \n",
    "model = MobileNetV2(weights='imagenet')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow1) ###\n",
    "\n",
    "# Find the last convolutional layer\n",
    "# Inspect the model summary and find the last convolution layer\n",
    "# Get the name of the last convolution layer\n",
    "conv_layer_name = model.layers[___].name\n",
    "print(conv_layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a sample image to find the saliency map\n",
    "img_path = './cat.png'\n",
    "\n",
    "# Load the image with the target_size for mobilenet\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "# Convert the image to a numpy array \n",
    "x = image.img_to_array(img)\n",
    "\n",
    "# Add an extra dimension for batch size \n",
    "# to change it to (1,224,224,3)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "# Use the MobileNetV2 preprocess_input function on the image\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pretrained model to make a prediction \n",
    "preds = model.predict(x)\n",
    "\n",
    "# Useful dictionary to go from label index to actual label\n",
    "with open('idx2name.pkl', 'rb') as handle:\n",
    "    keras_idx_to_name = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the output predictions is:\n",
    "prediction_class = keras_idx_to_name[np.argmax(preds,axis=1).item(0)]\n",
    "print(f'Prediction class is {prediction_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the tf.keras Functional API to get \n",
    "# 1. The model prediction probabilities\n",
    "# 2. The feature maps after the last convolution in the model\n",
    "\n",
    "# Get the last convolution layer in the network\n",
    "last_conv_layer = model.get_layer(conv_layer_name)\n",
    "\n",
    "# Get the output predictions and the last_conv_layer\n",
    "# Using tf.keras functional API\n",
    "get_maps = Model(inputs = [model.inputs], outputs = [model.output, last_conv_layer.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we perform the Grad-CAM,\n",
    "# We take the gradient of the output with respect to the feature maps\n",
    "# after the convolution \n",
    "with tf.GradientTape() as tape:\n",
    "\n",
    "    # Getting the required outputs \n",
    "    model_out, last_conv_layer = get_maps(x)\n",
    "    \n",
    "    # We choose the output with maximum probability\n",
    "    # But this can be different depending on your choice\n",
    "    # For eg. you could select the second highest probability value \n",
    "    class_out = tf.reduce_max(model_out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⏸ **Take the gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow2) ###\n",
    "\n",
    "# We take the gradients \n",
    "# tape.gradient() takes the gradient of something with respect to \n",
    "# something else. Here we want the derivative of the output class\n",
    "# with respect to to the last conv layer\n",
    "grads = tape.gradient(___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here we combine all the gradients for each feature map \n",
    "pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "  \n",
    "# As per grad-CAM literature, here we need to multiply \n",
    "# the pooled grads with each feature map and take the average across\n",
    "# all the feature maps to make the heat map\n",
    "heatmap = tf.reduce_mean(tf.multiply(pooled_grads, last_conv_layer), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below we convert heatmap to numpy\n",
    "# Make all values positive\n",
    "# and reshape from (1,7,7) to (7,7) for ease of plotting\n",
    "heatmap = heatmap.numpy()\n",
    "heatmap[heatmap < 0] = 0 #relu\n",
    "heatmap = (heatmap - heatmap.min())/(heatmap.max() - heatmap.min())\n",
    "heatmap = heatmap.reshape((7, 7))\n",
    "\n",
    "# We plot the (7,7) heatmap\n",
    "plt.imshow(heatmap,cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inorder to map to the original image\n",
    "# This heatmap has to be be resized\n",
    "resized_heatmap = np.uint8(cv2.resize(heatmap,(224,224))*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to add a pre-processing step\n",
    "# to convert the grayscale heatmap\n",
    "# to a true JET colormap of 3 channels\n",
    "# for ease of viewing\n",
    "val = np.uint8(256-resized_heatmap)\n",
    "heatmap_final = cv2.applyColorMap(val, cv2.COLORMAP_JET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also prepare the image for plotting\n",
    "# by converting to tensor\n",
    "# and converting dtype to int8\n",
    "img = image.img_to_array(img)\n",
    "img = np.uint8(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we use the cv2.addweighted function\n",
    "# to superimpose the heatmap on the original image\n",
    "# Use the helper code below to do the same\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,6))\n",
    "ax.imshow(cv2.addWeighted(heatmap_final, 0.5, img, 0.5, 0))\n",
    "ax.axis('off');\n",
    "fig.suptitle(f'Predicted class: {prediction_class}',y=0.92,fontsize=14);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⏸ Will Grad-CAM work if we took the output from the last ReLU instead ? (True or False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow3) ###\n",
    "\n",
    "# Type your answer within in the quotes given \n",
    "answer3 = '___'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⏸ The heatmap output is displaying:\n",
    "\n",
    "A: The weights of the layer <br/>\n",
    "\n",
    "B: A 7x7 mask from the input image <br/>\n",
    "\n",
    "C: The pixels that activates the most in red and the least in blues <br/> \n",
    "\n",
    "D: A feature map of the input image <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow4) ###\n",
    "\n",
    "# Type your answer within in the quotes given \n",
    "answer4 = '_'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
