var tipuesearch = {"pages":[{"title":"FAQ","text":"General I'm unable to attend lectures in person. Can I take the class asynchronously? Lecture attendance is required. Non-DCE students should only register if they can attend in person. Does the individual HW mean I have to submit on my own but can I still work with a HW partner? An individual CS109B HW means you are supposed to work on your own, without any human intervention, assistance, or input. You are not allowed to work with a partner. You are allowed to use OHs to ask clarification questions, but we may not be able to answer all of your questions or help with your coding. You are allowed to use all of your materials from the course up to the HW, look at problem resolution online, and look at documentation. Where should I send my questions? Use Ed for anying related to the course content or assignments. All other concerns should be sent to the course helpline: cs109b2022@gmail.com Auditing Can I audit this course? What are the rules for auditing? Yes, CS109B does accept auditors, but all auditors must agree to abide by the following rules: Auditors must attend class in person. This is a Harvard policy. Auditors who do not confirm their presence with the head TF during the first week of in-class instruction will lose course access. Auditors are held to the same standard of academic honesty as our registered students. Please do not share homeworks or solutions with anyone. Violations will be reported to the Harvard Administrative Board. Auditors are not permitted to take the course for credit in the future. Audiors should not submit HWs or participate in projects. Auditors should refrain from using any course and TF resources that are designed for our registered students like Ed, FASOnDemand, JupyterHub, and office hours. Auditors must have an active HUID number. If you agree to the above, send an email to cs109a2022@gmail.com with your HUID and state that you've read the auditing policy and that you agree to abide by these rules. Quizzes & Exercises When are quizzes and exercises due? All 'pre-class reading check' quizzes and exercises presented in class are ungraded and so have no due date, though solutions will be released within a few days. All graded quizzes and exercises on Ed will be 'bundled' with a given HW assignment and so will share that HW's due date. Each HW assignment will make clear which Ed quizzes and exercises are to be completed as part of the assignment.","tags":"pages","url":"pages/faq.html"},{"title":"Modules","text":"A. Machine Learning for Medical Diagnosis B. Searching for Near Earth Asteroids C. Predicting Citation Networks in Legal Texts D. Scene Detection for Design E. Applied Differential Privacy on U.S. Census Data F. Microbiome Dynamics in Health and Disease G. Visual Recognition of Images on Ancient Greek Vases H. Dynamics of Disease Transmission and Human Behavior","tags":"pages","url":"pages/modules.html"},{"title":"Schedule","text":"Date (Mon) Lecture (Mon) Lecture (Wed) Lab (Fri) Advanced Section (Wed) Assignment (R:Released Wed - D:Due Wed) 24-Jan Lecture 1: Clustering 1 Lecture 2: Clustering 2 Lab 1 R:HW1 31-Jan Lecture 3: Bayes 1 Lecture 4: Bayes 2 Lab 2 Advanced Section 1: Gaussian Mixture Models 7-Feb Lecture 5: Bayes 3 Lecture 6: Bayes 4 Lab 3 R:HW2 - D:HW1 14-Feb Lecture 7: Bayes 5 Lecture 8: Neural Networks 1 (MLP) Lab 4 Advanced Section 2: Particle Filters/Sequential Monte Carlo 21-Feb No Lecture (Holiday) Lecture 9: NN 2 Gradient Descent; SGD; BackProp) Lab 5 R:HW3 - D:HW2 28-Feb Lecture 10: NN 3 (Optimizers) Lecture 11: NN 4 (Regularization) Lab 6 Advanced Section 3: Solvers 7-Mar Lecture 12: Convolutional Neural Networks 1 (Basics) Lecture 13: CNNs 2 (Regularization) Lab 7 Advanced Section 4: Segmentation R:HW4 - D:HW3 14-Mar No Lecture (Spring Break) No Lecture No Lab 21-Mar Lecture 14: CNNs 3 (Receptive Field) Lecture 15: CNNs 4 (Saliency Map) No Lab (Midterm) Advanced Section 5: SOTA & Transfer Learning 28-Mar Lecture 16: Intro to Language Models Lecture 17: Recurrent Neural Networks Lab 8 Advanced Section 6: Autoencoders R:HW5(Individual) - D:HW4 4-Apr Lecture 18: NLP 1 (GRUs/LSTMs) Lecture 19: NLP 2 (ELMO) Lab 9 Advanced Section 7: Word2Vec R:HW6(Individual) - D:HW5(Individual) 11-Apr Lecture 20: NLP 3 (Seq2Seq & Attention) Lecture 21: NLP 4 (Transformers) Lab 10 Advanced Section 8: BERT 18-Apr Lecture 22: GANs 1 Lecture 23: GANs 2 Lab 11 Advanced Section 9: More GANs! D:HW6(Individual) - R:HW7 25-Apr Module: Lecture Domain Module: Problem Background Project Work D:HW7 2-May Project Work Project Work Project Submission Due 9-May Peer Evaluations Due Final Project Showcase","tags":"pages","url":"pages/schedule.html"},{"title":"Syllabus","text":"Draft Syllabus Subject to Change Advanced Topics in Data Science (Spring 2022) CS 109b, AC 209b, Stat 121b, or CSCI E-109b Instructors Pavlos Protopapas (SEAS) & Mark Glickman (Statistics) Lectures: Mon & Wed 9:45‐11am SEC 1.321 Labs: Fri 9:45-11am SEC 1.321 Advanced Sections: Wed 2:15-3:30pm SEC 1.321 (starting 2/2; see schedule for specific dates) Office Hours: See Ed Post Prerequisites: CS 109a, AC 209a, Stat 121a, or CSCI E-109a or the equivalent. Course description Tentative Course Topics Course Objectives Course Components Lectures Labs Advanced Sections Midterm Projects Homework Assignments Course Resources Online Materials Recommended Textbooks & Articles Getting Help Course Policies and Expectations Grading Collaboration Policy Late or Incorrectly Submitted Assignments Re-grade Requests Auditing the Class Academic Integrity Accommodations for students with disabilities Diversity and Inclusion Statement Course Description Advanced Topics in Data Science (CS109b) is the second half of a one-year introduction to data science. Building upon the material in Introduction to Data Science, the course introduces advanced methods for data wrangling, data visualization, statistical modeling, and prediction. Topics include big data, multiple deep learning architectures such as CNNs, RNNs, language models, transformers, autoencoders, and generative models as well as Bayesian modeling, sampling methods, and unsupervised learning. The programming language will be Python. Tentative Course Topics Unsupervised Learning, Clustering Bayesian Inference Hierarchical Bayesian Modeling Fully Connected Neural Networks Convolutional Neural Networks Autoencoders Recurrent Neural Networks NLP / Text Analysis Transformers Variational AutoEncoders & Generative Models Generative Adversarial Networks Course Objectives Upon successful completion of this course, you should feel comfortable with the material mentioned above, and you will have gained experience working with others on real-world problems. The content knowledge, the project, and teamwork will prepare you for the professional world or further studies. Course Components Lectures, labs, and advanced sections will be live-streamed for Extension School students and can be accessed through the Zoom section on Canvas. Video recordings of the live stream will be made available to all students within 24 hours after the event, and will be accessible from the Lecture Video section on Canvas. Lectures The class meets for lectures twice a week (M & W). Attending and participating in lectures is a crucial component of learning the material presented in this course. Students may be asked to complete short readings before certain lectures. Some lectures will also include real-time coding exercises which we will complete as a class. Labs Lab will be held every Friday. Labs will present guided, hands-on coding challenges to prepare students for successfully completing the homework assignments. Advanced Sections The course will include advanced sections for 209b students and will cover a different topic per week. These 75 min sessions will cover advanced topics like the mathematical underpinnings of the methods seen in the main course lectures and lab as well as extensions of those methods. The material covered in the advanced sections is required for all AC209b students. Tentative topics are: Gaussian Mixture Models Particle Filters/Sequential Monte Carlo NN as Universal Approximator Solvers Segmentation Techniques, YOLO, Unet and M-RCNN Variational Autoencoders Word2Vec BERT GANS, Cycle GANS, etc. Note: Advanced Section are not held every week. Consult the course calendar for exact dates. Midterm There will be a midterm exam on Friday, March 25th from 9:45-11am (regular lab time). The exam will likely consist of multiple choice questions with a take-home coding component. More information to follow. Projects Beginning the last week of classes (4/25), students will join groups of 3-4 and be divided into break-out, thematic sections to study an open problem in one of various domains. The domains are tentative at the moment but may include medicine, law, astronomy, e-commerce, government, and areas in the humanities. Each section will include lectures by Harvard faculty who are experts in the field. Project work will continue on through reading period and conclude with final submissions on 5/6. The final submission will consist of a written report, a Jupyter notebook with all relevant code, and a 6-minute, pre-recorded presentation video. Homework Assignments There will be 7 graded homework assignments. Some of them will be due one week after being assigned and some will be due two weeks after being assigned. For 5 assignments, you have the option to work and submit in pairs, the 2 remaining are to be completed individually. Standard assignments are graded out of 5 points. AC209b students will have additional homework content for most assignments worth 1 point. Course Resources Online Materials All course materials, including lecture notes, lab notes, and section notes will be published on Ed, the course GitHub repo, and the public site's 'Materials' section. Note: Lecture content for lectures 1-7 will only be accessible to registered students. Assignments will only be posted on Canvas. Working Environment You will be working in Jupyter Notebooks which you can run in your own machine or in the SEAS JupyterHub. Recommended Textbooks ISLR: An Introduction to Statistical Learning by James, Witten, Hastie, Tibshirani (Springer: New York, 2013) BDA3: Bayesian Data Analysis by Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, Donald B. Rubin (CRC Press: New York, 2013) DL: Deep Learning by Goodfellow, Bengio and Courville. (The MIT Press: Cambridge, 2016) Glassner: Deep Learning, Vol. 1 & 2 by Andrew Glassner SLP Speech and Language Processing by Jurafsky and Martin (3rd Edition Draft) INLP Introduction to Natural Language Processing by Jacob Eisenstein (The MIT Press: Cambridge, 2019) Free electronic versions are available ( ISLR , DL , SLP , INLP ) or hard copy through Amazon ( ISLR , DL , Glassner , SLP , INLP ). Articles & Excerpts Unsupervised learning: Basics: James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An introduction to statistical learning (2nd ed.). New York: Springer. Chapter 12 https://hastie.su.domains/ISLR2/ISLRv2_website.pdf Silhouette plots: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html Gap statistic: Tibshirani, R., Walther, G., & Hastie, T. (2001). Estimating the number of clusters in a data set via the gap statistic. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), 411-423. https://hastie.su.domains/Papers/gap.pdf DBSCAN: Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017). DBSCAN revisited, revisited: why and how you should (still) use DBSCAN. ACM Transactions on Database Systems (TODS), 42(3), 1-21. https://dl.acm.org/doi/pdf/10.1145/3068335?casa_token=_P479lYnlpsAAAAA:PckzU6ZiTt3yMNzFrXyzESZ3N_pp904kN0N2QEwIoq6CxtfPCxnL9bNTGtjhuiNtzSfKyXoM-QI Bayesian material Basics: Glickman, Mark E. and Van Dyk, David A. (2007) \"Basic Bayesian Methods\" In Topics in Biostatistics (Methods in Molecular Biology). Edited by Walter Ambrosius. The Humana Press Inc., Totowa, NJ. ISBN 1-58829-531-1. pp 319-338. Chapter accessible from http://www.glicko.net/research/glickman-vandyk.pdf Importance sampling, rejection sampling, MCMC, Metropolis, Gibbs sampler: Andrieu, C., De Freitas, N., Doucet, A., & Jordan, M. I. (2003). An introduction to MCMC for machine learning. Machine learning, 50(1), 5-43. Article accessible from: https://www.cs.ubc.ca/~arnaud/andrieu_defreitas_doucet_jordan_intromontecarlomachinelearning.pdf Bayesian examples - regression, hierarchical modeling: Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A., & Rubin, D.B. (2013). Bayesian Data Analysis (3rd ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b16018 http://www.stat.columbia.edu/~gelman/book/BDA3.pdf Chapter 14: Introduction to regression models Chapter 15: Hierarchical linear models Chapter 16: Generalized linear models (includes logistic regression) Getting Help For questions about homework, course content, package installation, the process is: try to troubleshoot yourself by reading the lecture, lab, and section notes, and looking up online resources. go to office hours this is the best way to get help. post on the class Ed forum; we want you and your peers to engage in helping each other. TFs also monitor Ed and will respond within 24 hours. Note that Ed questions are visible to everyone. If you are citing homework solution code you must post privately so that only the staff sees your message. watch for official announcements via Ed. These announcements will also be sent to the email address associated with your Canvas account so make sure you have it set appropriately. send an email to the Helpline cs109b2022@gmail.com for administrative issues, regrade requests, and non-content specific questions. for personal matters that you do not feel comfortable sharing with the TFs, you may send an email to either or both of the instructors. Course Policies and Expectations Grading for CS109b, STAT121b, and CS209b: Your final score for the course will be computed using the following weights: Assignment Final Grade Weight Paired Homework (5) 45% Individual Homework (2) 23% Midterm 12% Project 20% Total 100% Note: Regular homework (for everyone) counts as 5 points. 209b extra homework counts as 1 point. Collaboration Policy We expect you to adhere to the Harvard Honor Code at all times. Failure to adhere to the honor code and our policies may result in serious penalties, up to and including automatic failure in the course and reference to the ad board. If you work with a partner on an assignment make sure both parties solve all the problems. Do not divide and conquer. You are expected to be intellectually honest and give credit where credit is due. In particular: if you work with a fellow student but decide to submit different papers, include the name of each other in the designated area of the submission paper. if you work with a fellow student and want to submit the same paper you need to form a group prior to the submission. Details in the assignment. Not all assignments will permit group submissions. you need to write your solutions entirely on your own or with your collaborator you are welcome to take ideas from code presented in labs, lecture, or sections but you need to change it, adapt it to your style, and ultimately write your own. We do not want to see code copied verbatim from the above sources. if you use code found on the internet, books, or other sources you need to cite those sources. you should not view any written materials or code created by other students for the same assignment; you may not provide or make available solutions to individuals who take or may take this course in the future. if the assignment allows it you may use third-party libraries and example code, so long as the material is available to all students in the class and you give proper attribution. Do not remove any original copyright notices and headers. Late or Incorrectly Submitted Assignments Each student is allowed up to 3 late days over the semester with at most 1 day applied to any single homework . Outside of these allotted late days, late homework will not be accepted unless there is a medical (if accompanied by a doctor's note) or other official University-excused reasons. There is no need to ask before using one of your late days. If you forgot to join a Group with your peer and are asking for the same grade we will accept this with no penalty up to HW3. For homeworks beyond that we feel that you should be familiar with the process of joining groups. After that there will be a penalty of -1 point for both members of the group provided the submission was on time. Grading Guidelines Homework will be graded based on: How correct your code is (the Notebook cells should run, we are not troubleshooting code) How you have interpreted the results — we want text not just code. It should be a report. How well you present the results. The scale is 0 to 5 for each assignment and 0 to 1 for the additional 209 assignments. Re-grade Requests Our graders and instructors make every effort in grading accurately and in giving you a lot of feedback. If you discover that your answer to a homework problem was correct but it was marked as incorrect, send an email to the Helpline with a description of the error. Please do not submit regrade requests based on what you perceive is overly harsh grading . The points we take off are based on a grading rubric that is being applied uniformly to all assignments. If you decide to send a regrade request , send an email to the Helpline with subject line \"Regrade HW1: Grader=johnsmith\" replacing 'HW1' with the current assignment and 'johnsmith' with the name of the grader within 48 hours of the grade release. Auditing the Class You are welcome to audit this course. To request access, send an email to cs109b2022@gmail.com with you HUID (required) and a statement of agreement to the terms below. All auditors must agree to abide by the following rules: Auditors must attend class in person. This is a Harvard policy. Auditors who do not confirm their presence with the head TF during the first week of in-class instruction will lose course access. Auditors are held to the same standard of academic honesty as our registered students. Please do not share homeworks or solutions with anyone. Violations will be reported to the Harvard Administrative Board. Auditors are not permitted to take the course for credit in the future. Audiors should not submit HWs or participate in projects. Auditors should refrain from using any course and TF resources that are designed for our registered students like Ed, FASOnDemand, JupyterHub, and office hours. Academic Integrity Ethical behavior is an important trait of a Data Scientist, from ethically handling data to attribution of code and work of others. Thus, in CS109b we give a strong emphasis to Academic Honesty. As a student your best guidelines are to be reasonable and fair. We encourage teamwork for problem sets, but you should not split the homework and you should work on all the problems together. For more detailed expectations, please refer to the Collaborations section above. You are responsible for understanding Harvard Extension School policies on academic integrity https://www.extension.harvard.edu/resources-policies/student-conduct/academic-integrity and how to use sources responsibly. Stated most broadly, academic integrity means that all course work submitted, whether a draft or a final version of a paper, project, take-home exam, online exam, computer program, oral presentation, or lab report, must be your own words and ideas, or the sources must be clearly acknowledged. The potential outcomes for violations of academic integrity are serious and ordinarily include all of the following: required withdrawal (RQ), which means a failing grade in the course (with no refund), the suspension of registration privileges, and a notation on your transcript. Using sources responsibly https://www.extension.harvard.edu/resources-policies/resources/avoiding-plagiarism is an essential part of your Harvard education. We provide additional information about our expectations regarding academic integrity on our website. We invite you to review that information and to check your understanding of academic citation rules by completing two free online 15-minute tutorials that are also available on our site. (The tutorials are anonymous open-learning tools.) Accommodations for students with disabilities Harvard students needing academic adjustments or accommodations because of a documented disability must present their Faculty Letter from the Accessible Education Office (AEO) and speak with the professor by the end of the second week of the term, (fill in specific date). Failure to do so may result in the Course Head's inability to respond in a timely manner. All discussions will remain confidential, although Faculty are invited to contact AEO to discuss appropriate implementation. Harvard Extension School is committed to providing an inclusive, accessible academic community for students with disabilities and chronic health conditions. The Accessibility Services Office (ASO) https://www.extension.harvard.edu/resources-policies/accessibility-services-office-aso offers accommodations and supports to students with documented disabilities. If you have a need for accommodations or adjustments in your course, please contact the Accessibility Services Office by email at accessibility@extension.harvard.edu or by phone at 617-998-9640. Diversity and Inclusion Statement Data Science and Computer Science have historically been representative of only a small sliver of the population. This is despite the contributions of a diverse group of early pioneers - see Ada Lovelace, Dorothy Vaughan, and Grace Hopper for just a few examples. As educators, we aim to build a diverse, inclusive, and representative community offering opportunities in data science to those who have been historically marginalized. We will encourage learning that advances ethical data science, exposes bias in the way data science is used, and advances research into fair and responsible data science. We need your help to create a learning environment that supports a diversity of thoughts, perspectives, and experiences, and honors your identities (including but not limited to race, gender, class, sexuality, religion, ability, etc.) To help accomplish this: If you have a name and/or set of pronouns that differ from those in your official Harvard records, please let us know! If you feel like your performance in the class is being impacted by your experiences outside of class, please do not hesitate to come and talk with us. We want to be a resource for you. Remember that you can also submit anonymous feedback (which will lead to us making a general announcement to the class, if necessary, to address your concerns). If you prefer to speak with someone outside of the course, you may find helpful resources at the Harvard Office of Diversity and Inclusion. We (like many people) are still learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to us about it. As a participant in course discussions, you are expected to respect your classmates' diverse backgrounds and perspectives. Our course will discuss diversity, inclusion, and ethics in data science. Please contact us (in person or electronically) or submit anonymous feedback if you have any suggestions for how we can improve.","tags":"pages","url":"pages/syllabus.html"},{"title":"Advanced Section 1: Gaussian Mixture Models","text":"Slides GMM (PDF) Notebooks GMM (IPYNB)","tags":"a-sections","url":"a-sections/a-sec01/"},{"title":"Advanced Section 1: Gaussian Mixture Models","text":"Advanced Section: Gaussian Mixture Models CS 109B Spring, 2021 In [1]: ### Import basic libraries import numpy as np import scipy as sp import pandas as pd import sklearn as sk from sklearn.linear_model import LinearRegression import matplotlib.pyplot as plt % matplotlib inline Motivation for Latent Variable Models A Model for Birth Weights Recall our model for birth weigths, $Y_1,\\ldots, Y_N$. We posited that the birth weights are iid normally distributed with known $\\sigma&#94;2$, $Y_n \\sim \\mathcal{N}(\\mu, 1)$. Compare the maximum likelihood model and the Bayesian model for bith weight. Which model would you use to make clinical decisions? What's hard about this comparison? A Similarity Measure for Distributions: Kullback–Leibler Divergence Visually comparing models to the empirical distribution of the data is impractical. Fortunately, there are a large number of quantitative measures for comparing two distributions, these are called divergence measures . For example, the Kullback–Leibler (KL) Divergence is defined for two distributions $p(\\theta)$ and $q(\\theta)$ supported on $\\Theta$ as: $$ D_{\\text{KL}}[q \\,\\|\\, p] = \\int_{\\Theta} \\log\\left[\\frac{q(\\theta)}{p(\\theta)} \\right] q(\\theta)d\\theta $$ The KL-divergence $D_{\\text{KL}}[q \\,\\|\\, p]$ is bounded below by 0, which happens if and only if $q=p$. The KL-divergence has information theoretic interpretations that we will explore later in the course. Note: The KL-divergence is defined in terms of the pdf's of $p$ and $q$. If $p$ is a distribution from which we only have samples and not the pdf (like the empirical distribution), we can nontheless estimate $D_{\\text{KL}}[q \\,\\|\\, p]$. Techniques that estimate the KL-divergence from samples are called non-parametric . We will use them later in the course. Why is the KL bounded below by 0? First let's see why the answer isn't obvious. Recall that the KL divergence is the expected log ratio between two distribution : $$ D_{\\text{KL}} [q\\| p] = \\mathbb{E}_{q}\\left[ \\log \\frac{q}{p}\\right] $$ Now, we know that when $q$ is less than $p$ (i.e. $q/p < 1$) then the log can be an arbitrarily negative number. So it's not immediately obvious that the expected value of this fraction should always be non-negative! An intuitive explanation: Let the blue curve be q and the red be p. We have $q < p$ from $(-\\infty, 55)$, on this part of the domain $\\log(q/p)$ is negative. On $[55, \\infty)$, $\\log(q/p)$ is nonnegative. However, since we are sampling from $q$, and $q$'s mass is largely over $[55, \\infty)$, the log fraction $\\log(q/p$) will tend to be nonnegative. A formal argument: There are many proofs of the non-negativity of the KL. Ranging from the very complex to the very simple. Here is one that just involves a bit of algebra: We want to show that $D_{\\text{KL}}[q\\|p] \\geq 0$. Instead we'll show, equivalently, that $-D_{\\text{KL}}[q\\|p] \\leq 0$ (we're choosing show the statement about the negative KL, just so we can flip the fraction on the inside of the log and cancel terms): Class Membership as a Latent Variable We observe that there are three clusters in the data. We posit that there are three classes of infants in the study: infants with low birth weights, infants with normal birth weights and those with high birth weights. The numbers of infants in the classes are not equal. For each observation $Y_n$, we model its class membership $Z_n$ as a categorical variable, $$Z_n\\sim Cat(\\pi),$$ where $\\pi_i$ in $\\pi = [\\pi_1, \\pi_2, \\pi_3]$ is the class proportion. Note that we don't have the class membership $Z_n$ in the data! So $Z_n$ is called a latent variable . Depending on the class, the $n$-th birth weight $Y_n$ will have a different normal distribution, $$ Y_n | Z_n \\sim \\mathcal{N}\\left(\\mu_{Z_n}, \\sigma&#94;2_{Z_n}\\right) $$ where $\\mu_{Z_n}$ is one of the three class means $[\\mu_1, \\mu_2, \\mu_3]$ and $\\sigma&#94;2_{Z_n}$ is one of the three class variances $[\\sigma&#94;2_1, \\sigma&#94;2_2, \\sigma&#94;2_3]$. Common Latent Variable Models Latent Variable Models Models that include an observed variable $Y$ and at least one unobserved variable $Z$ are called latent variable models . In general, our model can allow $Y$ and $Z$ to interact in many different ways. Today, we will study models with one type of interaction: Gaussian Mixture Models (GMMs) In a Gaussian Mixture Model (GMM) , we posit that the observed data $Y$ is generated by a mixture, $\\pi=[\\pi_1, \\ldots, \\pi_K]$, of $K$ number of Gaussians with means $\\mu = [\\mu_1, \\ldots, \\mu_K]$ and covariances $\\Sigma = [\\Sigma_1, \\ldots, \\Sigma_K]$. For each observation $Y_n$ the class of the observation $Z_n$ is a latent variable that indicates which of the $K$ Gaussian is responsible for generating $Y_n$: \\begin{aligned} Z_n &\\sim Cat(\\pi),\\\\ Y_n | Z_n&\\sim \\mathcal{N}(\\mu_{Z_n}, \\Sigma_{Z_n}), \\end{aligned} where $n=1, \\ldots, N$ and $\\sum_{k=1}&#94;K \\pi_k = 1$. GMMs are examples of model based clustering - breaking up a data set into natural clusters based on a statistical model fitted to the data. Item-Response Models In item-response models , we measure an real-valued unobserved trait $Z$ of a subject by performing a series of experiments with binary observable outcomes, $Y$: \\begin{aligned} Z_n &\\sim \\mathcal{N}(\\mu, \\sigma&#94;2),\\\\ \\theta_n &= g(Z_n)\\\\ Y_n|Z_n &\\sim Ber(\\theta_n), \\end{aligned} where $n=1, \\ldots, N$ and $g$ is some fixed function of $Z_n$. Applications Item response models are used to model the way \"underlying intelligence\" $Z$ relates to scores $Y$ on IQ tests. Item response models can also be used to model the way \"suicidality\" $Z$ relates to answers on mental health surveys. Building a good model may help to infer when a patient is at psychiatric risk based on in-take surveys at points of care through out the health-care system. Factor Analysis Models In factor analysis models , we posit that the observed data $Y$ with many measurements is generated by a small set of unobserved factors $Z$: \\begin{aligned} Z_n &\\sim \\mathcal{N}(0, I),\\\\ Y_n|Z_n &\\sim \\mathcal{N}(\\mu + \\Lambda Z_n, \\Phi), \\end{aligned} where $n=1, \\ldots, N$, $Z_n\\in \\mathbb{R}&#94;{D'}$ and $Y_n\\in \\mathbb{R}&#94;{D}$. We typically assume that $D'$ is much smaller than $D$. Applications Factor analysis models are useful for biomedical data, where we typically measure a large number of characteristics of a patient (e.g. blood pressure, heart rate, etc), but these characteristics are all generated by a small list of health factors (e.g. diabetes, cancer, hypertension etc). Building a good model means we may be able to infer the list of health factors of a patient from their observed measurements. Maximum Likelihood Estimation for Latent Variable Models: Expectation Maximization Expectation Maximization: Estimating the MLE for Latent Variable Models Given a latent variable model $p(Y, Z| \\phi, \\theta) = p(Y | Z, \\phi) p(Z|\\theta)$, we are interested computing the MLE of parameters $\\phi$ and $\\theta$: \\begin{aligned} \\theta_{\\text{MLE}}, \\phi_{\\text{MLE}} &= \\underset{\\theta, \\phi}{\\mathrm{argmax}}\\; \\ell(\\theta, \\phi)\\\\ &= \\underset{\\theta, \\phi}{\\mathrm{argmax}}\\; \\log \\prod_{n=1}&#94;N \\int_{\\Omega_Z} p(y_n, z_n | \\theta, \\phi) dz\\\\ &= \\underset{\\theta, \\phi}{\\mathrm{argmax}}\\; \\log \\prod_{n=1}&#94;N \\int_{\\Omega_Z} p(y_n| z_n, \\phi)p(z_n| \\theta) dz \\end{aligned} where $\\Omega_Z$ is the domain of $Z$. Why is this an hard optimization problem? There are two major problems: the product in the integrand gradients cannot be past the integral (i.e. we cannot easily compute the gradient to solve the optimization problem). We solve these two problems by: pushing the log past the integral so that it can be applied to the integrand (Jensen's Inequality) introducing an auxiliary variables $q(Z_n)$ to allow the gradient to be pushed past the integral. \\begin{aligned} \\underset{\\theta, \\phi}{\\mathrm{max}}\\; \\ell(\\theta, \\phi) &= \\underset{\\theta, \\phi, q}{\\mathrm{max}}\\; \\log \\prod_{n=1}&#94;N\\int_{\\Omega_Z} \\left(\\frac{p(y_n, z_n|\\theta, \\phi)}{q(z_n)}q(z_n)\\right) dz\\\\ &= \\underset{\\theta, \\phi, q}{\\mathrm{max}}\\; \\log\\,\\prod_{n=1}&#94;N\\mathbb{E}_{Z\\sim q(Z)} \\left[ \\frac{p(y_n, Z|\\theta, \\phi)}{q(Z)}\\right]\\\\ &= \\underset{\\theta, \\phi, q}{\\mathrm{max}}\\; \\sum_{n=1}&#94;N \\log \\mathbb{E}_{Z\\sim q(Z)} \\left[\\,\\left( \\frac{p(y_n, Z|\\theta, \\phi)}{q(Z)}\\right)\\right]\\\\ &\\geq \\underset{\\theta, \\phi, q}{\\mathrm{max}}\\; \\underbrace{\\sum_{n=1}&#94;N\\mathbb{E}_{Z_n\\sim q(Z)} \\left[ \\log\\,\\left(\\frac{p(y_n, Z_n|\\theta, \\phi)}{q(Z_n)}\\right)\\right]}_{ELBO(\\theta, \\phi)}, \\quad (\\text{Jensen's Inequality})\\\\ \\end{aligned} We call $\\sum_{n=1}&#94;N\\mathbb{E}_{Z_n\\sim q(Z)} \\left[ \\log\\,\\left(\\frac{p(y_n, Z_n|\\theta, \\phi)}{q(Z)}\\right)\\right]$ the Evidence Lower Bound (ELBO). Note that maximizing the ELBO will yield a lower bound of the maximum value of the log likelihood. Although the optimal point of the ELBO may not be the optimal point of the log likelihood , we nontheless prefer to optimize the ELBO because the gradients, with respect to $\\theta, \\phi$, of the ELBO are easier to compute: $$ \\nabla_{\\theta, \\phi} ELBO(\\theta, \\phi) = \\nabla_{\\theta, \\phi}\\left[ \\sum_{n=1}&#94;N\\mathbb{E}_{Z_n\\sim q(Z)} \\left[ \\log\\,\\left(\\frac{p(y_n, Z_n|\\theta, \\phi)}{q(Z_n)}\\right)\\right]\\right] = \\sum_{n=1}&#94;N\\mathbb{E}_{Z_n\\sim q(Z)} \\left[ \\nabla_{\\theta, \\phi} \\left( \\log\\,\\left(\\frac{p(y_n, Z_n|\\theta, \\phi)}{q(Z_n)}\\right)\\right)\\right] $$ Note that we can push the gradient $\\nabla_{\\theta, \\phi}$ past the expectation $\\mathbb{E}_{Z_n\\sim q(Z)}$ since the expectation is not computed with respect to our optimization variables! Rather than optimizing the ELBO over all variables $\\theta, \\phi, q$ (this would be hard), we optimize one set of variables at a time: Step I: the M-step Optimize the ELBO with respect to $\\theta, \\phi$: \\begin{aligned} \\theta&#94;*, \\phi&#94;* = \\underset{\\theta, \\phi}{\\mathrm{max}}\\; ELBO(\\theta, \\phi, q) &= \\underset{\\theta, \\phi}{\\mathrm{max}}\\; \\sum_{n=1}&#94;N\\mathbb{E}_{Z_n\\sim q(Z)} \\left[ \\log\\,\\left(\\frac{p(y_n, Z_n|\\theta, \\phi)}{q(Z_n)}\\right)\\right]\\\\ &= \\underset{\\theta, \\phi}{\\mathrm{max}}\\; \\sum_{n=1}&#94;N \\int_{\\Omega_Z} \\log\\,\\left(\\frac{p(y_n, z_n|\\theta, \\phi)}{q(z_n)}\\right)q(z_n) dz_n\\\\ &= \\underset{\\theta, \\phi}{\\mathrm{max}}\\; \\sum_{n=1}&#94;N \\int_{\\Omega_Z} \\log\\,\\left(p(y_n, z_n|\\theta, \\phi)\\right) q(z_n)dz_n - \\underbrace{\\int_{\\Omega_Z} \\log \\left(q(z_n)\\right)q(z_n) dz_n}_{\\text{constant with respect to }\\theta, \\phi}\\\\ &\\equiv \\underset{\\theta, \\phi}{\\mathrm{max}}\\;\\sum_{n=1}&#94;N \\int_{\\Omega_Z} \\log\\,\\left(p(y_n, z_n|\\theta, \\phi)\\right) q(z_n)dz_n\\\\ &= \\underset{\\theta, \\phi}{\\mathrm{max}}\\;\\sum_{n=1}&#94;N \\mathbb{E}_{Z_n\\sim q(Z)} \\left[ \\log\\left(p(y_n, z_n|\\theta, \\phi)\\right)\\right] \\end{aligned} Step II: the E-step Optimize the ELBO with respect to $q$: \\begin{aligned} q&#94;*(Z_n) = \\underset{q}{\\mathrm{argmax}}\\;\\left(\\underset{\\theta, \\phi}{\\mathrm{argmax}}\\; ELBO(\\theta, \\phi, q) \\right) = \\underset{q}{\\mathrm{argmax}}\\; ELBO(\\theta&#94;*, \\phi&#94;*, q) \\end{aligned} Rather than optimizing the ELBO with respect to $q$, which seems hard, we will argue that optimizing the ELBO is equivalent to optimizing another function of $q$, one whose optimum is easy for us to compute. Note: We can recognize the difference between the log likelihood and the ELBO as a function we've seen: \\begin{aligned} \\ell(\\theta, \\phi) - ELBO(\\theta, \\phi, q) &= \\sum_{n=1}&#94;N \\log p(y_n| \\theta, \\phi) - \\sum_{n=1}&#94;N \\int_{\\Omega_Z} \\log\\left(\\frac{p(y_n, z_n|\\theta, \\phi)}{q(z_n)}\\right)q(z_n) dz_n\\\\ &= \\sum_{n=1}&#94;N \\int_{\\Omega_Z} \\log\\left(p(y_n| \\theta, \\phi)\\right) q(z_n) dz_n - \\sum_{n=1}&#94;N \\int_{\\Omega_Z} \\log\\left(\\frac{p(y_n, z_n|\\theta, \\phi)}{q(z_n)}\\right)q(z_n) dz_n\\\\ &= \\sum_{n=1}&#94;N \\int_{\\Omega_Z} \\left(\\log\\left(p(y_n| \\theta, \\phi)\\right) - \\log\\left(\\frac{p(y_n, z_n|\\theta, \\phi)}{q(z_n)}\\right) \\right)q(z_n) dz_n\\\\ &= \\sum_{n=1}&#94;N \\int_{\\Omega_Z} \\log\\left(\\frac{p(y_n| \\theta, \\phi)q(z_n)}{p(y_n, z_n|\\theta, \\phi)} \\right)q(z_n) dz_n\\\\ &= \\sum_{n=1}&#94;N \\int_{\\Omega_Z} \\log\\left(\\frac{q(z_n)}{p(z_n| y_n, \\theta, \\phi)} \\right)q(z_n) dz_n, \\quad\\left(\\text{Baye's Rule: } \\frac{p(y_n, z_n|\\theta, \\phi)}{p(y_n| \\theta, \\phi)} = p(z_n| y_n, \\theta, \\phi)\\right)\\\\ &= \\sum_{n=1}&#94;N D_{\\text{KL}} \\left[ q(Z_n) \\| p(Z_n| Y_n, \\theta, \\phi)\\right]. \\end{aligned} Since $\\ell(\\theta, \\phi)$ is a constant, the difference $\\sum_{n=1}&#94;N D_{\\text{KL}} \\left[ q(Z_n) \\| p(Z_n| Y_n, \\theta, \\phi)\\right] = \\ell(\\theta, \\phi) - ELBO(\\theta, \\phi, q)$ descreases when $ELBO(\\theta, \\phi, q)$ increases (and vice versa). Thus, maximizing the ELBO is equivalent to minimizing $D_{\\text{KL}} \\left[ q(Z_n) \\| p(Y_n| Z_n, \\theta, \\phi)\\right]$: $$ \\underset{q}{\\mathrm{argmax}}\\, ELBO(\\theta, \\phi, q) = \\underset{q}{\\mathrm{argmin}}\\sum_{n=1}&#94;N D_{\\text{KL}} \\left[ q(Z_n) \\| p(Z_n| Y_n, \\theta, \\phi)\\right]. $$ Thus, we see that \\begin{aligned} q&#94;*(Z_n) = \\underset{q}{\\mathrm{argmax}}\\; ELBO(\\theta&#94;*, \\phi&#94;*, q) = \\underset{q}{\\mathrm{argmin}}\\sum_{n=1}&#94;N D_{\\text{KL}} \\left[ q(Z_n) \\| p(Z_n| Y_n, \\theta, \\phi)\\right] = p(Z_n| Y_n, \\theta, \\phi) \\end{aligned} That is, we should set the optimal distribution $q$ to be the posterior $p(Z_n| Y_n, \\theta, \\phi)$. Iteration Of course, we know that optimizing a function with respect to each variable is not sufficient for finding the global optimum over all the variables, considered together! Thus, performing one E-step and one M-step is not enough to maximize the ELBO. We need to repeat the two steps over and over. Question: Why don't gradients commute with expectation? We have the following property of expectations: $$ \\nabla_z \\mathbb{E}_{x\\sim p(x)}[f(x, z)] = \\mathbb{E}_{x\\sim p(x)}[ \\nabla_z f(x, z)] $$ That is, when the gradient is with respect to a variable that does not appear in the distribution with respect to which you are taking the expectation, then you can push the gradient past the expectation. The intuition: the gradient with respect to $z$ is computing the changes in a function by making infinitesimally small changes to $z$, the expectation is computing the average value of a function by sampling $x$ from a distribution that does not depend on $z$. Each operation is making an independent change to two different variables and hence can be done in any order. Why can't you do this in general? I.e. why is it that, $$ \\nabla_z\\mathbb{E}_{x\\sim p(x|z)}[f(x, z)] \\neq \\mathbb{E}_{x\\sim p(x|z)}[ \\nabla_z f(x, z)]?$$ The intuition: the gradient with respect to z is computing the changes in a function by making infinitesimally small changes to z, which in turn affects the samples produced by p(x|z), these samples finally affect the output of f. This is a chain of effects and the order matters. The formal proof: Consider the following case, $$ p(x\\vert z) = (z+1)x&#94;z,\\; x\\in [0, 1] $$ and $$ f(x, z) = xzf ( x , z ) = x z. $$ Then, we have $$\\nabla_z \\mathbb{E}_{x\\sim p(x|z)} [f(x, z)] = \\nabla_z \\int_0&#94;1 f(x, z) p(x|z) dx = \\nabla_z\\int_0&#94;1 xz \\cdot (z+1)x&#94;z dx = \\nabla_z z (z+1)\\int_0&#94;1x&#94;{z+1} dx = \\nabla_z \\frac{z (z+1)}{z+2} [x&#94;{z+2} ]_0&#94;1 = \\nabla_z \\frac{z (z+1)}{z+2} = \\frac{z&#94;2 + 4z + 2}{(z+2)&#94;2} $$ On the other hand, we have $$ \\mathbb{E}_{x\\sim p(x|z)}\\left[ \\nabla_z f(x, z) \\right] = \\int_0&#94;1 \\nabla_z[ xz] (z+1)x&#94;zdx = \\int_0&#94;1(z+1)x&#94;{z+1}dx = \\frac{z+1}{z+2} [x&#94;{z+2}]_0&#94;1 = \\frac{z+1}{z+2}. $$ Note that: $$ \\nabla_z \\mathbb{E}_{x\\sim p(x|z)} [f(x, z)] = \\frac{z&#94;2 + 4z+ 2}{(z+2)&#94;2} \\neq \\frac{z+1}{z+2} = \\mathbb{E}_{x\\sim p(x|z)}\\left[ \\nabla_z f(x, z) \\right]. $$ Question: Why do we need to maximize the ELBO with respect to q? Recall that in the derivation of the ELBO, we first introduced an auxiliary variable q to rewrite the observed log-likelihood: $$ \\log p(y|\\theta, \\phi) = \\log \\int_\\Omega p(y, z| \\theta, \\phi) dz = \\log \\int_\\Omega \\frac{p(y, z| \\theta, \\phi}{q(z)}q(z) dz = \\log \\mathbb{E}_{q(z)} \\left[ \\frac{p(y, z|\\theta, \\phi)}{q(z)} \\right] $$ Again, the reason why we do this is because: when we eventually take the gradient wrt to $\\theta, \\phi$ during optimization we can use the identity $$ \\nabla_{\\theta, \\phi} \\mathbb{E}_{q(z)}\\left[\\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right] = \\mathbb{E}_{q(z)}\\left[\\nabla_{\\theta, \\phi} \\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right] $$ At this point, there is no need to maximize over q , that is: $$ \\max_{\\theta, \\phi, q}\\log \\mathbb{E}_{q(z)}\\left[\\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right] = \\max_{\\theta, \\phi}\\log \\mathbb{E}_{q(z)}\\left[\\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right] $$ The $q$ cancels and has no effect on the outcome or process of the optimization (but you can't just choose any $q$ you want - can you see what are the constraints on $q$?). Now, the problem is that the log is on the outside of the expectation. This isn't a problem in the sense that we don't know how to take the derivative of a logarithm of a complex function (this is just the chain rule ), the problem is that $$ \\nabla_{\\phi, \\theta} \\frac{p(y, z|\\theta, \\phi)}{q(z)} $$ can be very complex (since p and q are pdf's) and so over all the gradient of the log expectation is not something you can compute roots for. Here is where we push the log inside the expectation using Jensen's inequality: $$ \\log \\mathbb{E}_{q(z)}\\left[\\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right] \\geq \\mathbb{E}_{q(z)}\\left[\\log \\left(\\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right)\\right] \\overset{\\text{def}}{=} ELBO(\\phi, \\theta, q) $$ When we push the log inside the expectation, we obtain the E vidence L ower Bo und (ELBO). Now, for any choice of $q$, we always have: $$ \\max_{\\theta, \\phi}\\log \\mathbb{E}_{q(z)}\\left[\\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right] \\geq \\max_{\\theta, \\phi}ELBO(\\phi, \\theta, q) $$ But the ELBO is not necessarily a tight bound (i.e. maximizing the ELBO can be very far from maximizing the log-likelihood!)! In particular, some choices of $q$ might give you a tighter bound on the log-likelihood than others. Thus, we want to select the $q$ that give us the tightest bound: $$ \\max_{\\theta, \\phi}\\log \\mathbb{E}_{q(z)}\\left[\\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right] \\geq \\max_{\\theta, \\phi, q}ELBO(\\phi, \\theta, q). $$ The Expectation Maximization Algorithm The exepectation maximization (EM) algorithm maximize the ELBO of the model, Initialization: Pick $\\theta_0$, $\\phi_0$. Repeat $i=1, \\ldots, I$ times: E-Step: $$q_{\\text{new}}(Z_n) = \\underset{q}{\\mathrm{argmax}}\\; ELBO(\\theta_{\\text{old}}, \\phi_{\\text{old}}, q) = p(Z_n|Y_n, \\theta_{\\text{old}}, \\phi_{\\text{old}})$$ M-Step: \\begin{aligned} \\theta_{\\text{new}}, \\phi_{\\text{new}} &= \\underset{\\theta, \\phi}{\\mathrm{argmax}}\\; ELBO(\\theta, \\phi, q_{\\text{new}})\\\\ &= \\underset{\\theta, \\phi}{\\mathrm{argmax}}\\; \\sum_{n=1}&#94;N\\mathbb{E}_{Z_n\\sim p(Z_n|Y_n, \\theta_{\\text{old}}, \\phi_{\\text{old}})}\\left[\\log \\left( p(y_n, Z_n | \\phi, \\theta\\right) \\right]. \\end{aligned} The Auxiliary Function We often denote the expectation in the M-step by $Q\\left(\\theta, \\phi| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right)$ $$ Q\\left(\\theta, \\phi| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right) = \\sum_{n=1}&#94;N\\mathbb{E}_{Z_n\\sim p(Z_n|Y_n, \\theta_{\\text{old}}, \\phi_{\\text{old}})}\\left[\\log \\left( p(y_n, Z_n | \\phi, \\theta\\right) \\right] $$ and call $Q$ the auxiliary function. Frequently, the EM algorithm is equivalently presented as E-step: compute the auxiliary function: $Q\\left(\\theta, \\phi| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right)$ M-step: maximize the auxiliary function: $\\theta&#94;{\\text{new}}, \\phi&#94;{\\text{new}} = \\underset{\\theta, \\phi}{\\mathrm{argmax}}\\,Q\\left(\\theta, \\phi| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right)$. The log of the joint distribution $\\prod_{n=1}&#94;N p(Z_n, Y_n, \\theta, \\phi)$ is called the complete data log-likelihood (since it is the likelihood of both observed and latent variables), whereas $\\log \\prod_{n=1}&#94;N p(Y_n| \\theta, \\phi)$ is called the observed data log-likelihood (since it is the likelihood of only the observed variable). The auxiliary function presentation of EM is easy to interpret: In the E-step, you fill in the latent variables in the complete data log-likelihood using \"average\" values, this leaves just an estimate of the observed log-likelihood. In the M-step, you find parameters $\\phi$ and $\\theta$ that maximizes your estimate of the observed log-likelihood. We chose to derive EM via the ELBO in this lecture because it makes an explicit connection between the EM algorithm for estimating MLE and variational inference method for approximating the posterior of Bayesian models. It is, however, worthwhile to derive EM using the auxiliary function $Q$, as $Q$ makes it convient for us to prove properties of the EM algorithm. Monotonicity and Convergence of EM Before we run off estimating MLE parameters of latent variable models with EM, we need to sanity check two points: (Monotonicity) we need to know that repeating the E, M-steps will never decrease the ELBO! (Convergence) we need to know that at some point the EM algorithm will naturally terminate (the algorithm will cease to update the parameters). We first prove the monotonicity of EM. Consider the difference between $\\ell(\\theta, \\phi) - \\ell(\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})$, i.e. the amount by which the log-likelihood can increase or decrease by going from $\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}$ to $\\theta, \\phi$: \\begin{aligned} \\ell(\\theta, \\phi) - \\ell(\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}) &= \\sum_{n=1}&#94;N\\log \\left[ \\frac{p(y_n|\\theta, \\phi)}{p(y_n| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})}\\right]\\\\ &= \\sum_{n=1}&#94;N \\log\\int \\frac{p(y_n, z_n|\\theta, \\phi)}{p(y_n| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})} dz_n\\\\ &= \\sum_{n=1}&#94;N \\log\\int \\frac{p(y_n, z_n|\\theta, \\phi)}{p(y_n| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}) p(z_n|y_n, \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})}p(z_n|y_n, \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}) dz_n\\\\ &= \\sum_{n=1}&#94;N \\log\\int \\frac{p(y_n, z_n|\\theta, \\phi)}{p(y_n, z_n| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})}p(z_n|y_n, \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}) dz_n\\\\ &= \\sum_{n=1}&#94;N \\log \\mathbb{E}_{p(z_n|y_n, \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})} \\left[\\frac{p(y_n, z_n|\\theta, \\phi)}{p(y_n, z_n| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})}\\right]\\\\ &\\geq \\sum_{n=1}&#94;N \\mathbb{E}_{p(z_n|y_n, \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})} \\log\\left[\\frac{p(y_n, z_n|\\theta, \\phi)}{p(y_n, z_n| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})}\\right]\\\\ &= \\sum_{n=1}&#94;N \\mathbb{E}_{p(z_n|y_n, \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})} \\left[\\log p(y_n, z_n|\\theta, \\phi) - \\log p(y_n, z_n| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})\\right]\\\\ &= \\sum_{n=1}&#94;N \\mathbb{E}_{p(z_n|y_n, \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})} \\left[\\log p(y_n, z_n|\\theta, \\phi)\\right] - \\sum_{n=1}&#94;N \\mathbb{E}_{p(z_n|y_n, \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})}\\left[ \\log p(y_n, z_n| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})\\right]\\\\ &= Q\\left(\\theta, \\phi| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right) - Q\\left(\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right) \\end{aligned} Thus, when we maximize the gain in log-likelihood going from $\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}$ to $\\theta, \\phi$, we get: \\begin{aligned} \\underset{\\theta, \\phi}{\\max} \\left[\\ell(\\theta, \\phi) - \\ell(\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})\\right] \\geq \\underset{\\theta, \\phi}{\\max} \\left[Q\\left(\\theta, \\phi| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right) - Q\\left(\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right)\\right] \\end{aligned} or equivalently, \\begin{aligned} \\underset{\\theta, \\phi}{\\max} \\left[\\ell(\\theta, \\phi)\\right] - \\ell(\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}) \\geq \\underset{\\theta, \\phi}{\\max} \\left[Q\\left(\\theta, \\phi| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right)\\right] - Q\\left(\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right). \\end{aligned} Note that the above max is always greater than or equal to zero: $$\\underset{\\theta, \\phi}{\\max} \\left[Q\\left(\\theta, \\phi| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right)\\right] - Q\\left(\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right) \\geq 0$$ since we can always maintain the status quo by choosing $theta = \\theta&#94;{\\text{old}}$ $\\phi = \\phi&#94;{\\text{old}}$: $$ Q\\left(\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right) - Q\\left(\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right) = 0.$$ Thus, we have that by maximizing $Q\\left(\\theta, \\phi| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right)$, we ensure that $\\ell(\\theta, \\phi) - \\ell(\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})\\geq 0$ in each iteration of EM. If the likelihood of the model is bounded above (i.e. $\\ell(\\theta, \\phi) \\leq M$ for some constant $M$), then EM is guaranteed to convergence. This is because we've proved that EM increases (or maintains) log-likelihood in each iteration, therefore, if $\\ell(\\theta, \\phi)$ is bounded, the process must converge. Disclaimer: Although EM converges for bounded likelihoods, it is not guaranteed to converge to the global max of the log-likelihood! Maximizing a lower bound of a function does not necessarily maximize the function itself! Often time, EM converges to local optima of the likelihood function and the point to which it converges may be very sensitive to initialization. We will study this kind of behaviour in more detail when we cover non-convex optimization later in the course. Example: EM for the Gaussian Mixture Model of Birth Weight The Gaussian mixture model for the birth weight data has 3 Gaussians with meand $\\mu = [\\mu_1, \\mu_2, \\mu_3]$ and variances $\\sigma&#94;2 = [\\sigma_1&#94;2, \\sigma_2&#94;2, \\sigma_3&#94;2]$, and the model is defined as: \\begin{aligned} Z_n &\\sim Cat(\\pi),\\\\ Y_n | Z_n &\\sim \\mathcal{N}(\\mu_{Z_n}, \\sigma&#94;2_{Z_n}), \\end{aligned} where $n=1, \\ldots, N$ and $\\sum_{k=1}&#94;3 \\pi_k = 1$. The E-Step The E-step in EM computes the distribution: $$q_{\\text{new}}(Z_n) = \\underset{q}{\\mathrm{argmax}}\\; ELBO(\\mu_{i-1}, \\sigma&#94;2_{i-1}, \\pi_{i_1}, q) = p(Z_n|Y_n, \\mu_{\\text{old}}, \\sigma&#94;2_{\\text{old}}, \\pi_{\\text{old}}).$$ Since $Z_n$ is a label, $p(Z_n|Y_n, \\ldots)$ is a categorical distribution, with the probability of $Z_n=k$ given by: $$ p(Z_n = k|Y_n, \\mu_{\\text{old}}, \\sigma&#94;2_{\\text{old}}, \\pi_{\\text{old}}) = \\frac{p(y_n|Z_n = k, \\mu_{\\text{old}}, \\sigma&#94;2_{\\text{old}})p(Z_n=k | \\pi_{\\text{old}})}{\\sum_{k=1}&#94;K p(y|Z_n = k, \\mu_{\\text{old}}, \\sigma&#94;2_{\\text{old}})p(Z_n=k | \\pi_{\\text{old}})} = \\underbrace{\\frac{\\pi_{k, \\text{old}}\\,\\mathcal{N}(y_n; \\mu_{k, \\text{old}}, \\sigma&#94;2_{k, \\text{old}})}{\\mathcal{Z}}}_{r_{n, k}}, $$ where $\\mathcal{Z} = \\sum_{k=1}&#94;K \\pi_{k, \\text{old}}\\,\\mathcal{N}(y_n; \\mu_{k, \\text{old}}, \\sigma&#94;2_{k, \\text{old}})$. Example: EM for the Gaussian Mixture Model of Birth Weight Setting Up the M-Step The M-step in EM maximize the following: $$\\underset{\\mu, \\sigma&#94;2, \\pi}{\\mathrm{argmax}}\\; ELBO(\\mu, \\sigma&#94;2, \\pi, q_{\\text{new}}) = \\underset{\\mu, \\sigma&#94;2, \\pi}{\\mathrm{argmax}}\\; \\sum_{n=1}&#94;N\\mathbb{E}_{Z_n\\sim p(Z_n|Y_n, \\mu_{k, \\text{old}}, \\sigma&#94;2_{k, \\text{old}})}\\left[\\log \\left( p(y_n, Z_n | \\mu, \\sigma&#94;2, \\pi\\right) \\right].$$ If we expand the expectation a little, we get: \\begin{aligned} \\sum_{n=1}&#94;N\\mathbb{E}_{Z_n\\sim p(Z_n|Y_n, \\mu_{\\text{old}}, \\sigma&#94;2_{\\text{old}}, \\pi_{\\text{old}})}\\left[\\log \\left(p(y_n, Z_n | \\mu, \\sigma&#94;2, \\pi) \\right) \\right] &= \\sum_{n=1}&#94;N \\underbrace{\\sum_{n=1}&#94;K \\log \\left(\\underbrace{ p(y_n| Z_n=k, \\mu, \\sigma&#94;2) p(Z_n=k| \\pi)}_{\\text{factoring the joint }p(y_n, Z_n| \\ldots) } \\right) p(Z_n=k|y_n, \\theta_{\\text{old}}, \\phi_{\\text{old}})}_{\\text{expanding the expectation}}\\\\ &=\\sum_{n=1}&#94;N \\sum_{k=1}&#94;K \\underbrace{r_{n, k}}_{p(Z_n=k|y_n, \\theta_{\\text{old}}, \\phi_{\\text{old}})} \\left[\\log \\underbrace{\\mathcal{N}(y_n; \\mu_k, \\sigma&#94;2_k)}_{p(y_n| Z_n=k, \\mu, \\sigma&#94;2)} + \\log \\underbrace{\\pi_k}_{p(Z_n=k| \\pi)}\\right]\\\\ &= \\underbrace{\\sum_{n=1}&#94;N \\sum_{k=1}&#94;K r_{n, k} \\log \\mathcal{N}(y_n; \\mu_k, \\sigma&#94;2_k)}_{\\text{Term #1}} + \\underbrace{\\sum_{n=1}&#94;N \\sum_{k=1}&#94;K r_{n, k}\\pi_k}_{\\text{Term #2}} \\end{aligned} We can maximize each Term #1 and Term #2 individually. Example: EM for the Gaussian Mixture Model of Birth Weight Solving the M-Step We see that the optimization problem in the M-step: $\\mu_{\\text{new}}, \\sigma&#94;2_{\\text{new}}, \\pi_{\\text{new}} = \\underset{\\mu, \\sigma&#94;2, \\pi}{\\mathrm{argmax}}\\; ELBO(\\mu, \\sigma&#94;2, \\pi, q_{\\text{new}})$ is equivalent to two problems \\begin{aligned} &1.\\quad \\underset{\\mu, \\sigma&#94;2}{\\mathrm{argmax}}\\; \\sum_{n=1}&#94;N \\sum_{k=1}&#94;K r_{n, k} \\log \\mathcal{N}(y_n; \\mu_k, \\sigma&#94;2_k)\\\\ &2.\\quad \\underset{\\pi}{\\mathrm{argmax}}\\; \\sum_{n=1}&#94;N \\sum_{k=1}&#94;K r_{n, k}\\pi_k \\end{aligned} We can solve each optimization problem analytically by finding stationary points of the gradient (or the Lagrangian): $\\mu_{\\text{new}} = \\frac{1}{ \\sum_{n=1}&#94;N r_{n, k}} \\sum_{n=1}&#94;N r_{n, k} y_n$ $\\sigma&#94;2_{\\text{new}} = \\frac{1}{ \\sum_{n=1}&#94;N r_{n, k}} \\sum_{n=1}&#94;N r_{n, k} (y_n - \\mu_{\\text{new}})&#94;2$ $\\pi_{\\text{new}} = \\frac{\\sum_{n=1}&#94;N r_{n, k}}{N}$ Example: EM for the Gaussian Mixture Model of Birth Weight All Together Initialization: Pick any $\\pi$, $\\mu$, $\\sigma&#94;2$ E-Step: Compute $r_{n, k} = \\displaystyle\\frac{\\pi_{k, \\text{old}}\\,\\mathcal{N}(y_n; \\mu_{k, \\text{old}}, \\sigma&#94;2_{k, \\text{old}})}{\\mathcal{Z}}$, where $\\mathcal{Z} = \\sum_{k=1}&#94;K \\pi_{k, \\text{old}}\\,\\mathcal{N}(y_n; \\mu_{k, \\text{old}}, \\sigma&#94;2_{k, \\text{old}})$. M-Step: Compute model parameters: $\\mu_{\\text{new}} = \\frac{1}{ \\sum_{n=1}&#94;N r_{n, k}} \\sum_{n=1}&#94;N r_{n, k} y_n$ $\\sigma&#94;2_{\\text{new}} = \\frac{1}{ \\sum_{n=1}&#94;N r_{n, k}} \\sum_{n=1}&#94;N r_{n, k} (y_n - \\mu_{\\text{new}})&#94;2$ $\\pi_{\\text{new}} = \\frac{\\sum_{n=1}&#94;N r_{n, k}}{N}$ Implementing EM for the Gaussian Mixture Model of Birth Weight In [2]: #Generate data N = 2000 pis = [ 0.2 , 0.6 , 0.2 ] mus = [ 4.3 , 6 , 7.8 ] sigmas = [ 0.5 ** 2 , 0.7 ** 2 , 0.5 ** 2 ] K = 3 zs = np . random . choice ( np . arange ( K ), size = N , p = pis ) y = np . array ([ np . random . normal ( mus [ z ], sigmas [ z ] ** 0.5 , 1 )[ 0 ] for z in zs ]) #initialization mu_init = [ 2 , 4 , 5 ] sigma_init = [ 2. , 2. , 2. ] pi_init = [ 0.33 , 0.33 , 0.33 ] #implement EM mu_current = mu_init sigma_current = sigma_init pi_current = pi_init log_lkhd = [] total_iter = 1500 threshold = 1e-10 mu_diff = 1. pi_diff = 1. sigma_diff = 1. i = 0 while i < total_iter and mu_diff > threshold and pi_diff > threshold and sigma_diff > threshold : #E-step r_unnormalized = np . array ([( pi_current [ k ] * sp . stats . norm ( mu_current [ k ], sigma_current [ k ] ** 0.5 ) . pdf ( y )) for k in range ( K )]) . T r = r_unnormalized / r_unnormalized . sum ( axis = 1 ) . reshape (( - 1 , 1 )) #M-step mu_next = np . array ([ 1. / r [:, k ] . sum () * ( r [:, k ] * y ) . sum () for k in range ( K )]) sigma_next = np . array ([ 1. / r [:, k ] . sum () * ( r [:, k ] * ( y - mu_next [ k ]) ** 2 ) . sum () for k in range ( K )]) pi_next = r . sum ( axis = 0 ) / r . shape [ 0 ] #compute log observed likelihood if i % 100 == 0 : print ( 'iteration ' , i ) ll = 0 for n in range ( len ( y )): ll += np . log ( np . sum ([ sp . stats . norm ( mu_next [ k ], sigma_next [ k ] ** 0.5 ) . pdf ( y [ n ]) * pi_next [ k ] for k in range ( K )])) log_lkhd . append ( ll ) #convergence check mu_diff = np . linalg . norm ( mu_next - mu_current ) pi_diff = np . linalg . norm ( pi_next - pi_current ) sigma_diff = np . linalg . norm ( sigma_next - sigma_current ) #update parameters mu_current = mu_next sigma_current = sigma_next pi_current = pi_next i += 1 x = np . linspace ( y . min (), y . max (), 100 ) iteration 0 iteration 100 iteration 200 iteration 300 iteration 400 iteration 500 iteration 600 iteration 700 iteration 800 iteration 900 iteration 1000 iteration 1100 iteration 1200 iteration 1300 iteration 1400 In [3]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) ax . hist ( y , bins = 60 , density = True , color = 'gray' , alpha = 0.5 , label = 'histogram of birth weights' ) ax . plot ( x , pi_current [ 0 ] * sp . stats . norm ( mu_current [ 0 ], sigma_current [ 0 ] ** 0.5 ) . pdf ( x ), color = 'red' , label = 'First Gaussian' ) ax . plot ( x , pi_current [ 1 ] * sp . stats . norm ( mu_current [ 1 ], sigma_current [ 1 ] ** 0.5 ) . pdf ( x ), color = 'blue' , label = 'Second Gaussian' ) ax . plot ( x , pi_current [ 2 ] * sp . stats . norm ( mu_current [ 2 ], sigma_current [ 2 ] ** 0.5 ) . pdf ( x ), color = 'green' , label = 'Third Gaussian' ) ax . set_title ( 'GMM for Birth Weights' ) ax . legend ( loc = 'best' ) plt . show () Example: EM for Gaussian Mixture Models (Multivariate) Recall that our Gaussian mixture model, of $K$ number of Gaussians with means $\\mu = [\\mu_1, \\ldots, \\mu_K]$ and covariances $\\Sigma = [\\Sigma_1, \\ldots, \\Sigma_K]$, is defined as: \\begin{aligned} Z_n &\\sim Cat(\\pi),\\\\ Y_n &\\sim \\mathcal{N}(\\mu_{Z_n}, \\Sigma_{Z_n}), \\end{aligned} where $n=1, \\ldots, N$ and $\\sum_{k=1}&#94;K \\pi_k = 1$. We derive the updates for $\\pi$, $\\mu$ and $\\Sigma$ for the EM algorithm E-step: $$ q_{\\text{new}} = p(Z_n|y_n, \\pi_{\\text{old}}, \\mu_{\\text{old}}, \\Sigma_{\\text{old}}) = \\frac{p(y_n|Z_n, \\mu_{\\text{old}}, \\Sigma_{\\text{old}})p(Z_n|\\pi_{\\text{old}})}{\\int p(y_n|z_n, \\mu_{\\text{old}}, \\Sigma_{\\text{old}})p(z_n|\\pi_{\\text{old}}) dz_n} $$ Since $Z_n$ is a categorical variable, we compute the probability of $Z_n = k$ separately: $$ p(Z_n = k|y_n, \\pi_{\\text{old}}, \\mu_{\\text{old}}, \\Sigma_{\\text{old}}) = \\frac{p(y_n|Z_n = k, \\mu_{\\text{old}}, \\Sigma_{\\text{old}})p(Z_n=k | \\pi_{\\text{old}})}{\\sum_{k=1}&#94;K p(y|Z_n = k, \\mu_{\\text{old}}, \\Sigma_{\\text{old}})p(Z_n=k | \\pi_{\\text{old}})} = \\underbrace{\\frac{\\pi_{k, \\text{old}}\\,\\mathcal{N}(y_n; \\mu_{k, \\text{old}}, \\Sigma_{k, \\text{old}})}{\\mathcal{Z}}}_{r_{n, k}} $$ where $\\mathcal{Z} = \\sum_{k=1}&#94;K \\pi_{k, \\text{old}}\\,\\mathcal{N}(y_n; \\mu_{k, \\text{old}}, \\Sigma_{k, \\text{old}})$. Thus, $q_{\\text{new}}(Z_n)$ is a categorical distribution $Cat([r_{n, 1}, \\ldots, r_{n, K}])$. M-Step: \\begin{aligned} \\mu_{\\text{new}}, \\Sigma_{\\text{new}}, \\pi_{\\text{new}} &= \\underset{\\mu, \\Sigma, \\pi}{\\mathrm{argmax}}\\, \\sum_{n=1}&#94;N\\mathbb{E}_{Z_n\\sim p(Z_n|Y_n, \\mu_{\\text{old}}, \\Sigma_{\\text{old}}, \\pi_{\\text{old}})}\\left[\\log \\left( p(y_n, Z_n | \\mu, \\sigma \\right) \\right]\\\\ &= \\underset{\\mu, \\Sigma, \\pi}{\\mathrm{argmax}}\\,\\sum_{n=1}&#94;N \\sum_{k=1}&#94;K r_{n, k} \\left[\\log p(y_n | Z_n=k, \\mu, \\Sigma) + \\log p(Z_n=k | \\pi)\\right]\\\\ &= \\underset{\\mu, \\Sigma}{\\mathrm{argmax}}\\,\\sum_{n=1}&#94;N \\sum_{k=1}&#94;K r_{n, k} \\log p(y_n | Z_n=k, \\mu, \\Sigma) + \\underset{\\pi}{\\mathrm{argmax}}\\,\\sum_{n=1}&#94;N \\sum_{k=1}&#94;K r_{n, k} \\log p(Z_n=k | \\pi)\\\\ &=\\underset{\\mu, \\Sigma}{\\mathrm{argmax}}\\,\\sum_{n=1}&#94;N \\sum_{k=1}&#94;K r_{n, k} \\log \\mathcal{N}(y_n; \\mu_{k}, \\Sigma_{k}) + \\underset{\\pi}{\\mathrm{argmax}}\\,\\sum_{n=1}&#94;N \\sum_{k=1}&#94;K r_{n, k} \\log \\pi_k \\end{aligned} where $n=1, \\ldots, N$ and $\\sum_{k=1}&#94;K \\pi_k = 1$. We solve the two optimization problems separately. The optimization problem $$ \\underset{\\pi}{\\mathrm{argmax}}\\,\\sum_{n=1}&#94;N \\sum_{k=1}&#94;K r_{n, k} \\log \\pi_k,\\quad \\sum_{k=1}&#94;K \\pi_k = 1 $$ can be solved using Lagrangian multipliers yielding the solution: $$ \\pi_{\\text{new}, k} = \\frac{\\sum_{n=1}&#94;N r_{n, k}}{N} $$ The optimization problem $$ \\underset{\\mu, \\Sigma}{\\mathrm{argmax}}\\,\\sum_{n=1}&#94;N \\sum_{k=1}&#94;K r_{n, k} \\log \\mathcal{N}(y_n; \\mu_{k}, \\Sigma_{k}) $$ can be solved by taking the gradient with respect to $\\mu_k$, $\\Sigma_k$ for each $k$ and computing the stationary points of the gradient (remember to check for the global concavity to ensure you've found a global max). Doing so gives us the optimal points \\begin{aligned} \\mu_{\\text{new},k} &= \\frac{1}{\\sum_{n=1}&#94;N r_{n, k}} \\sum_{n=1}&#94;N r_{n,k}y_n, &\\quad (\\text{weighted sample mean})\\\\ \\Sigma_{\\text{new},k} &= \\frac{1}{\\sum_{n=1}&#94;N r_{n, k}} \\sum_{n=1}&#94;N r_{n,k} (y_n - \\mu_{\\text{new},k})(y_n - \\mu_{\\text{new},k})&#94;\\top, &\\quad (\\text{weighted sample covariance}) \\end{aligned} Exercise: Verify that the updates for $\\pi_{\\text{new},k}, \\mu_{\\text{new},k}, \\Sigma_{\\text{new},k}$ maximizes $\\mathbb{E}_{Z_n\\sim p(Z_n|Y_n, \\mu_{\\text{old}}, \\Sigma_{\\text{old}}, \\pi_{\\text{old}})}\\left[\\log \\left( p(y_n, Z_n | \\mu, \\sigma \\right) \\right]$. Sanity Check: Log-Likelihood During Training Remember that ploting the MLE model against actual data is not always an option (e.g. high-dimensional data). A sanity check for that your EM algorithm has been implemented correctly is to plot the observed data log-likelihood over the iterations of the algorithm: $$ \\ell_y(\\mu, \\sigma&#94;2, \\pi) = \\sum_{n=1}&#94;N \\log \\sum_{k=1}&#94;K \\mathcal{N}(y_n; \\mu_k, \\sigma_k&#94;2) \\pi_k $$ In [4]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 3 )) ax . plot ( range ( len ( log_lkhd )), log_lkhd , color = 'red' , alpha = 0.5 ) ax . set_title ( 'observed data log-likelihood over iterations of EM' ) plt . show () Expectation Maximization versus Gradient-based Optimization Pros of EM: No learning rates to adjust Don't need to worry about incorporating constraints (i.e. $p(Z_n|Y_n)$ is between 0 and 1) Each iteration is guaranteed to increase or maintain observed data log-likelihood Is guaranteed to converge to local optimum Can be very fast to converge (when parameters are fewer) Cons of EM: Can get stuck in local optima May not maximize observed data log-likelihood (the ELBO is just a lower bound) Requires you to do math - you need analytic solutions for E-step and M-step May be much slower than fancier gradient-based optimization Review of EM for Latent Variable Models Review: Latent Variable Models Models that include an observed variable $Y$ and at least one unobserved variable $Z$ are called latent variable models . In general, our model can allow $Y$ and $Z$ to interact in many different ways. We have studied models with one type of interaction: We treat the parameters $\\theta$ and $\\phi$ as unknown constants , and we estimate them from the observed data $y_1, \\ldots, y_N$. Example: Gaussian Mixture Models (GMMs) In a Gaussian Mixture Model (GMM) , we posit that the observed data $Y$ is generated by a mixture, $\\pi=[\\pi_1, \\ldots, \\pi_K]$, of $K$ number of Gaussians with means $\\mu = [\\mu_1, \\ldots, \\mu_K]$ and covariances $\\Sigma = [\\Sigma_1, \\ldots, \\Sigma_K]$. For each observation $Y_n$ the class of the observation $Z_n$ is a latent variable that indicates which of the $K$ Gaussian is responsible for generating $Y_n$: \\begin{aligned} Z_n &\\sim Cat(\\pi),\\\\ Y_n | Z_n&\\sim \\mathcal{N}(\\mu_{Z_n}, \\Sigma_{Z_n}), \\end{aligned} where $n=1, \\ldots, N$ and $\\sum_{k=1}&#94;K \\pi_k = 1$. GMMs are examples of model based clustering - breaking up a data set into natural clusters based on a statistical model fitted to the data. Inference for this model may mean that we want to learn the mean and covariance for each class in the mixture. Or we may want to infer the class membership $z_n$ for each observation $y_n$. Maximum Likelihood Estimate Inference for Latent Variable Models If we are interested in computing the maximum likelihood estimators of the parameters $\\theta$ and $\\phi$, we need to compute them with respect to the observed likelihood $p(y| \\theta, \\phi)$ - this is simply because we don't have access to the latent variable values, so we can't evaluate $p(y, z| \\theta, \\phi)$ given values for $\\theta$ and $\\phi$. Just like from before, we maximize the log-likelihood rather than the likelihood due to the simplifying properties of the log function: $$ \\theta&#94;*, \\phi&#94;* = \\underset{\\theta, \\phi}{\\text{argmax}}\\; \\ell_y(\\theta, \\phi) = \\underset{\\theta, \\phi}{\\text{argmax}}\\; \\log p(y| \\theta, \\phi) = \\underset{\\theta, \\phi}{\\text{argmax}}\\;\\log \\int p(y, z| \\theta, \\phi)\\, dz $$ Maximizing the the above requires taking a gradient, $$ \\nabla_{\\theta, \\phi} \\log \\int p(y, z| \\theta, \\phi)\\, dz $$ but it's not clear how to evaluate this expression. Rewriting the integral as an expectation, it turns out, illuminates the source of the problem: $$ \\nabla_{\\theta, \\phi} \\log \\int p(y, z| \\theta, \\phi)\\, dz = \\nabla_{\\theta, \\phi} \\log \\int p(y| z, \\phi)p(z|\\theta)\\, dz = \\nabla_{\\theta, \\phi} \\log \\mathbb{E}_{z\\sim p(z|\\theta)}[p(y| z, \\phi)] = \\frac{\\nabla_{\\theta, \\phi} \\mathbb{E}_{z\\sim p(z|\\theta)}[p(y| z, \\phi)]}{\\mathbb{E}_{z\\sim p(z|\\theta)}[p(y| z, \\phi)]},\\quad \\text{(chain rule)} $$ The above makes it clear that the gradient is not trivial to compute -- the gradient cannot be pushed into the expectation, since the distribution with respect to which we are taking the expectation depends on the optimization variable $\\theta$. To make the gradient computation easier, we make two changes: we introduce an auxiliary variable $q(z)$ so that we can replace $\\mathbb{E}_{z\\sim p(z|\\theta)}$ with $\\mathbb{E}_{z\\sim q(z)}$. Note then the latter expectation no longer depends on $\\theta$. we push the log inside the expectation using Jensen's inequality. That is, \\begin{aligned} \\ell_y(\\theta, \\phi) &= \\log \\int p(y, z| \\theta, \\phi)\\, dz\\\\ &= \\log \\int \\frac{p(y, z| \\theta, \\phi)}{q(z)}q(z)\\, dz\\\\ &= \\log \\mathbb{E}_{z\\sim q(z)}\\left[\\frac{p(y, z| \\theta, \\phi)}{q(z)}\\right]\\\\ &\\geq \\underbrace{\\mathbb{E}_{z\\sim q(z)} \\left[\\log\\left(\\frac{p(y, z| \\theta, \\phi)}{q(z)}\\right)\\right]}_{ELBO(\\theta, \\phi, q)} \\end{aligned} We have dervied that $ELBO(\\theta, \\phi, q)$ is a lower bound of the log-likelihood $\\ell_y(\\theta, \\phi)$, for any choice of $q$. So rather than maximizing the log-likelihood, we maximize the $ELBO(\\theta, \\phi, q)$, thus ensuring that $\\ell_y(\\theta, \\phi)$ is at least as big: $$ \\underset{\\theta, \\phi}{\\max}\\ell_y(\\theta, \\phi)\\geq \\underset{\\theta, \\phi, q}{\\max}ELBO(\\theta, \\phi, q) $$ In order to maximize the ELBO, we use coordinate ascent. That is, we take turns maximizing the ELBO with respect to $q$ and then with repect to $\\theta, \\phi$. This algorithm is called expectation maximization (EM) .","tags":"a-sections","url":"a-sections/a-sec01/notebook/"},{"title":"Advanced Section 2: Particle Filters / Sequential Monte Carlo","text":"Slides Particle Filters (PDF)","tags":"a-sections","url":"a-sections/a-sec02/"},{"title":"Advanced Section 3: Solvers","text":"Slides Solvers (PDF)","tags":"a-sections","url":"a-sections/a-sec03/"},{"title":"Advanced Section 4: Semantic Segmentation & Object Detecion","text":"Slides Semantic Segmentation & Object Detection (PDF)","tags":"a-sections","url":"a-sections/a-sec04/"},{"title":"Advanced Section 5: SOTA & Transfer Learning","text":"Slides SOTA (PDF) Transfer Learning (PDF)","tags":"a-sections","url":"a-sections/a-sec05/"},{"title":"Lecture 8: Neural Networks 1","text":"Slides Perceptron and Multilayer Perceptron (PDF) Anatomy of NN (PDF)","tags":"lectures","url":"lectures/lecture08/"},{"title":"Lecture 9: Neural Networks 2","text":"Slides Gradient Descent (PDF) Stochastic Gradient Descent Backpropagation","tags":"lectures","url":"lectures/lecture09/"},{"title":"Lecture 10: Optimizers","text":"Slides Optimizers (PDF) Bias Correction (PDF)","tags":"lectures","url":"lectures/lecture10/"},{"title":"Lecture 11: Regularization of NNs","text":"Slides Parameter Initialization (PDF) Regularization: L2 & L1 Norm and Early Stopping (PDF) Regularization: Data Augmentation & Dropout (PDF) Batch Norm (PDF)","tags":"lectures","url":"lectures/lecture11/"},{"title":"Lecture 12: CNNs - The Basics","text":"Slides Motivation and Basic Concepts for CNNs (PDF)","tags":"lectures","url":"lectures/lecture12/"},{"title":"Lecture 13: CNNs - Pooling and CNN Structure","text":"Slides Pooling and Convolution Structure (PDF)","tags":"lectures","url":"lectures/lecture13/"},{"title":"Lecture 14: CNNs 3","text":"Slides Backpropagation and Receptive Fields (PDF)","tags":"lectures","url":"lectures/lecture14/"},{"title":"Lecture 15: CNNs 4","text":"Slides Saliency Maps (PDF)","tags":"lectures","url":"lectures/lecture15/"},{"title":"Lab 4: FFNN","text":"Notebooks FFNN","tags":"labs","url":"labs/lab04/"},{"title":"Lab 04:","text":"CS109B Introduction to Data Science Lab 4: Feed Forward Neural Networks I Harvard University Spring 2022 Instructors : Mark Glickman & Pavlos Protopapas Lab Leaders : Marios Mattheakis & Chris Gumb The goal of this section is to become familiar with a basic Artificial Neural Network architecture, the Feed-Forward Neural Network (FFNN). Specifically, we will: Quickly review the FFNN anatomy . Design a simple FFNN from scratch (using numpy) and fit toy datasets. Quantitatively evaluate the prediction (fit) by using the mean square error (MSE) metric. Develop intuition for the FFNN as a universal approximator and understand this property by inspecting the functions generated by an FFNN. Use forward propagation with TensorFlow and Keras with our previous designs as well as more complex network architectures. PyTorch implementation (extra material) Import packages In [1]: import numpy as np import pandas as pd import matplotlib import matplotlib.pyplot as plt #### import tensorflow as tf # from sklearn.metrics import mean_squared_error 2022-02-18 01:23:37.420114: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /.singularity.d/libs 2022-02-18 01:23:37.420179: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. 1. Review of the ANN anatomy Input, Hidden Layers, and Output Layers The forward pass through an FFNN is a sequence of linear (affine) and nonlinear operations (activation). The Activation function is a nonlinear function. A list of activation functions can be found here . 2. Design a Feed Forward Neural Network Let's create a simple FFNN with one input neuron, one hidden layer of arbitrary number of neurons, and one linear neuron for the output layer. The purpose here is to become familiar with the forward propagation . Define the ReLU and Sigmoid nonlinear functions. These are two commonly used activation functions. Create an FFNN with one hidden neuron and become familiar with the activation function. Activity : Load the toyDataSet_1.csv and fit (manually tuning the weights). This is a simple regression problem with one input and one output. Write a function for the forward pass of a single input/output FFNN with a single hidden layer of arbitrary number of neurons. Tune the weights randomly and inspect the generated functions. Is this network a universal approximator ? Define activation functions Rectified Linear Unit (ReLU) function is defined as $$g(x)=\\max(0,x)$$ Sigmoid function is defined as $$\\sigma(x)=\\frac{1}{1+e&#94;{-z}}$$ In [2]: def g ( z : float ) -> float : return np . maximum ( 0 , z ) # or # g = lambda z: np.maximum(0, z) def sig ( z : float ) -> float : return 1 / ( 1 + np . exp ( - z )) Construct a FFNN with hard-coded parameters. No training is performed here. ReLU activation In [4]: # create an input vector x_train = np . linspace ( - 1 , 1 , 100 ) # set the network parameters w1 , b1 = 1 , 0. w2 , b2 = 1 , 0 #### HIDDEN LAYER #### # affine operation l1 = w1 * x_train + b1 # RELU activation h = g ( l1 ) # output linear layer y_train = w2 * h + b2 plt . plot ( x_train , y_train , '-b' ) plt . title ( 'ReLU Activation' ) plt . show () Sigmoid activation In [5]: # input vector x_train = np . linspace ( - 1 , 1 , 100 ) # set the network parameters w1 , b1 = 10 , 0. w2 , b2 = 1 , 0 #### HIDDEN LAYER #### # affine operation l1 = w1 * x_train + b1 # Sigmoid activation h = sig ( l1 ) # output linear layer y_train = w2 * h + b2 plt . plot ( x_train , y_train , '-b' ) plt . title ( 'Sigmoid Activation' ) plt . show () Plot a few cases to become familiar with the activation In [6]: #weights and biases that we want to explore. weight1, bias1, weight2, bias2 weights1 = 1 , 0 , 1 , 0 weights2 = 1 , 0.5 , 1 , 0 weights3 = 1 , 0.5 , 1 , - 0.5 weights4 = 1 , 0.5 , 4 , - .5 weights_list = [ weights1 , weights2 , weights3 , weights4 ] def simple_FFN ( w1 , b1 , w2 , b2 , activation ): \"\"\" Takes weights, biases, and an activation function and returns a simple prediction. Arguments: w1, w2: weights 1 and 2 b1, b2: biases 1 and 2 \"\"\" # linear transformation l1 = w1 * x_train + b1 #activation function + output linear layer y_pred = w2 * activation ( l1 ) + b2 return y_pred #make our plot plt . figure ( figsize = [ 12 , 8 ]) for i , w_list in enumerate ( weights_list ): #make our weight dictionary then feed the dictionary as arguments to the FFN to get a prediction. w_dict = dict ( zip ([ \"w1\" , \"b1\" , \"w2\" , \"b2\" ], w_list )) # print(w_dict) y_train_pred = simple_FFN ( ** w_dict , activation = g ) #make the plot plt . subplot ( 2 , 2 , i + 1 ) plt . plot ( x_train , y_train_pred , 'b' ) plt . ylim ([ - 1 , 1 ]) plt . xlim ([ - 1 , 1 ]) plt . title ( 'w1, b1, w2, b2 = {} ' . format ( w_list )) plt . ylabel ( \"y(x)\" ) plt . xlabel ( \"x\" ) plt . grid ( 'on' ) plt . tight_layout () Explore the sigmoid activation In [7]: #weights and biases that we want to explore. weight1, bias1, weight2, bias2 weights_1 = 10 , 0 , 1 , 0 weights_2 = 10 , 5 , 1 , 0 weights_3 = 10 , 5 , 1 , - .5 weights_4 = 10 , 5 , 2 , - .5 weights_list = [ weights_1 , weights_2 , weights_3 , weights_4 ] #make our plot plt . figure ( figsize = [ 12 , 8 ]) for i , w_list in enumerate ( weights_list ): #make our weight dictionary then feed the dictionary as arguments to the FFN to get a prediction. #note how we have changed the activation function to sigmoid. w_dict = dict ( zip ([ \"w1\" , \"b1\" , \"w2\" , \"b2\" ], w_list )) y_train_pred = simple_FFN ( ** w_dict , activation = sig ) #make the plot plt . subplot ( 2 , 2 , i + 1 ) plt . plot ( x_train , y_train_pred , 'b' ) plt . ylim ([ - 1 , 1.6 ]) plt . xlim ([ - 1 , 1 ]) plt . title ( 'w1, b1, w2, b2 = {} ' . format ( w_list )) plt . ylabel ( \"y(x)\" ) plt . xlabel ( \"x\" ) plt . grid ( 'on' ) plt . tight_layout () Activity 1 Design a simple FFNN to fit a simple dataset Load the toyDataSet_1.csv from the current directory. Write an FFNN with one hidden layer of one neuron and fit the data. Between ReLU and Sigmoid , choose which activation function works better Make a plot with the ground truth data and the prediction Write a custom mean square error (MSE) or use the sklearn mean_squared_error() to evaluate the prediction For this example, don't split the data into training and test sets. Just fit and evaluate the predictions on the entire set. In [8]: def plot_toyModels ( x_data , y_data , y_pred = None ): \"\"\" Generates a plot of the data, as well as the predictions when provided. Arguments: x_data: x from training data y_data: y from training data y_pred: predicted y values \"\"\" if type ( y_data ) != type ( None ): plt . plot ( x_data , y_data , 'or' , label = 'data' ) if type ( y_pred ) != type ( None ): plt . plot ( x_data , y_pred , '-b' , linewidth = 4 , label = 'FFNN' , alpha = .7 ) plt . xlabel ( 'x' ) plt . ylabel ( 'y(x)' ) plt . legend () def mean_squared_error ( y_true , y_network ): \"\"\" Calculate the MSE between the true values and the predicted. Arguments: y_true: actual y from data y_pred: predicted y from model \"\"\" # This can also be loaded from sklearn or tensorflow # e.g. sklearn.metrics.mean_squared_error return ( ( y_true - y_network ) ** 2 ) . mean () In [9]: toySet_1 = pd . read_csv ( 'toyDataSet_1.csv' ) x_train = toySet_1 [ 'x' ] . values . reshape ( - 1 , 1 ) y_train = toySet_1 [ 'y' ] . values . reshape ( - 1 , 1 ) plot_toyModels ( x_train , y_train ) In [9]: ## your code here # set the network parameters w1 = b1 = w2 = b2 = # affine operation l1 = # activation (Choose between ReLu or Sigmoid) h = # output linear layer y_model_train = # Make a plot (use the ploting function defined earlier) plot_toyModels ( x_train , y_train , y_model_train ) # Use MSE to evaluate the prediction mse_toy = print ( 'The MSE for the training set is ' , np . round ( mse_toy , 5 )) Input In [9] w1 = &#94; SyntaxError : invalid syntax In [10]: ###################### ## Using ReLU ###################### # set the network parameters w1 = b1 = w2 = b2 = # affine operation l1 = w1 * x_train + b1 # activation (Choose between ReLu or Sigmoid) h = g ( l1 ) # for relu # output linear layer y_model_train = w2 * h + b2 # Make a plot the plot_toyModels function plt . figure ( figsize = [ 12 , 4 ]) plt . subplot ( 1 , 2 , 1 ) plot_toyModels ( x_train , y_train , y_model_train ) plt . title ( 'ReLU activation' ) # Evaluate the prediction mse_toy = mean_squared_error ( y_train , y_model_train ) print ( 'ReLU: The MSE for the training set is ' , np . round ( mse_toy , 5 )) ###################### ## Using sigmoid ###################### w1 = b1 = w2 = b2 = # affine operation l1 = w1 * x_train + b1 # activation (Choose between ReLu or Sigmoid) h = sig ( l1 ) # for sigmoid # output linear layer y_model_train = w2 * h + b2 # Make a plot the plot_toyModels function plt . subplot ( 1 , 2 , 2 ) plot_toyModels ( x_train , y_train , y_model_train ) plt . title ( 'Sigmoid activation' ) # Evaluate the prediction mse_toy = mean_squared_error ( y_train , y_model_train ) print ( 'Sigmoid: The MSE for the training set is ' , np . round ( mse_toy , 5 )) Input In [10] w1 = &#94; SyntaxError : invalid syntax Why does the choice of ReLU activation give lower MSE than Sigmoid? A function for a more complex Forward Pass Let's write a function for the forward propagation through an FFNN with one input, one linear output neuron, and one hidden layers with arbitrary number of neurons. General Scheme: One input vector: $x$ $$$$ Affine (linear) transformation: $l_1$ where $w_{1},~b_{1}$ are the parameter vectors (or $w_{1i},~b_{1i}$): $$l_1 = \\sum_{i=1}&#94;\\text{\\# neurons} w_{1i}x+b_{1i} = w&#94;T_1 x + b_1 = w_1 \\cdot x + b_1 = W_1\\cdot X$$ $$$$ Activation function (nonlinear transformation): $g(\\cdot)$ $$h = g(l_1)$$ $$$$ Linear Output layer with a vector for weights $w_o$ and a scalar bias $b_o$: $$y = w_o&#94;T h+b_o = w_o \\cdot h + b_o = W_o\\cdot H$$ NOTE for the future: You can have a nonlinear output neuron(s) (e.g. for classification tasks) by just activating the above linear output, namely $$y = w_o&#94;T h+b_o = w_o \\cdot h + b_o = \\sigma(W_o\\cdot H)$$ In [11]: def myFFNN ( X , W1 , Wo , activation = 'relu' ): \"\"\" This function gives the forward pass of a simple feed forward neural network. The network propagates a single input X through a single hidden layer of weights W1, and return a single output (yhat) through a linear output layer with weights Wo. The input and the weight matrices should be given. Network specifications: input dimensions = 1 output dimensions = 1 hidden layers = 1 **hidden neurons are determined by the size of W1 or W0** Parameters: Design Matrix: X: the design matrix on which to make the predictions. weights vectors: W1 : parameters of first layer Wo : parameters of output layer activation: The default activation is the relu. It can be changed to sigmoid \"\"\" # Input: # add a constant column for the biases to the input vector X ones = np . ones (( len ( X ), 1 )) l1 = X l1 = np . append ( l1 , ones , axis = 1 ) # hidden layer: Affine and activation a1 = np . dot ( W1 , l1 . T ) if activation == 'relu' : h1 = g ( a1 ) elif activation == 'sigmoid' : h1 = sig ( a1 ) # Output layer (linear layer) (2 steps) # (a) Add a const column the h1 for the affine transformation ones = np . ones (( len ( X ), 1 )) H = np . append ( h1 . T , ones , axis = 1 ) . T # (b) Affine a = np . dot ( Wo , H ) y_hat = a . T return y_hat Use the previous parameters in our forward propagation function to fit the toyDataSet_1.csv. Plot the results and print the associated loss (the MSE) In [12]: w11 = 2 b11 = 0.0 w21 = 1 b21 = 0.5 # make the parameters matrices # First layer W1 = np . array ([[ w11 , b11 ]]) # Output Layer (only one bias term) Wo = np . array ([[ w21 , b21 ]]) # run the model y_model_1 = myFFNN ( x_train , W1 , Wo ) # plot the prediction and the ground truth plot_toyModels ( x_train , y_train , y_model_1 ) # quantify your prediction Loss_1 = mean_squared_error ( y_train , y_model_1 ) print ( 'MSE Loss = ' , np . round ( Loss_1 , 4 )) MSE Loss = 0.0023 Q: Can we easily can we generalize this network architecture to have many inputs, outputs, and hidden layers? A: No, we will need Tensor algebra, and it is much easier to use deep learning packages like TensorFlow and PyTorch for this process. FFNN is a Universal Function Approximator Here we will explore which functions can be generated using a single-hidden layer network with many neurons. It is proved that an FFNN can approximate with arbitrary accuracy any continuous function if the network has a sufficient number of hidden neurons. For a rigorous proof you can check the original paper . In [13]: # Two Neurons NNet w11 = - .8 b11 = - .1 w12 = .4 b12 = - .1 w21 = 1.3 w22 = - .8 b2 = 0.5 # First Layer W1 = np . array ([[ w11 , b11 ], [ w12 , b12 ]]) # Output Layer (only one bias term) Wo = np . array ([[ w21 , w22 , b2 ]]) # run the model y_model_p = myFFNN ( x_train , W1 , Wo , activation = 'relu' ) plot_toyModels ( x_train , y_data = None , y_pred = y_model_p ) In [14]: # Three Neurons NNet w11 = - .1 b11 = .3 w12 = .9 b12 = - .1 w13 = .7 b13 = - .2 w21 = - 1. w22 = - .7 w33 = .8 b2 = 0.25 # First Layer W1 = np . array ([[ w11 , b11 ], [ w12 , b12 ], [ w13 , b13 ]]) # Output Layer (only one bias term) Wo = np . array ([[ w21 , w22 , w33 , b2 ]]) # run the model y_model_p = myFFNN ( x_train , W1 , Wo ) # plot the prediction and the ground truth plot_toyModels ( x_train , y_data = None , y_pred = y_model_p ) plt . show () In [15]: # Random numbers between a,b # (b-a) * np.random.random_sample((4, 4)) + a a = - 20 b = 20 # N neurons N = 50 # Create random parameter matrices W1 = ( b - a ) * np . random . random_sample (( N , 2 )) + a Wo = ( b - a ) * np . random . random_sample (( 1 , N + 1 )) + a # make a bigger interval x_train_p2 = np . linspace ( - 2 , 2 , 1000 ) x_train_p2 = x_train_p2 . reshape ( - 1 , 1 ) ## run the models and plot the predictions plt . figure ( figsize = [ 12 , 4 ]) # # RELU ACTIVATION y_model_p2 = myFFNN ( x_train_p2 , W1 , Wo , activation = 'relu' ) plt . subplot ( 1 , 2 , 1 ) plot_toyModels ( x_train_p2 , y_data = None , y_pred = y_model_p2 ) plt . title ( 'Relu activation' ) # ## SIGMOID ACTIVATION y_model_p2 = myFFNN ( x_train_p2 , W1 , Wo , activation = 'sigmoid' ) plt . subplot ( 1 , 2 , 2 ) plot_toyModels ( x_train_p2 , y_data = None , y_pred = y_model_p2 ) plt . title ( 'Sigmoid activation' ) plt . show () Run the above cell multiple times to see how many different predictions the network can make. 3. TensorFlow and Keras Keras, Sequential: [Source] ( https://keras.io/models/sequential/ ) There are many powerful deep learning packages to work with neural networks like TensorFlow and PyTorch . These packages provide both the forward and back propagations, and many other other functionalities. The forward pass is used to make predictions while the backward (back propagation) is used to train (optimize) a network. Training means to find the optimal parameters for a specific task. Here, we use TensorFlow (TF) and Keras to employ FFNN. Use Keras to fit the simple toyDataSet_1 dataset. Tune the weights manually. Learn the Sequential method At the end of this notebook we have a PyTorch implementation of the same network architecture. Next Lab: We will learn how to use backpropagation (fit method) to find the optimal parameters. Import packages from keras In [16]: from tensorflow.keras import layers from tensorflow.keras import models from tensorflow.keras import optimizers Import the toyDataSet_1 and define the weights used in Activity 1 for the ReLU architecture In [17]: toySet_1 = pd . read_csv ( 'toyDataSet_1.csv' ) x_train = toySet_1 [ 'x' ] . values . reshape ( - 1 , 1 ) y_train = toySet_1 [ 'y' ] . values . reshape ( - 1 , 1 ) w1 = 2 b1 = 0.0 w2 = 1 b2 = 0.5 Use Keras to build a model similar to our myFFNN defined earlier Below we will use the Keras' package to first define a sequential model called Single_neurons_model_fixedWeights . We will then use the add method to add a single hidden layer to the model with one neuron using ReLU activation. In [18]: model = models . Sequential ( name = 'Single_neurons_model_fixedWeights' ) ### hidden layer with 1 neuron (or node) model . add ( layers . Dense ( 1 , activation = 'relu' , input_shape = ( 1 ,))) # output layer, one neuron model . add ( layers . Dense ( 1 , activation = 'linear' )) model . summary () Model: \"Single_neurons_model_fixedWeights\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 1) 2 dense_1 (Dense) (None, 1) 2 ================================================================= Total params: 4 Trainable params: 4 Non-trainable params: 0 _________________________________________________________________ 2022-02-18 01:25:08.274076: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /.singularity.d/libs 2022-02-18 01:25:08.274169: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303) 2022-02-18 01:25:08.274243: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (holy2a02104.rc.fas.harvard.edu): /proc/driver/nvidia/version does not exist 2022-02-18 01:25:08.275305: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Read and change the network parameters In [19]: def print_weights ( model ): \"\"\" A function that reads, prints and changes the model weights/biases. Arguments: model: a Tensorflow model object \"\"\" weights = model . get_weights () print ( dict ( zip ([ \"w1\" , \"b1\" , \"w2\" , \"b2\" ], [ weight . flatten ()[ 0 ] for weight in weights ]))) print ( 'Initial values of the parameters' ) print_weights ( model ) # MANUALLY SETTING THE WEIGHTS/BIASES # Read the model's weight with the get_weights method weights = model . get_weights () # Update the weights/biases of the hidden layer weights [ 0 ][ 0 ] = np . array ([ w1 ]) #weights weights [ 1 ] = np . array ([ b1 ]) # biases # Adjust the weights/biases for the output layer weights [ 2 ] = np . array ([[ w2 ]]) # weights weights [ 3 ] = np . array ([ b2 ]) # bias # Apply the new weights/bias to the model model . set_weights ( weights ) print ( ' \\n After setting the parameters' ) print_weights ( model ) Initial values of the parameters {'w1': -0.8445749, 'b1': 0.0, 'w2': -0.93036306, 'b2': 0.0} After setting the parameters {'w1': 2.0, 'b1': 0.0, 'w2': 1.0, 'b2': 0.5} In [20]: # Pass the data to the predict method to get predictions y_model_tf1 = model . predict ( x_train ) # Plot the true values and our predictions plot_toyModels ( x_train , y_train , y_pred = y_model_tf1 ) # Calculate the MSE of our predicitons Loss_tf1 = mean_squared_error ( y_train , y_model_tf1 ) print ( 'MSE Loss = ' , np . round ( Loss_tf1 , 4 )) MSE Loss = 0.0023 Construct a more complex architecture Here we will build a FFNN with one hidden layer containing 20 neurons. In [21]: # Define a new sequential model called Many_neurons model_2 = models . Sequential ( name = 'Many_neurons' ) # Add a dense layer with 20 neurons and ReLU activation model_2 . add ( layers . Dense ( 20 , activation = 'relu' , # kernel_initializer='random_normal', bias_initializer='random_uniform', input_shape = ( 1 , ))) # Add the output layer, still with a single neuron model_2 . add ( layers . Dense ( 1 , activation = 'linear' )) # Inspect the model model_2 . summary () ## Run a forward pass and plot the output function y_model_2 = model_2 . predict ( x_train ) plt . plot ( x_train , y_model_2 , linewidth = 4 , label = 'many neurons' , alpha = 1 ) plt . xlabel ( 'x' ) plt . ylabel ( 'y(x)' ) plt . legend () Model: \"Many_neurons\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_2 (Dense) (None, 20) 40 dense_3 (Dense) (None, 1) 21 ================================================================= Total params: 61 Trainable params: 61 Non-trainable params: 0 _________________________________________________________________ Out[21]: Build a model with multiple hidden layers Below we create a new model with three hidden layers, each with a different number of neurons and activations. In [22]: model_3 = models . Sequential ( name = 'Many_layers_neurons' ) #### First hidden layer model_3 . add ( layers . Dense ( 20 , activation = 'relu' , kernel_initializer = 'random_uniform' , bias_initializer = 'random_uniform' , input_shape = ( 1 ,))) #### Second hidden layer model_3 . add ( layers . Dense ( 100 , activation = 'tanh' , kernel_initializer = 'random_normal' , bias_initializer = 'random_uniform' )) #### Third hidden layer model_3 . add ( layers . Dense ( 50 , activation = 'relu' , kernel_initializer = 'random_normal' , bias_initializer = 'random_uniform' )) # Add the output layer and check the summary model_3 . add ( layers . Dense ( 1 , activation = 'linear' )) model_3 . summary () # Define a new input vector # Notice that the architecture does not depend on the size of the input vector x_train_3 = np . linspace ( - 10 , 10 , 1500 ) ## Run a forward pass and plot the output function y_model_3 = model_3 . predict ( x_train_3 ) plt . plot ( x_train_3 , y_model_3 , linewidth = 4 , label = 'many layers with many neurons' , alpha = 1 ) plt . xlabel ( 'x' ) plt . ylabel ( 'y(x)' ) plt . legend () Model: \"Many_layers_neurons\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_4 (Dense) (None, 20) 40 dense_5 (Dense) (None, 100) 2100 dense_6 (Dense) (None, 50) 5050 dense_7 (Dense) (None, 1) 51 ================================================================= Total params: 7,241 Trainable params: 7,241 Non-trainable params: 0 _________________________________________________________________ Out[22]: Q: How many parameters does our Many_layers_neurons model have? Does it sound like a good idea to manually tune them? Stay Tuned... In the next Lab we will learn how to find the optimal parameters! 4. PyTorch Implementation: Extra Material PyTorch is another library used to build, train, and deploy deep learning models. Most course material will be performed using Tensorflow, but it is good to know about another common library you might encounter. A great PyTorch tutorial for an FFNN implementation can be found at: https://pytorch.org/tutorials/beginner/pytorch_with_examples.html In [24]: import torch dtype = torch . float In PyTorch we have to define everything as torch tensors instead of numpy matrices Though, we can transform numpy objects to pytorch tensors by using: torch.from_numpy(np_array) In [25]: # Create an input vector using PyTorch x = torch . linspace ( - 10 , 10 , 1000 ) print ( 'PyTorch tensor: ' , x . size ()) # Note how this is nearly the same as using Numpy x_ = np . linspace ( - 10 , 10 , 1000 ) print ( 'Numpy array: ' , x_ . shape ) # Convert a numpy array to a PyTorch tensor x = torch . from_numpy ( x_ ) print ( 'from numpy to PyTorch: ' , x . size ()) PyTorch tensor: torch.Size([1000]) Numpy array: (1000,) from numpy to PyTorch: torch.Size([1000]) Define the input tensor: Specify the type Reshape or unsqueeze. This is what pytorch wants to get In [26]: x = torch . linspace ( - 10 , 10 , 1000 , dtype = dtype ) print ( 'PyTorch Tensor: ' , x . size ()) # x=x.reshape(-1,1) x = x . unsqueeze ( - 1 ) print ( 'Reshaped tensor: ' , x . size ()) PyTorch Tensor: torch.Size([1000]) Reshaped tensor: torch.Size([1000, 1]) PyTorch also provides a sequential functionality Build the network architecture In [27]: model_p = torch . nn . Sequential ( torch . nn . Linear ( 1 , 10 ), torch . nn . ReLU (), torch . nn . Linear ( 10 , 1 )) print ( model_p ) Sequential( (0): Linear(in_features=1, out_features=10, bias=True) (1): ReLU() (2): Linear(in_features=10, out_features=1, bias=True) ) Forward pass To perform a forward pass in PyTorch, you pass the data directly to the model as an argument. In [28]: y_p = model_p ( x ) Trying to operate tensors may cause problems. For example, try to plot them In [29]: # plt.plot(x, y_p) We can convert PyTorch tensors to numpy by using: torchTensor.detach().numpy() In [30]: x_ = x . detach () . numpy () y_ = y_p . detach () . numpy () plt . plot ( x_ , y_ , linewidth = 4 , label = 'My First PyTorch model' ) plt . xlabel ( 'x' ) plt . ylabel ( 'y(x)' ) plt . legend () Out[30]: A deeper network In [31]: model_p2 = torch . nn . Sequential ( torch . nn . Linear ( 1 , 10 ), torch . nn . ReLU (), torch . nn . Linear ( 10 , 20 ), torch . nn . Sigmoid (), torch . nn . Linear ( 20 , 10 ), torch . nn . Tanh (), torch . nn . Linear ( 10 , 1 ) ) y_p2 = model_p2 ( x ) x2_ = x . detach () . numpy () y2_ = y_p2 . detach () . numpy () plt . plot ( x2_ , y2_ , linewidth = 4 , label = 'My First Deep PyTorch model' ) plt . xlabel ( 'x' ) plt . ylabel ( 'y(x)' ) plt . legend () Out[31]: End of Section In [ ]:","tags":"labs","url":"labs/lab04/notebook/"},{"title":"Lab 5: FFNN 2","text":"Notebooks Lab 5 - FFNN 2","tags":"labs","url":"labs/lab05/"},{"title":"Lab 5: FFNN 2","text":"CS109A Introduction to Data Science Lab 5: Feed Forward Neural Networks 2 (Training, Evaluation, & Interogation) Harvard University Spring 2022 Instructors : Mark Glickman & Pavlos Protopapas Lab Team : Eleni Kaxiras, Marios Mattheakis, Chris Gumb, Shivas Jayaram In [1]: #RUN THIS CELL import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: Table of Contents Building a NN /w Keras Quick Review Learning weights from Data Evaluating a Keras Model Inspecting Training History Multi-class Classification Example (Tabular Data) Interpreting Our Black Box NN Bagging Review Image Classification Example In [2]: import pandas as pd import matplotlib.pyplot as plt import numpy as np Let's revisit the toy dataset from the first NN lab. In [3]: # Load toy data toydata = pd . read_csv ( 'data/toyDataSet_1.csv' ) x_toy = toydata [ 'x' ] . values . reshape ( - 1 , 1 ) y_toy = toydata [ 'y' ] . values . reshape ( - 1 , 1 ) In [4]: # Plot toy data ax = plt . gca () ax . scatter ( x_toy , y_toy ) ax . set_xlabel ( 'x' ) ax . set_ylabel ( 'y' ) ax . set_title ( 'Toy Dataset' ); In [5]: from tensorflow.keras import models , layers , activations from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense , Input # Input \"layer\"?!? Here we construct a sequential Keras model with one Dense hidden layer containing only a single neuron with a relu activation.\\ The output layer will be of size 1 and have no activation (e.g., 'linear'). In [6]: # Instantiate sequential Keras model and give it a name toy_model = Sequential ( name = 'toy_model' ) # Despite designation in Keras, Input is not a true layer # It only specifies the shape of the input toy_model . add ( Input ( shape = ( 1 ,))) # hidden layer with 1 neurons (or nodes) toy_model . add ( Dense ( 1 , activation = 'relu' )) # output layer, one neuron toy_model . add ( Dense ( 1 , activation = 'linear' )) toy_model . summary () Model: \"toy_model\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 1) 2 _________________________________________________________________ dense_1 (Dense) (None, 1) 2 ================================================================= Total params: 4 Trainable params: 4 Non-trainable params: 0 _________________________________________________________________ 2022-02-25 08:14:41.077294: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set 2022-02-25 08:14:41.077488: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: SSE4.1 SSE4.2 AVX AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-02-25 08:14:41.077795: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. Compiling the NN model.compile(optimizer, loss, metrics, **kwargs) optimizer - defines how the weights are updated (we'll use SGD)\\ loss - what the model is trying to minimize\\ metric - list of metrics to report during training process compile is used to configure a NN model be for it can be fit. We aren't ready to fit just yet, but we are compiling here because doing so reinitilizes the model weights. We are going to manually set our weights before training so we need to to the compilation first. Q: Why do I want metrics if I already have a loss? In [7]: import tensorflow as tf from tensorflow.keras import optimizers , losses , metrics from tensorflow.keras.optimizers import SGD from tensorflow.keras.losses import mse In [8]: toy_model . compile ( optimizer = SGD ( learning_rate = 1e-1 ), loss = 'mse' , metrics = []) A little nudge... Our toy model is very simply. It only has 4 weights. But the problem there are only 4 possible weight values that would make this a good fit. That is like finding a needle in a haystack. So we will cheat a bit and initialize our weights in the 'neighborhood' of the true weights which generated the data. Our future models will be complex enough that they won't need to worry about finding a specific combination of weights: some local minima (but not all) will do the job just fine. In [9]: # A FUNCTION THAT READS AND PRINTS OUT THE MODEL WEIGHTS/BIASES def print_weights ( model ): weights = model . get_weights () print ( dict ( zip ([ \"w1\" , \"b1\" , \"w2\" , \"b2\" ], [ weight . flatten ()[ 0 ] for weight in weights ]))) # MANUALLY SETTING THE WEIGHTS/BIASES ## True weights from data generating function # w1 = 2 # b1 = 0.0 # w2 = 1 # b2 = 0.5 # Initialize weights to that 'neighborhood' w1 = 1.85 b1 = - 0.5 w2 = 0.9 b2 = 0.4 # Store current weight data structure weights = toy_model . get_weights () # hidden layer weights [ 0 ][ 0 ] = np . array ([ w1 ]) #weights weights [ 1 ] = np . array ([ b1 ]) # biases # output layer weights [ 2 ] = np . array ([[ w2 ]]) # weights weights [ 3 ] = np . array ([ b2 ]) # bias # hidden layer # Set the weights toy_model . set_weights ( weights ) print ( 'Manually Initialized Weights:' ) print_weights ( toy_model ) Manually Initialized Weights: {'w1': 1.85, 'b1': -0.5, 'w2': 0.9, 'b2': 0.4} Forward Pass Review Input, Hidden Layers, and Output Layers The forward pass through an FFNN is a sequence of linear (affine) and nonlinear operations (activation). We use the model's predict method to execut the forward pass with a linspace spanning the range of the x data as input. In [10]: # Predict x_lin = np . linspace ( x_toy . min (), x_toy . max (), 500 ) y_hat = toy_model . predict ( x_lin ) # Plot plt . scatter ( x_toy , y_toy , alpha = 0.4 , lw = 4 , label = 'data' ) plt . plot ( x_lin , y_hat , label = 'NN' , ls = '--' , c = 'r' ) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . title ( 'Predictions with Manually Set Weights' ) plt . legend (); 2022-02-25 08:14:41.292267: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2) 2022-02-25 08:14:41.311287: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 1800000000 Hz We'll let back propogation and stochastic gradient descent take it from here. Back Propogation & SGD Review The backward pass is the training. It is based on the chain rule of calculus, and it calculates the gradient of the loss w.r.t. the weights. This gradient is used by the optimizer to update the weights to minimize the loss function. Batching, stochastic gradient descent, and epochs Shuffle and partition the dataset in mini-batches to help escape from local minima. Each batch is seen once per epoch. And thus each observation is also seen once per epoch. We can train the network for as many epochs as we like. Reproducibility There is a lot of stochasticity in the training of neural networks, from weight initilizations, to shuffling of data between epochs.\\ Below is some code that appears to be working for me to get reproducible results. Though I think some of the steps taken may be purely superstitious. In [11]: # Advice gleaned from: https://deeplizard.com/learn/video/HcW0DeWRggs import os import random as rn os . environ [ 'PYTHONHASHSEED' ] = '0' os . environ [ 'CUDA_VISIBLE_DEVICES' ] = '' tf . random . set_seed ( 109 ) np . random . seed ( 109 ) rn . seed ( 109 ) Fitting the NN Model.fit(x=None, y=None, batch_size=None, epochs=1, verbose=\"auto\", validation_split=0.0, validation_data=None, shuffle=True, **kwargs) batch_size - number of observations overwhich the loss is calculated before each weight update\\ epochs - number of times the complete dataset is seen in the fitting process\\ verbose - you can silence the training output by setting this to 0 \\ validation_split - splits off a portion of the x and y training data to be used as validation (see warning below)\\ validation_data - tuple designating a seperate x_val and y_val dataset\\ shuffle - whether to shuffle the training data before each epoch We fit the model for 100 epochs and set batch_size to 64. The results of fit() are then stored in a variable called history . In [12]: # Fit model and store training histry history = toy_model . fit ( x_toy , y_toy , epochs = 100 , batch_size = 64 , verbose = 1 ) Epoch 1/100 1/1 [==============================] - 0s 361ms/step - loss: 0.2432 Epoch 2/100 1/1 [==============================] - 0s 24ms/step - loss: 0.1465 Epoch 3/100 1/1 [==============================] - 0s 22ms/step - loss: 0.0889 Epoch 4/100 1/1 [==============================] - 0s 8ms/step - loss: 0.0558 Epoch 5/100 1/1 [==============================] - 0s 16ms/step - loss: 0.0375 Epoch 6/100 1/1 [==============================] - 0s 6ms/step - loss: 0.0271 Epoch 7/100 1/1 [==============================] - 0s 6ms/step - loss: 0.0213 Epoch 8/100 1/1 [==============================] - 0s 15ms/step - loss: 0.0179 Epoch 9/100 1/1 [==============================] - 0s 9ms/step - loss: 0.0158 Epoch 10/100 1/1 [==============================] - 0s 11ms/step - loss: 0.0141 Epoch 11/100 1/1 [==============================] - 0s 5ms/step - loss: 0.0129 Epoch 12/100 1/1 [==============================] - 0s 15ms/step - loss: 0.0120 Epoch 13/100 1/1 [==============================] - 0s 11ms/step - loss: 0.0112 Epoch 14/100 1/1 [==============================] - 0s 13ms/step - loss: 0.0105 Epoch 15/100 1/1 [==============================] - 0s 15ms/step - loss: 0.0099 Epoch 16/100 1/1 [==============================] - 0s 14ms/step - loss: 0.0094 Epoch 17/100 1/1 [==============================] - 0s 13ms/step - loss: 0.0090 Epoch 18/100 1/1 [==============================] - 0s 17ms/step - loss: 0.0086 Epoch 19/100 1/1 [==============================] - 0s 18ms/step - loss: 0.0083 Epoch 20/100 1/1 [==============================] - 0s 14ms/step - loss: 0.0080 Epoch 21/100 1/1 [==============================] - 0s 14ms/step - loss: 0.0077 Epoch 22/100 1/1 [==============================] - 0s 13ms/step - loss: 0.0074 Epoch 23/100 1/1 [==============================] - 0s 8ms/step - loss: 0.0071 Epoch 24/100 1/1 [==============================] - 0s 12ms/step - loss: 0.0069 Epoch 25/100 1/1 [==============================] - 0s 9ms/step - loss: 0.0067 Epoch 26/100 1/1 [==============================] - 0s 14ms/step - loss: 0.0065 Epoch 27/100 1/1 [==============================] - 0s 18ms/step - loss: 0.0063 Epoch 28/100 1/1 [==============================] - 0s 15ms/step - loss: 0.0062 Epoch 29/100 1/1 [==============================] - 0s 6ms/step - loss: 0.0060 Epoch 30/100 1/1 [==============================] - 0s 20ms/step - loss: 0.0059 Epoch 31/100 1/1 [==============================] - 0s 20ms/step - loss: 0.0057 Epoch 32/100 1/1 [==============================] - 0s 6ms/step - loss: 0.0056 Epoch 33/100 1/1 [==============================] - 0s 9ms/step - loss: 0.0055 Epoch 34/100 1/1 [==============================] - 0s 8ms/step - loss: 0.0054 Epoch 35/100 1/1 [==============================] - 0s 8ms/step - loss: 0.0053 Epoch 36/100 1/1 [==============================] - 0s 12ms/step - loss: 0.0052 Epoch 37/100 1/1 [==============================] - 0s 4ms/step - loss: 0.0051 Epoch 38/100 1/1 [==============================] - 0s 13ms/step - loss: 0.0050 Epoch 39/100 1/1 [==============================] - 0s 6ms/step - loss: 0.0049 Epoch 40/100 1/1 [==============================] - 0s 6ms/step - loss: 0.0048 Epoch 41/100 1/1 [==============================] - 0s 5ms/step - loss: 0.0048 Epoch 42/100 1/1 [==============================] - 0s 4ms/step - loss: 0.0047 Epoch 43/100 1/1 [==============================] - 0s 5ms/step - loss: 0.0046 Epoch 44/100 1/1 [==============================] - 0s 5ms/step - loss: 0.0046 Epoch 45/100 1/1 [==============================] - 0s 3ms/step - loss: 0.0045 Epoch 46/100 1/1 [==============================] - 0s 6ms/step - loss: 0.0044 Epoch 47/100 1/1 [==============================] - 0s 6ms/step - loss: 0.0044 Epoch 48/100 1/1 [==============================] - 0s 11ms/step - loss: 0.0043 Epoch 49/100 1/1 [==============================] - 0s 4ms/step - loss: 0.0043 Epoch 50/100 1/1 [==============================] - 0s 5ms/step - loss: 0.0042 Epoch 51/100 1/1 [==============================] - 0s 6ms/step - loss: 0.0041 Epoch 52/100 1/1 [==============================] - 0s 4ms/step - loss: 0.0041 Epoch 53/100 1/1 [==============================] - 0s 7ms/step - loss: 0.0040 Epoch 54/100 1/1 [==============================] - 0s 6ms/step - loss: 0.0040 Epoch 55/100 1/1 [==============================] - 0s 4ms/step - loss: 0.0039 Epoch 56/100 1/1 [==============================] - 0s 6ms/step - loss: 0.0039 Epoch 57/100 1/1 [==============================] - 0s 5ms/step - loss: 0.0038 Epoch 58/100 1/1 [==============================] - 0s 5ms/step - loss: 0.0038 Epoch 59/100 1/1 [==============================] - 0s 10ms/step - loss: 0.0037 Epoch 60/100 1/1 [==============================] - 0s 8ms/step - loss: 0.0037 Epoch 61/100 1/1 [==============================] - 0s 6ms/step - loss: 0.0036 Epoch 62/100 1/1 [==============================] - 0s 5ms/step - loss: 0.0036 Epoch 63/100 1/1 [==============================] - 0s 5ms/step - loss: 0.0036 Epoch 64/100 1/1 [==============================] - 0s 9ms/step - loss: 0.0035 Epoch 65/100 1/1 [==============================] - ETA: 0s - loss: 0.003 - 0s 5ms/step - loss: 0.0035 Epoch 66/100 1/1 [==============================] - 0s 8ms/step - loss: 0.0035 Epoch 67/100 1/1 [==============================] - 0s 11ms/step - loss: 0.0034 Epoch 68/100 1/1 [==============================] - 0s 7ms/step - loss: 0.0034 Epoch 69/100 1/1 [==============================] - 0s 6ms/step - loss: 0.0034 Epoch 70/100 1/1 [==============================] - 0s 14ms/step - loss: 0.0033 Epoch 71/100 1/1 [==============================] - 0s 8ms/step - loss: 0.0033 Epoch 72/100 1/1 [==============================] - 0s 9ms/step - loss: 0.0033 Epoch 73/100 1/1 [==============================] - 0s 9ms/step - loss: 0.0032 Epoch 74/100 1/1 [==============================] - 0s 7ms/step - loss: 0.0032 Epoch 75/100 1/1 [==============================] - 0s 14ms/step - loss: 0.0032 Epoch 76/100 1/1 [==============================] - 0s 8ms/step - loss: 0.0032 Epoch 77/100 1/1 [==============================] - 0s 11ms/step - loss: 0.0031 Epoch 78/100 1/1 [==============================] - 0s 9ms/step - loss: 0.0031 Epoch 79/100 1/1 [==============================] - 0s 11ms/step - loss: 0.0031 Epoch 80/100 1/1 [==============================] - 0s 9ms/step - loss: 0.0031 Epoch 81/100 1/1 [==============================] - 0s 8ms/step - loss: 0.0030 Epoch 82/100 1/1 [==============================] - 0s 6ms/step - loss: 0.0030 Epoch 83/100 1/1 [==============================] - 0s 5ms/step - loss: 0.0030 Epoch 84/100 1/1 [==============================] - 0s 4ms/step - loss: 0.0030 Epoch 85/100 1/1 [==============================] - 0s 8ms/step - loss: 0.0030 Epoch 86/100 1/1 [==============================] - 0s 8ms/step - loss: 0.0029 Epoch 87/100 1/1 [==============================] - 0s 8ms/step - loss: 0.0029 Epoch 88/100 1/1 [==============================] - 0s 10ms/step - loss: 0.0029 Epoch 89/100 1/1 [==============================] - 0s 19ms/step - loss: 0.0029 Epoch 90/100 1/1 [==============================] - 0s 7ms/step - loss: 0.0029 Epoch 91/100 1/1 [==============================] - 0s 15ms/step - loss: 0.0029 Epoch 92/100 1/1 [==============================] - 0s 7ms/step - loss: 0.0028 Epoch 93/100 1/1 [==============================] - 0s 5ms/step - loss: 0.0028 Epoch 94/100 1/1 [==============================] - 0s 11ms/step - loss: 0.0028 Epoch 95/100 1/1 [==============================] - 0s 7ms/step - loss: 0.0028 Epoch 96/100 1/1 [==============================] - 0s 8ms/step - loss: 0.0028 Epoch 97/100 1/1 [==============================] - 0s 6ms/step - loss: 0.0028 Epoch 98/100 1/1 [==============================] - 0s 8ms/step - loss: 0.0028 Epoch 99/100 1/1 [==============================] - 0s 15ms/step - loss: 0.0028 Epoch 100/100 1/1 [==============================] - 0s 4ms/step - loss: 0.0027 Plot Training History history.history is a dictionary which contains information from each training epoch (no, I don't know the rationale behind the double name). Use it to plot the loss across epochs. Don't for get those labels! In [13]: # Plot training history plt . plot ( history . history [ 'loss' ], c = 'r' ) plt . ylabel ( 'MSE loss' ) plt . xlabel ( 'epoch' ) plt . title ( 'NN Training History' ); In [14]: # Weights learned for the data print_weights ( toy_model ) {'w1': 1.987246, 'b1': -0.051335987, 'w2': 1.0406225, 'b2': 0.5044429} We can see we've moved much closer to the original weights after fitting. But visualizing our model's predictions will make this more clear. Predict & Plot We use the model's predict method on a linspace, x_lin , which we construct to span the range of the dataset's $x$ values. We save the resulting predictions in y_hat In [15]: # Predict x_lin = np . linspace ( x_toy . min (), x_toy . max (), 500 ) y_hat = toy_model . predict ( x_lin ) # Plot plt . scatter ( x_toy , y_toy , alpha = 0.4 , lw = 4 , label = 'data' ) plt . plot ( x_lin , y_hat , label = 'NN' , ls = '--' , c = 'r' ) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . title ( 'Predictions After Training' ) plt . legend (); Much better! But perhaps you are not impressed yet? An Ugly Function In [16]: def ugly_function ( x ): if x < 0 : return np . exp ( - ( x ** 2 )) / 2 + 1 + np . exp ( - (( 10 * x ) ** 2 )) else : return np . exp ( - ( x ** 2 )) + np . exp ( - (( 10 * x ) ** 2 )) How do you feel about the prospect of manually setting the weights to approximate this beauty? In [17]: # Generate data x_ugly = np . linspace ( - 3 , 3 , 1500 ) # create x-values for input y_ugly = np . array ( list ( map ( ugly_function , x_ugly ))) # Plot data plt . plot ( x_ugly , y_ugly ); plt . title ( 'An Ugly Function' ) plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ); And here we don't even have the option of cheating by initializing our weights strategically! 🏋🏻‍♂️ TEAM ACTIVITY: We're Gonna Need a Bigger Model... Complete the build_nn function for quickly constructing different NN architectures. Use build_nn to construct an NN to approximate the ugly function Compile the model & print its summary Tip: Remember, if it is the last line of the cell, Jupyter will display the return value without an explicit call to print() required. In fact, Jupyter uses its own display() function which often results in prettier output for tables Fit the model Plot the training history Hyperparameters to play with: Architecture Number of hidden layers Number of neurons in each hidden layer Hidden layers' activation function Training SGD 's learning_rate batch_size epochs NN Build Function \\ Arguments: name : str - A name for your NN. input_shape : tuple - number of predictors in input (remember the trailing ','!) hidden_dims : list of int - specifies the number of neurons in each hidden layer Ex: [2,4,8] would mean 3 hidden layers with 2, 4, and 8 neurons respectively hidden_act : str (or Keras activation object) - activation function used by all hidden layers out_dim : int - number of output neurons a.k.a 'ouput units' out_act : str (or Keras activation object) - activation function used by output layer Hint: We will reuse this function throughout the notebook in different settings, but you should go ahead and set some sensible defaults for all of the arguments. In [18]: # your code here def build_NN ( name = 'NN' , input_shape = ( 1 ,), hidden_dims = [ 2 ], hidden_act = 'relu' , out_dim = 1 , out_act = 'linear' ): model = Sequential ( name = name ) model . add ( Input ( shape = input_shape )) for hidden_dim in hidden_dims : model . add ( Dense ( hidden_dim , activation = hidden_act )) model . add ( Dense ( out_dim , activation = out_act )) return model In [19]: # %load ../solutions/sol1_1.py def build_NN ( name = 'NN' , input_shape = ( 1 ,), hidden_dims = [ 2 ], hidden_act = 'relu' , out_dim = 1 , out_act = 'linear' ): model = Sequential ( name = name ) model . add ( Input ( shape = input_shape )) for hidden_dim in hidden_dims : model . add ( Dense ( hidden_dim , activation = hidden_act )) model . add ( Dense ( out_dim , activation = out_act )) return model Build & Print Model Summary You can play with hidden_dims and hidden_act . In [20]: # your code here ugly_model = build_NN ( name = 'ugly' , hidden_dims = [ 64 , 32 , 16 , 8 ]) ugly_model . summary () Model: \"ugly\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_2 (Dense) (None, 64) 128 _________________________________________________________________ dense_3 (Dense) (None, 32) 2080 _________________________________________________________________ dense_4 (Dense) (None, 16) 528 _________________________________________________________________ dense_5 (Dense) (None, 8) 136 _________________________________________________________________ dense_6 (Dense) (None, 1) 9 ================================================================= Total params: 2,881 Trainable params: 2,881 Non-trainable params: 0 _________________________________________________________________ Compile \\ Use the SGD optimizer and 'mse' as your loss.\\ You can expermiment with SGD 's learning_rate . In [21]: # Compile # your code here ugly_model . compile ( optimizer = SGD ( learning_rate = 1e-1 ), loss = 'mse' ) Fit \\ Fit ugly_model on x_ugly and y_ugly and story the results in a variable called history .\\ You can experiment with epochs and batch_size . In [22]: # Fit # your code here history = ugly_model . fit ( x_ugly , y_ugly , epochs = 100 , batch_size = 32 ) Epoch 1/100 47/47 [==============================] - 1s 6ms/step - loss: 0.1613 Epoch 2/100 47/47 [==============================] - 0s 3ms/step - loss: 0.0156 Epoch 3/100 47/47 [==============================] - 0s 3ms/step - loss: 0.0127 Epoch 4/100 47/47 [==============================] - 0s 3ms/step - loss: 0.0106 Epoch 5/100 47/47 [==============================] - 0s 3ms/step - loss: 0.0088 Epoch 6/100 47/47 [==============================] - 0s 3ms/step - loss: 0.0089 Epoch 7/100 47/47 [==============================] - 0s 3ms/step - loss: 0.0075 Epoch 8/100 47/47 [==============================] - 0s 3ms/step - loss: 0.0069 Epoch 9/100 47/47 [==============================] - 0s 3ms/step - loss: 0.0069 Epoch 10/100 47/47 [==============================] - 0s 4ms/step - loss: 0.0062 Epoch 11/100 47/47 [==============================] - 0s 3ms/step - loss: 0.0059 Epoch 12/100 47/47 [==============================] - 0s 3ms/step - loss: 0.0053 Epoch 13/100 47/47 [==============================] - 0s 5ms/step - loss: 0.0045 Epoch 14/100 47/47 [==============================] - 0s 2ms/step - loss: 0.0055 Epoch 15/100 47/47 [==============================] - 0s 3ms/step - loss: 0.0042 Epoch 16/100 47/47 [==============================] - 0s 7ms/step - loss: 0.0034 Epoch 17/100 47/47 [==============================] - 0s 5ms/step - loss: 0.0031 Epoch 18/100 47/47 [==============================] - 0s 5ms/step - loss: 0.0043 Epoch 19/100 47/47 [==============================] - 0s 4ms/step - loss: 0.0039 Epoch 20/100 47/47 [==============================] - 0s 4ms/step - loss: 0.0037 Epoch 21/100 47/47 [==============================] - 0s 4ms/step - loss: 0.0044 Epoch 22/100 47/47 [==============================] - 0s 4ms/step - loss: 0.0037 Epoch 23/100 47/47 [==============================] - 0s 5ms/step - loss: 0.0040 Epoch 24/100 47/47 [==============================] - 0s 5ms/step - loss: 0.0034 Epoch 25/100 47/47 [==============================] - 0s 5ms/step - loss: 0.0030 Epoch 26/100 47/47 [==============================] - 0s 4ms/step - loss: 0.0032 Epoch 27/100 47/47 [==============================] - 0s 5ms/step - loss: 0.0024 Epoch 28/100 47/47 [==============================] - 0s 5ms/step - loss: 0.0026 Epoch 29/100 47/47 [==============================] - 0s 7ms/step - loss: 0.0030 Epoch 30/100 47/47 [==============================] - 0s 7ms/step - loss: 0.0023 Epoch 31/100 47/47 [==============================] - 0s 6ms/step - loss: 0.0025 Epoch 32/100 47/47 [==============================] - 0s 8ms/step - loss: 0.0022 Epoch 33/100 47/47 [==============================] - 0s 7ms/step - loss: 0.0024 Epoch 34/100 47/47 [==============================] - 0s 4ms/step - loss: 0.0021 Epoch 35/100 47/47 [==============================] - 0s 6ms/step - loss: 0.0017 Epoch 36/100 47/47 [==============================] - 0s 8ms/step - loss: 0.0021 Epoch 37/100 47/47 [==============================] - 0s 5ms/step - loss: 0.0023 Epoch 38/100 47/47 [==============================] - 0s 4ms/step - loss: 0.0022 Epoch 39/100 47/47 [==============================] - 0s 6ms/step - loss: 0.0023 Epoch 40/100 47/47 [==============================] - 0s 7ms/step - loss: 0.0017 Epoch 41/100 47/47 [==============================] - 0s 3ms/step - loss: 0.0021 Epoch 42/100 47/47 [==============================] - 0s 4ms/step - loss: 0.0017 Epoch 43/100 47/47 [==============================] - 0s 5ms/step - loss: 0.0031 Epoch 44/100 47/47 [==============================] - 0s 6ms/step - loss: 0.0015 Epoch 45/100 47/47 [==============================] - 0s 6ms/step - loss: 0.0014 Epoch 46/100 47/47 [==============================] - 0s 6ms/step - loss: 0.0015 Epoch 47/100 47/47 [==============================] - 0s 5ms/step - loss: 0.0013 Epoch 48/100 47/47 [==============================] - 0s 7ms/step - loss: 0.0026 Epoch 49/100 47/47 [==============================] - 0s 6ms/step - loss: 0.0017 Epoch 50/100 47/47 [==============================] - 0s 5ms/step - loss: 0.0011 Epoch 51/100 47/47 [==============================] - 0s 6ms/step - loss: 0.0014 Epoch 52/100 47/47 [==============================] - 0s 6ms/step - loss: 0.0027 Epoch 53/100 47/47 [==============================] - 0s 4ms/step - loss: 0.0030 Epoch 54/100 47/47 [==============================] - 0s 7ms/step - loss: 9.6525e-04 Epoch 55/100 47/47 [==============================] - 0s 10ms/step - loss: 0.0014 Epoch 56/100 47/47 [==============================] - 0s 7ms/step - loss: 0.0019 Epoch 57/100 47/47 [==============================] - 0s 6ms/step - loss: 9.6049e-04 Epoch 58/100 47/47 [==============================] - 0s 9ms/step - loss: 0.0014 Epoch 59/100 47/47 [==============================] - 0s 8ms/step - loss: 9.2362e-04 Epoch 60/100 47/47 [==============================] - 0s 7ms/step - loss: 0.0027 Epoch 61/100 47/47 [==============================] - 0s 5ms/step - loss: 6.6775e-04 Epoch 62/100 47/47 [==============================] - 0s 6ms/step - loss: 0.0018 Epoch 63/100 47/47 [==============================] - 0s 6ms/step - loss: 7.6751e-04 Epoch 64/100 47/47 [==============================] - 0s 8ms/step - loss: 8.1408e-04 Epoch 65/100 47/47 [==============================] - 0s 6ms/step - loss: 6.6690e-04 Epoch 66/100 47/47 [==============================] - 0s 6ms/step - loss: 9.3049e-04 Epoch 67/100 47/47 [==============================] - 0s 10ms/step - loss: 0.0014 Epoch 68/100 47/47 [==============================] - 0s 8ms/step - loss: 0.0019 Epoch 69/100 47/47 [==============================] - 0s 10ms/step - loss: 0.0025 Epoch 70/100 47/47 [==============================] - 0s 6ms/step - loss: 0.0012 Epoch 71/100 47/47 [==============================] - 0s 7ms/step - loss: 8.4905e-04 Epoch 72/100 47/47 [==============================] - 0s 5ms/step - loss: 9.3626e-04 Epoch 73/100 47/47 [==============================] - 0s 3ms/step - loss: 0.0012 Epoch 74/100 47/47 [==============================] - 0s 5ms/step - loss: 7.5984e-04 Epoch 75/100 47/47 [==============================] - 0s 5ms/step - loss: 7.9879e-04 Epoch 76/100 47/47 [==============================] - 0s 5ms/step - loss: 8.6573e-04 Epoch 77/100 47/47 [==============================] - 0s 4ms/step - loss: 7.4727e-04 Epoch 78/100 47/47 [==============================] - 0s 4ms/step - loss: 0.0014 Epoch 79/100 47/47 [==============================] - 1s 11ms/step - loss: 0.0011 Epoch 80/100 47/47 [==============================] - 0s 5ms/step - loss: 5.1671e-04 Epoch 81/100 47/47 [==============================] - 0s 5ms/step - loss: 4.6552e-04 Epoch 82/100 47/47 [==============================] - 0s 5ms/step - loss: 5.5920e-04 Epoch 83/100 47/47 [==============================] - 0s 6ms/step - loss: 0.0011 Epoch 84/100 47/47 [==============================] - 0s 7ms/step - loss: 7.2296e-04 Epoch 85/100 47/47 [==============================] - 0s 6ms/step - loss: 7.0101e-04 Epoch 86/100 47/47 [==============================] - 0s 6ms/step - loss: 5.1551e-04 Epoch 87/100 47/47 [==============================] - 0s 8ms/step - loss: 0.0012 Epoch 88/100 47/47 [==============================] - 0s 5ms/step - loss: 7.8156e-04 Epoch 89/100 47/47 [==============================] - 0s 5ms/step - loss: 9.7620e-04 Epoch 90/100 47/47 [==============================] - 0s 4ms/step - loss: 0.0016 Epoch 91/100 47/47 [==============================] - 0s 5ms/step - loss: 4.8010e-04 Epoch 92/100 47/47 [==============================] - 0s 5ms/step - loss: 5.3750e-04 Epoch 93/100 47/47 [==============================] - 0s 5ms/step - loss: 5.6266e-04 Epoch 94/100 47/47 [==============================] - 0s 6ms/step - loss: 9.4611e-04 Epoch 95/100 47/47 [==============================] - 0s 5ms/step - loss: 9.4847e-04 Epoch 96/100 47/47 [==============================] - 0s 6ms/step - loss: 3.8596e-04 Epoch 97/100 47/47 [==============================] - 0s 5ms/step - loss: 7.5726e-04 Epoch 98/100 47/47 [==============================] - 0s 4ms/step - loss: 4.1207e-04 Epoch 99/100 47/47 [==============================] - 0s 5ms/step - loss: 7.5537e-04 Epoch 100/100 47/47 [==============================] - 0s 4ms/step - loss: 0.0012 Plot Training History Plot the model's training history. Don't forget your axis labels!\\ Hint: Remember, fit returns a history object which itself has a history dictionary attribute. Because this (2nd object) is a dictionary, you can always use its keys method if you don't know what's in it. You can also access the history from the model itself. Ex: ugly_model.history.history In [23]: # Plot History # your code here plt . plot ( history . history [ 'loss' ]) plt . xlabel ( 'epoch' ) plt . ylabel ( 'Train MSE' ); Get Predictions \\ Similar to sklearn models, keras models have a predict method. Use your model's predict method to predict on x_ugly and store the results in a variable called y_hat_ugly . In [24]: # Predict y_hat_ugly = ugly_model . predict ( x_ugly ) Plot Predictions \\ Run the cell below to compare your model's predictions to the true (ugly) function. Still not quite right? Try tweaking some of the hyperparameters above and re-run the cells in this section to see if you can improve. In [25]: # Plot predictions plt . plot ( x_ugly , y_ugly , alpha = 0.4 , lw = 4 , label = 'true function' ) plt . plot ( x_ugly , y_hat_ugly , label = 'NN' , ls = '--' , c = 'r' ) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . legend (); End of Team Activity Multi-class Classification with Keras In [26]: import seaborn as sns So far we've only used our new Keras powers for toy regression problems. We'll move to classification next... but with 3 classes! This example will use seaborn 's penguins dataset (a worthy successor to the connonical iris dataset.) We'll build a model to identify a penguin's species from its other features. In the process we'll dust off our Python skills with a quick run through a basic model building workflow. In [27]: # Bring on the penguins! penguins = sns . load_dataset ( 'penguins' ) penguins . head () Out[27]: species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex 0 Adelie Torgersen 39.1 18.7 181.0 3750.0 Male 1 Adelie Torgersen 39.5 17.4 186.0 3800.0 Female 2 Adelie Torgersen 40.3 18.0 195.0 3250.0 Female 3 Adelie Torgersen NaN NaN NaN NaN NaN 4 Adelie Torgersen 36.7 19.3 193.0 3450.0 Female We have 3 species of penguins living across 3 different islands. There are measurements of bill length, bill depth, flipper length, and body mass. We also have categorcial variable for each penguin's sex giving us a total of 7 features. Here's a plot that tries to show too much at once. But you can ignore the marker shapes and sizes. The bill and flipper length alone ($x$ and $y$ axes) seem too already provide a fair amount of information about the species (color). In [28]: # Plot penguins with too much info sns . relplot ( data = penguins , x = 'flipper_length_mm' , y = 'bill_length_mm' , hue = 'species' , style = 'sex' , size = 'body_mass_g' , height = 6 ); plt . title ( 'Penguins!' , fontdict = { 'color' : 'teal' , 'size' : 20 , 'weight' : 'bold' , 'family' : 'serif' }); You may have noticed some pesky NaN s when we displayed the beginning of the DataFrame.\\ We should investigate further. Missingness In [29]: # How many missing values in each column? penguins . isna () . sum () Out[29]: species 0 island 0 bill_length_mm 2 bill_depth_mm 2 flipper_length_mm 2 body_mass_g 2 sex 11 dtype: int64 Let's take a look at them first all the rows with missing data. In [30]: # Rows with missingness penguins [ penguins . isna () . any ( axis = 1 )] Out[30]: species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex 3 Adelie Torgersen NaN NaN NaN NaN NaN 8 Adelie Torgersen 34.1 18.1 193.0 3475.0 NaN 9 Adelie Torgersen 42.0 20.2 190.0 4250.0 NaN 10 Adelie Torgersen 37.8 17.1 186.0 3300.0 NaN 11 Adelie Torgersen 37.8 17.3 180.0 3700.0 NaN 47 Adelie Dream 37.5 18.9 179.0 2975.0 NaN 246 Gentoo Biscoe 44.5 14.3 216.0 4100.0 NaN 286 Gentoo Biscoe 46.2 14.4 214.0 4650.0 NaN 324 Gentoo Biscoe 47.3 13.8 216.0 4725.0 NaN 336 Gentoo Biscoe 44.5 15.7 217.0 4875.0 NaN 339 Gentoo Biscoe NaN NaN NaN NaN NaN Yikes! There are two observations where all predictors except species and island are missing.\\ These rows won't be of any use to us. We see that dropping rows missing body_mass_g will take care of most our missingness. In [31]: # Drop the bad rows identified above penguins = penguins . dropna ( subset = [ 'body_mass_g' ]) # Check state of missingness after dropping penguins . isna () . sum () Out[31]: species 0 island 0 bill_length_mm 0 bill_depth_mm 0 flipper_length_mm 0 body_mass_g 0 sex 9 dtype: int64 It looks like there are 9 rows where sex is missing. We can try to impute these values.\\ But first, take a look at our DataFrame again. In [32]: penguins . head () Out[32]: species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex 0 Adelie Torgersen 39.1 18.7 181.0 3750.0 Male 1 Adelie Torgersen 39.5 17.4 186.0 3800.0 Female 2 Adelie Torgersen 40.3 18.0 195.0 3250.0 Female 4 Adelie Torgersen 36.7 19.3 193.0 3450.0 Female 5 Adelie Torgersen 39.3 20.6 190.0 3650.0 Male Notice how the indices go from 2 to 4 . What happened to 3 ?\\ It was one of the rows we dropped! This issue with the indices can cause headaches later on (think loc / iloc distinction). But we can make things good as new using the reset_index method. Just be sure to set drop=True , otherwise the old indices will be added to the DataFrame as a new column. In [33]: # Reset index penguins = penguins . reset_index ( drop = True ) penguins . head () Out[33]: species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex 0 Adelie Torgersen 39.1 18.7 181.0 3750.0 Male 1 Adelie Torgersen 39.5 17.4 186.0 3800.0 Female 2 Adelie Torgersen 40.3 18.0 195.0 3250.0 Female 3 Adelie Torgersen 36.7 19.3 193.0 3450.0 Female 4 Adelie Torgersen 39.3 20.6 190.0 3650.0 Male Much better!\\ Now, on to imputing the missing sex values. Let's take a look at the value_counts . In [34]: # Counts of each unique value in the dataset penguins . sex . value_counts () Out[34]: Male 168 Female 165 Name: sex, dtype: int64 It's almost an even split. We'll impute the mode because it's a quick fix. In [35]: # The mode here should match the value with the most counts above sex_mode = penguins . sex . mode ()[ 0 ] sex_mode Out[35]: 'Male' Finally, we use fillna to replace the remaining NaN s with the sex_mode and confirm that there are no more missing values in the DataFrame. In [36]: # Replace missing values with most common value (i.e, mode) penguins = penguins . fillna ( sex_mode ) penguins . isna () . sum () Out[36]: species 0 island 0 bill_length_mm 0 bill_depth_mm 0 flipper_length_mm 0 body_mass_g 0 sex 0 dtype: int64 Q: Imputing the mode here was very easy, but does this approach make you a bit nervous? Why? Is there some other way we could have imputed this values? Preprocessing We can't just throw this DataFrame at a neural network as it is. There's some work we need to do first. Separate predictors from response variable In [37]: # Isolate response from predictors response = 'species' X = penguins . drop ( response , axis = 1 ) y = penguins [ response ] Encode Categorical Predictor Variables In [38]: # Check the predictor data types X . dtypes Out[38]: island object bill_length_mm float64 bill_depth_mm float64 flipper_length_mm float64 body_mass_g float64 sex object dtype: object Both island and sex are categotical. We can use pd.get_dummies to one-hot-encode them (don't forget to drop_first !). In [39]: # Identify the categorical columns cat_cols = [ 'island' , 'sex' ] In [40]: # one-hot encode the categorical columns X_design = pd . get_dummies ( X , columns = cat_cols , drop_first = True ) X_design . head ( 1 ) Out[40]: bill_length_mm bill_depth_mm flipper_length_mm body_mass_g island_Dream island_Torgersen sex_Male 0 39.1 18.7 181.0 3750.0 0 1 1 From the remaining columns we can infer that the 'reference' values for our categorical variables are island = 'Biscoe' , and sex = 'Female' . Feature Scaling We should take a closer look at the range of values our predictors take on. In [41]: # Summary stats of predictors X_design . describe () Out[41]: bill_length_mm bill_depth_mm flipper_length_mm body_mass_g island_Dream island_Torgersen sex_Male count 342.000000 342.000000 342.000000 342.000000 342.000000 342.000000 342.000000 mean 43.921930 17.151170 200.915205 4201.754386 0.362573 0.149123 0.517544 std 5.459584 1.974793 14.061714 801.954536 0.481447 0.356731 0.500424 min 32.100000 13.100000 172.000000 2700.000000 0.000000 0.000000 0.000000 25% 39.225000 15.600000 190.000000 3550.000000 0.000000 0.000000 0.000000 50% 44.450000 17.300000 197.000000 4050.000000 0.000000 0.000000 1.000000 75% 48.500000 18.700000 213.000000 4750.000000 1.000000 0.000000 1.000000 max 59.600000 21.500000 231.000000 6300.000000 1.000000 1.000000 1.000000 Our features are not on the same scale. Just compare the min/max of bill_depth_mm and body_mass_g for example.\\ This can slow down neural network training for reasons we'll see in an upcoming lecture. Let's make use of sklearn 's StandardScaler to standardize the data, centering each predictor at 0 and setting their standard deviations to 1. In [42]: from sklearn.preprocessing import StandardScaler In [43]: # Remember the column names for later; we'll lose them when we scale X_cols = X_design . columns # Saving the scaler object in a variable allows us to reverse the transformation later scaler = StandardScaler () X_scaled = scaler . fit_transform ( X_design ) In [44]: # The scaler was passed a pandas DataFrame but returns a numpy array type ( X_scaled ), X_scaled . shape Out[44]: (numpy.ndarray, (342, 7)) In [45]: # We can always add the column names back later if we need to pd . DataFrame ( X_scaled , columns = X_cols ) . head ( 3 ) Out[45]: bill_length_mm bill_depth_mm flipper_length_mm body_mass_g island_Dream island_Torgersen sex_Male 0 -0.884499 0.785449 -1.418347 -0.564142 -0.754193 2.388699 0.965507 1 -0.811126 0.126188 -1.062250 -0.501703 -0.754193 2.388699 -1.035725 2 -0.664380 0.430462 -0.421277 -1.188532 -0.754193 2.388699 -1.035725 Encoding the Response Variable In [46]: # Take a look at our response y Out[46]: 0 Adelie 1 Adelie 2 Adelie 3 Adelie 4 Adelie ... 337 Gentoo 338 Gentoo 339 Gentoo 340 Gentoo 341 Gentoo Name: species, Length: 342, dtype: object Our response variable is still a string . We need to turn it into some numerical representation for our neural network.\\ We could to this ourselves with a few list comprehensions, but sklearn 's LabelEncoder makes this very easy. In [47]: from sklearn.preprocessing import LabelEncoder In [48]: # Encode string labels as integers # LabelEncoder uses the familar fit/transform methods we saw with StandardScaler labenc = LabelEncoder () . fit ( y ) y_enc = labenc . transform ( y ) y_enc Out[48]: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) In [49]: # We can recover the class labels from the encoder object later labenc . classes_ Out[49]: array(['Adelie', 'Chinstrap', 'Gentoo'], dtype=object) This gets us part of the way there. But the penguin species are categorical not ordinal . Keeping the labels as integers implies that species 2 is twice as \"different\" from species 1 as it is from species 0 . We want to perform a conversion here similar to the one-hot encoding above, except will will not 'drop' one of the values. This is where Keras's to_categorical utility function comes in. In [50]: from tensorflow.keras.utils import to_categorical In [51]: y_cat = to_categorical ( y_enc ) y_cat Out[51]: array([[1., 0., 0.], [1., 0., 0.], [1., 0., 0.], ..., [0., 0., 1.], [0., 0., 1.], [0., 0., 1.]], dtype=float32) Perfect! Q: If this is what our array of response variables looks like, what will this mean for the output layer of our neural network? Train-test Split In [52]: from sklearn.model_selection import train_test_split You may be familiar with using train_test_split to split the X and y arrays themselves. But here we will using it to create a set of train and test indices . We'll see later that being able to determine which rows in the original X and y ended up in train or test will be helpful. Q: But couldn't we just sample integers to get random indices? Why use train_test_split ? A: Because train_test_split allows for stratified splitting! Here we use a trick to stratify on both the sex and island variables by concatinating their values together. This gives us a total of 6 possible values (2 sexs x 3 islands). By stratifying on this column we help ensure that each of the 6 possible sex/island combinations is equally represented in both train and test. In [53]: # Concatenate categorical columns; use this for stratified splitting strat_col = penguins [ 'sex' ] . astype ( 'str' ) + penguins [ 'island' ] . astype ( 'str' ) strat_col Out[53]: 0 MaleTorgersen 1 FemaleTorgersen 2 FemaleTorgersen 3 FemaleTorgersen 4 MaleTorgersen ... 337 FemaleBiscoe 338 FemaleBiscoe 339 MaleBiscoe 340 FemaleBiscoe 341 MaleBiscoe Length: 342, dtype: object In [54]: # Create train/test indices train_idx , test_idx = train_test_split ( np . arange ( X_scaled . shape [ 0 ]), test_size = 0.5 , random_state = 109 , stratify = strat_col ) In [55]: # Index into X_scaled and y_cat to create the train and test sets X_train = X_scaled [ train_idx ] y_train = y_cat [ train_idx ] X_test = X_scaled [ test_idx ] y_test = y_cat [ test_idx ] In [56]: # Sanity check on the resulting shapes X_train . shape , y_train . shape , X_test . shape , y_test . shape Out[56]: ((171, 7), (171, 3), (171, 7), (171, 3)) Validation Split Here is where those indices we saved come in handy.\\ We also want to also ensure equal representation across train and validation. In [57]: # Subset original stratify column using saved train split indices strat_col2 = strat_col . iloc [ train_idx ] strat_col2 . shape Out[57]: (171,) In [58]: # Create train and validation splits from original train split X_train , X_val , y_train , y_val = train_test_split ( X_train , y_train , test_size = 0.5 , random_state = 109 , stratify = strat_col2 ) 🏋🏻‍♂️ TEAM ACTIVITY: Classify Those Penguins! In [59]: from tensorflow.keras.losses import categorical_crossentropy from tensorflow.keras.metrics import Accuracy , AUC from tensorflow.keras.activations import softmax Build Construct your NN penguin classifier. You can make use of your build_NN function from earlier. What output activation should you use? Hint: try to programaticlaly determin the input and output shape from your data rather than hard coding those values. In [60]: # Construct your NN and print the model summary # your code here model = build_NN ( name = 'penguins' , input_shape = ( X_train . shape [ 1 ],), hidden_dims = [ 8 , 16 , 3 ], hidden_act = 'relu' , out_dim = y_train . shape [ 1 ], out_act = 'softmax' ) model . summary () Model: \"penguins\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_7 (Dense) (None, 8) 64 _________________________________________________________________ dense_8 (Dense) (None, 16) 144 _________________________________________________________________ dense_9 (Dense) (None, 3) 51 _________________________________________________________________ dense_10 (Dense) (None, 3) 12 ================================================================= Total params: 271 Trainable params: 271 Non-trainable params: 0 _________________________________________________________________ In [111]: # %load solutions/sol2_1.py Compile Again, let's use SGD as our optimizer. You can fiddle with the learning_rate .\\ What loss and metric(s) do you think are appropriate? In [61]: # Compile # youre code here model . compile ( optimizer = SGD ( learning_rate = 1e-1 ), loss = 'categorical_crossentropy' , metrics = [ 'acc' , 'AUC' ]) Fit Fit your model and store the results in a variable called history .\\ Feel free to play with batch_size and epochs . Don't forget to include the validation_data ! In [62]: # Fit # your code here history = model . fit ( X_train , y_train , validation_data = ( X_val , y_val ), batch_size = 64 , epochs = 50 ) Epoch 1/50 2/2 [==============================] - 2s 865ms/step - loss: 0.9754 - acc: 0.5718 - auc: 0.7584 - val_loss: 0.8847 - val_acc: 0.7674 - val_auc: 0.8688 Epoch 2/50 2/2 [==============================] - 0s 205ms/step - loss: 0.8840 - acc: 0.6737 - auc: 0.8183 - val_loss: 0.7838 - val_acc: 0.7907 - val_auc: 0.9042 Epoch 3/50 2/2 [==============================] - 0s 270ms/step - loss: 0.7871 - acc: 0.7023 - auc: 0.8596 - val_loss: 0.7291 - val_acc: 0.8256 - val_auc: 0.9338 Epoch 4/50 2/2 [==============================] - 0s 245ms/step - loss: 0.7558 - acc: 0.7520 - auc: 0.8943 - val_loss: 0.6760 - val_acc: 0.8372 - val_auc: 0.9456 Epoch 5/50 2/2 [==============================] - 0s 262ms/step - loss: 0.7123 - acc: 0.7468 - auc: 0.9075 - val_loss: 0.6318 - val_acc: 0.8372 - val_auc: 0.9528 Epoch 6/50 2/2 [==============================] - 0s 232ms/step - loss: 0.6749 - acc: 0.7624 - auc: 0.9190 - val_loss: 0.5994 - val_acc: 0.8372 - val_auc: 0.9634 Epoch 7/50 2/2 [==============================] - 0s 248ms/step - loss: 0.6420 - acc: 0.7520 - auc: 0.9370 - val_loss: 0.5717 - val_acc: 0.8372 - val_auc: 0.9696 Epoch 8/50 2/2 [==============================] - 0s 238ms/step - loss: 0.6048 - acc: 0.7624 - auc: 0.9466 - val_loss: 0.5456 - val_acc: 0.8372 - val_auc: 0.9737 Epoch 9/50 2/2 [==============================] - 0s 228ms/step - loss: 0.6017 - acc: 0.7546 - auc: 0.9455 - val_loss: 0.5160 - val_acc: 0.8372 - val_auc: 0.9757 Epoch 10/50 2/2 [==============================] - 0s 243ms/step - loss: 0.5585 - acc: 0.7858 - auc: 0.9554 - val_loss: 0.4985 - val_acc: 0.8372 - val_auc: 0.9783 Epoch 11/50 2/2 [==============================] - 0s 214ms/step - loss: 0.5421 - acc: 0.7858 - auc: 0.9623 - val_loss: 0.4837 - val_acc: 0.8372 - val_auc: 0.9804 Epoch 12/50 2/2 [==============================] - 0s 274ms/step - loss: 0.5393 - acc: 0.7702 - auc: 0.9609 - val_loss: 0.4614 - val_acc: 0.8372 - val_auc: 0.9816 Epoch 13/50 2/2 [==============================] - 0s 226ms/step - loss: 0.5135 - acc: 0.7754 - auc: 0.9664 - val_loss: 0.4427 - val_acc: 0.8372 - val_auc: 0.9827 Epoch 14/50 2/2 [==============================] - 0s 261ms/step - loss: 0.5025 - acc: 0.7754 - auc: 0.9681 - val_loss: 0.4285 - val_acc: 0.8372 - val_auc: 0.9835 Epoch 15/50 2/2 [==============================] - 0s 208ms/step - loss: 0.4784 - acc: 0.7754 - auc: 0.9704 - val_loss: 0.4116 - val_acc: 0.8372 - val_auc: 0.9841 Epoch 16/50 2/2 [==============================] - 0s 236ms/step - loss: 0.4766 - acc: 0.7546 - auc: 0.9667 - val_loss: 0.3894 - val_acc: 0.8372 - val_auc: 0.9847 Epoch 17/50 2/2 [==============================] - 0s 260ms/step - loss: 0.4635 - acc: 0.7598 - auc: 0.9688 - val_loss: 0.3737 - val_acc: 0.8372 - val_auc: 0.9854 Epoch 18/50 2/2 [==============================] - 0s 233ms/step - loss: 0.4464 - acc: 0.7650 - auc: 0.9709 - val_loss: 0.3609 - val_acc: 0.8372 - val_auc: 0.9861 Epoch 19/50 2/2 [==============================] - 0s 239ms/step - loss: 0.4409 - acc: 0.7494 - auc: 0.9679 - val_loss: 0.3445 - val_acc: 0.8372 - val_auc: 0.9863 Epoch 20/50 2/2 [==============================] - 0s 233ms/step - loss: 0.3985 - acc: 0.7754 - auc: 0.9744 - val_loss: 0.3323 - val_acc: 0.8372 - val_auc: 0.9866 Epoch 21/50 2/2 [==============================] - 0s 251ms/step - loss: 0.4041 - acc: 0.7546 - auc: 0.9698 - val_loss: 0.3167 - val_acc: 0.8372 - val_auc: 0.9867 Epoch 22/50 2/2 [==============================] - 0s 263ms/step - loss: 0.3770 - acc: 0.7754 - auc: 0.9747 - val_loss: 0.3064 - val_acc: 0.8372 - val_auc: 0.9867 Epoch 23/50 2/2 [==============================] - 0s 239ms/step - loss: 0.3609 - acc: 0.7702 - auc: 0.9736 - val_loss: 0.2927 - val_acc: 0.8372 - val_auc: 0.9867 Epoch 24/50 2/2 [==============================] - 0s 158ms/step - loss: 0.3288 - acc: 0.7858 - auc: 0.9766 - val_loss: 0.2813 - val_acc: 0.8372 - val_auc: 0.9867 Epoch 25/50 2/2 [==============================] - 0s 232ms/step - loss: 0.3434 - acc: 0.7546 - auc: 0.9698 - val_loss: 0.2677 - val_acc: 0.8372 - val_auc: 0.9867 Epoch 26/50 2/2 [==============================] - 0s 153ms/step - loss: 0.3204 - acc: 0.7807 - auc: 0.9715 - val_loss: 0.2544 - val_acc: 0.8488 - val_auc: 0.9869 Epoch 27/50 2/2 [==============================] - 0s 228ms/step - loss: 0.3049 - acc: 0.7859 - auc: 0.9723 - val_loss: 0.2422 - val_acc: 0.8605 - val_auc: 0.9870 Epoch 28/50 2/2 [==============================] - 0s 233ms/step - loss: 0.3019 - acc: 0.8146 - auc: 0.9732 - val_loss: 0.2320 - val_acc: 0.8605 - val_auc: 0.9874 Epoch 29/50 2/2 [==============================] - 0s 242ms/step - loss: 0.2783 - acc: 0.8538 - auc: 0.9788 - val_loss: 0.2223 - val_acc: 0.8605 - val_auc: 0.9884 Epoch 30/50 2/2 [==============================] - 0s 239ms/step - loss: 0.2476 - acc: 0.8773 - auc: 0.9865 - val_loss: 0.2128 - val_acc: 0.8837 - val_auc: 0.9900 Epoch 31/50 2/2 [==============================] - 0s 266ms/step - loss: 0.2446 - acc: 0.9191 - auc: 0.9933 - val_loss: 0.2036 - val_acc: 0.9070 - val_auc: 0.9915 Epoch 32/50 2/2 [==============================] - 0s 262ms/step - loss: 0.2487 - acc: 0.9269 - auc: 0.9956 - val_loss: 0.1950 - val_acc: 0.9070 - val_auc: 0.9918 Epoch 33/50 2/2 [==============================] - 0s 166ms/step - loss: 0.2231 - acc: 0.9452 - auc: 0.9972 - val_loss: 0.1867 - val_acc: 0.9186 - val_auc: 0.9936 Epoch 34/50 2/2 [==============================] - 0s 297ms/step - loss: 0.2189 - acc: 0.9530 - auc: 0.9986 - val_loss: 0.1795 - val_acc: 0.9535 - val_auc: 0.9948 Epoch 35/50 2/2 [==============================] - 0s 236ms/step - loss: 0.2142 - acc: 0.9869 - auc: 0.9996 - val_loss: 0.1733 - val_acc: 0.9535 - val_auc: 0.9953 Epoch 36/50 2/2 [==============================] - 0s 210ms/step - loss: 0.2041 - acc: 0.9869 - auc: 0.9999 - val_loss: 0.1673 - val_acc: 0.9651 - val_auc: 0.9956 Epoch 37/50 2/2 [==============================] - 0s 215ms/step - loss: 0.1952 - acc: 1.0000 - auc: 1.0000 - val_loss: 0.1612 - val_acc: 0.9651 - val_auc: 0.9961 Epoch 38/50 2/2 [==============================] - 0s 189ms/step - loss: 0.1880 - acc: 1.0000 - auc: 1.0000 - val_loss: 0.1555 - val_acc: 0.9651 - val_auc: 0.9964 Epoch 39/50 2/2 [==============================] - 0s 227ms/step - loss: 0.1951 - acc: 1.0000 - auc: 1.0000 - val_loss: 0.1513 - val_acc: 0.9651 - val_auc: 0.9965 Epoch 40/50 2/2 [==============================] - 0s 191ms/step - loss: 0.1756 - acc: 1.0000 - auc: 1.0000 - val_loss: 0.1463 - val_acc: 0.9651 - val_auc: 0.9968 Epoch 41/50 2/2 [==============================] - 0s 230ms/step - loss: 0.1727 - acc: 1.0000 - auc: 1.0000 - val_loss: 0.1413 - val_acc: 0.9651 - val_auc: 0.9976 Epoch 42/50 2/2 [==============================] - 0s 231ms/step - loss: 0.1688 - acc: 1.0000 - auc: 1.0000 - val_loss: 0.1375 - val_acc: 0.9651 - val_auc: 0.9980 Epoch 43/50 2/2 [==============================] - 0s 249ms/step - loss: 0.1662 - acc: 1.0000 - auc: 1.0000 - val_loss: 0.1335 - val_acc: 0.9651 - val_auc: 0.9987 Epoch 44/50 2/2 [==============================] - 0s 238ms/step - loss: 0.1587 - acc: 1.0000 - auc: 1.0000 - val_loss: 0.1300 - val_acc: 0.9651 - val_auc: 0.9990 Epoch 45/50 2/2 [==============================] - 0s 242ms/step - loss: 0.1535 - acc: 1.0000 - auc: 1.0000 - val_loss: 0.1267 - val_acc: 0.9651 - val_auc: 0.9991 Epoch 46/50 2/2 [==============================] - 0s 261ms/step - loss: 0.1434 - acc: 1.0000 - auc: 1.0000 - val_loss: 0.1226 - val_acc: 0.9651 - val_auc: 0.9994 Epoch 47/50 2/2 [==============================] - 0s 238ms/step - loss: 0.1406 - acc: 1.0000 - auc: 1.0000 - val_loss: 0.1196 - val_acc: 0.9651 - val_auc: 0.9994 Epoch 48/50 2/2 [==============================] - 0s 260ms/step - loss: 0.1364 - acc: 1.0000 - auc: 1.0000 - val_loss: 0.1163 - val_acc: 0.9651 - val_auc: 0.9994 Epoch 49/50 2/2 [==============================] - 0s 202ms/step - loss: 0.1354 - acc: 1.0000 - auc: 1.0000 - val_loss: 0.1134 - val_acc: 0.9651 - val_auc: 0.9994 Epoch 50/50 2/2 [==============================] - 0s 243ms/step - loss: 0.1289 - acc: 1.0000 - auc: 1.0000 - val_loss: 0.1104 - val_acc: 0.9651 - val_auc: 0.9994 Plot Finally, write some code to visualize your loss and metric(s) across the training epochs. You should include both train and validation scores. This is where a legend is very important! Note: If you load the solutions they may not run for you unless you have selected the same metric(s) In [63]: # Plot training history # your code here fig , axs = plt . subplots ( 1 , 3 , figsize = ( 18 , 5 )) axs [ 0 ] . loglog ( history . history [ 'loss' ], linewidth = 4 , label = 'Training' ) axs [ 0 ] . loglog ( history . history [ 'val_loss' ], linewidth = 4 , label = 'Validation' , alpha = 0.7 ) axs [ 0 ] . set_ylabel ( 'Loss' ) axs [ 1 ] . plot ( history . history [ 'acc' ], label = 'Training' ) axs [ 1 ] . plot ( history . history [ 'val_acc' ], label = 'Validation' ) axs [ 2 ] . plot ( history . history [ 'auc' ], label = 'Training' ) axs [ 2 ] . plot ( history . history [ 'val_auc' ], label = 'Validation' ) titles = [ 'Categorical Crossentropy Loss' , 'Accuracy' , 'AUC' ] for ax , title in zip ( axs , titles ): ax . set_xlabel ( 'Epoch' ) ax . set_title ( title ) ax . legend () In [110]: #%load solutions/sol2_2.py End of Team Activity Evaluating the Model First, let's see how well we could to by simply predicting the majority class in the training data for all observations. In [64]: naive_acc = y_train . mean ( axis = 0 ) . max () print ( 'Naive Accuracy:' , naive_acc ) Naive Accuracy: 0.4 In [65]: # Train model . evaluate ( X_train , y_train ) 3/3 [==============================] - 0s 9ms/step - loss: 0.1281 - acc: 1.0000 - auc: 1.0000 Out[65]: [0.1281479299068451, 1.0, 0.9999999403953552] In [66]: # Validation model . evaluate ( X_val , y_val ) 3/3 [==============================] - 0s 19ms/step - loss: 0.1104 - acc: 0.9651 - auc: 0.9994 Out[66]: [0.11039040982723236, 0.9651162624359131, 0.9993915557861328] In [67]: # Test model . evaluate ( X_test , y_test ) 6/6 [==============================] - 0s 5ms/step - loss: 0.1212 - acc: 0.9942 - auc: 0.9998 Out[67]: [0.12122223526239395, 0.9941520690917969, 0.9997691512107849] Black Box Interpretation Proxy Model In [68]: from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import cross_validate , GridSearchCV In [69]: # Create train & test response variables for proxy model y_train_bb = model . predict ( X_train ) . argmax ( - 1 ) y_test_bb = model . predict ( X_test ) . argmax ( - 1 ) In [70]: # Use cross-validation to tune proxy's hyperparameters parameters = { 'max_depth' : range ( 1 , 10 ), 'criterion' : [ 'gini' , 'entropy' ]} clf = DecisionTreeClassifier ( random_state = 42 ) grid = GridSearchCV ( clf , parameters , cv = 5 ) # fit using same train but NN's predictions as response grid . fit ( X_train , y_train_bb ) print ( 'Best Score:' , grid . best_score_ ) print ( 'Best Params:' , grid . best_params_ ) Best Score: 1.0 Best Params: {'criterion': 'gini', 'max_depth': 3} In [71]: # Retrieve best estimator from the grid object proxy = grid . best_estimator_ bb_test_score = sum ( y_test_bb == y_test . argmax ( - 1 )) / len ( y_test_bb ) proxy_test_score = proxy . score ( X_test , y_test_bb ) print ( 'Black Box Test Score:' , bb_test_score ) print ( 'Proxy Model Test Score:' , proxy_test_score ) Black Box Test Score: 0.9941520467836257 Proxy Model Test Score: 0.9707602339181286 Feature Importance In [72]: feature_importances = proxy . feature_importances_ feature_importances Out[72]: array([0.50810343, 0. , 0. , 0. , 0.42405589, 0.06784068, 0. ]) In [73]: sort_idx = np . argsort ( feature_importances )[:: - 1 ] In [74]: ax = sns . barplot ( x = feature_importances [ sort_idx ], y = X_cols [ sort_idx ], color = 'purple' , orient = 'h' ) for index , val in enumerate ( feature_importances [ sort_idx ]): ax . text ( val / 3 , index , round ( val , 2 ), color = 'white' , weight = 'bold' , va = 'center' ) ax . set_title ( 'NN Feature Importance According to DTree Proxy' ) sns . despine ( right = True ) Fixing All But One Predictor We can alo try to see how a predictor affects the NN's output by fixing all the other to some \"reasonable\" values (e.g., mean, mode) and then only varying the predictor of interest. Based on the results above, let's explore how bill_length_mm effects the NN's output. Construct 'Average' Observation In [75]: # Review data types X_design . dtypes Out[75]: bill_length_mm float64 bill_depth_mm float64 flipper_length_mm float64 body_mass_g float64 island_Dream uint8 island_Torgersen uint8 sex_Male uint8 dtype: object In [76]: # Take means for continous means = X_scaled [:,: 4 ] . mean ( axis = 0 ) # And modes for catgoricals modes = pd . DataFrame ( X_scaled [:, 4 :]) . mode () . values . reshape ( - 1 ) In [77]: # Shape Sanity Check means . shape , modes . shape Out[77]: ((4,), (3,)) In [78]: # Concatenate these two back together avg_obs = np . concatenate ([ means , modes ]) In [79]: # And stick it back in a DataFrame avg_obs = pd . DataFrame ( avg_obs ) . transpose () avg_obs . columns = X_design . columns avg_obs Out[79]: bill_length_mm bill_depth_mm flipper_length_mm body_mass_g island_Dream island_Torgersen sex_Male 0 1.662088e-16 -1.412775e-15 -8.310441e-16 4.155221e-17 -0.754193 -0.418638 0.965507 In [80]: # Identify column in our array that corresponds to bill length bill_col = np . argmax ( X_design . columns == 'bill_length_mm' ) # Find the min and max bill length stdevs in the data set bill_min_std = np . min ( X_scaled [:, bill_col ]) bill_max_std = np . max ( X_scaled [:, bill_col ]) # Create 100 evenly spaced values within that range bill_lengths = np . linspace ( bill_min_std , bill_max_std , 100 ) # Create 100 duplicates of the average observation avg_df = pd . concat ([ avg_obs ] * bill_lengths . size , ignore_index = True ) # Set the bill length column to then linspace we just created avg_df [ 'bill_length_mm' ] = bill_lengths Notice now that all rows are identical except for bill_length_mm which slowly covers the entire range of values observed in the dataset. In [81]: avg_df . head () Out[81]: bill_length_mm bill_depth_mm flipper_length_mm body_mass_g island_Dream island_Torgersen sex_Male 0 -2.168526 -1.412775e-15 -8.310441e-16 4.155221e-17 -0.754193 -0.418638 0.965507 1 -2.117573 -1.412775e-15 -8.310441e-16 4.155221e-17 -0.754193 -0.418638 0.965507 2 -2.066619 -1.412775e-15 -8.310441e-16 4.155221e-17 -0.754193 -0.418638 0.965507 3 -2.015666 -1.412775e-15 -8.310441e-16 4.155221e-17 -0.754193 -0.418638 0.965507 4 -1.964712 -1.412775e-15 -8.310441e-16 4.155221e-17 -0.754193 -0.418638 0.965507 Return Predictor to Original Scale When we visualize our results we'll want to do so back in the original scale for better interpretability. Here we make use of our scaler object from way back when as it stores the means and standard deviations of the original, unscaled predictors. In [82]: # Recover the feature of interest on the original scale bill_std = np . sqrt ( scaler . var_ [ bill_col ]) bill_mean = scaler . mean_ [ bill_col ] bill_lengths_original = ( bill_std * bill_lengths ) + bill_mean We can sanity check out inverse transformation by confirming we recovereed the same min and max bill length from our very first DataFrame! In [83]: # Min Sanity Check bill_lengths_original . min (), penguins . bill_length_mm . min () Out[83]: (32.1, 32.1) In [84]: # Max Sanity Check bill_lengths_original . max (), penguins . bill_length_mm . max () Out[84]: (59.6, 59.6) Now we are ready to plot an approximation of how bill_length_mm affects the NN's predictions. In [85]: # Plot predicted class probabilities as a function of bill length (approx) avg_pred = model . predict ( avg_df ) fig , ax = plt . subplots () for idx , species in enumerate ( labenc . classes_ ): plt . plot ( bill_lengths_original , avg_pred [:, idx ], label = species ) ax . set_ylabel ( 'predicted probability' ) ax . set_xlabel ( 'bill length in mm' ) ax . set_title ( 'NN Predictions varying only bill length, holding all other predictors at mean/mode' ) ax . legend (); If you know your penguins this should be too surprising. Gentoo penguins are the 3rd largest species after the emperor and king penguins (not represented in our dataset). Q: Why is this only an approximation of how bill_length_mm affects the NN's predictions? Bagging You'll be using bagging (\"bootstrap aggregating\") in your HW so let's take a minute to review the idea and see how it would work with a Kerass model. The idea is to similuate multiple datasets by sampling our current one with replacement and fitting a model on this sample. The process is repeated multiple times until we have an ensemble of fitted models, all trained on slightly different datasets. We can then treat the ensemble as a singled 'bagged' model. When it is time to predict, each model in the ensemble makes its own predictions. These predictions can then be aggregated across models, for example, by taking the average or through majority voting. We may also be interested in looking at the distribution of the predictions for a given observation as this may help us quanity our uncertainty in a way in which we could not with a single model's predictions (even if that model outputs a probability!) In [86]: # Set sup parameters for the bagging process learning_rate = 1e-1 epochs = 50 batch_size = 64 n_boot = 30 bagged_model = [] np . random . seed ( 109 ) for n in range ( n_boot ): # Bootstrap boot_idx = np . random . choice ( X_train . shape [ 0 ], size = X_train . shape [ 0 ], replace = True ) X_train_boot = X_train [ boot_idx ] y_train_boot = y_train [ boot_idx ] # Build boot_model = build_NN ( name = f 'penguins_ { n } ' , input_shape = ( X_train_boot . shape [ 1 ],), hidden_dims = [ 8 , 16 , 32 ], hidden_act = 'relu' , out_dim = 3 , out_act = 'softmax' ) # Compile boot_model . compile ( optimizer = SGD ( learning_rate = learning_rate ), loss = 'categorical_crossentropy' , metrics = [ 'acc' , 'AUC' ]) # Fit boot_model . fit ( X_train_boot , y_train_boot , batch_size = batch_size , epochs = epochs , verbose = 0 ) # Store bootstrapped model's probability predictions bagged_model . append ( boot_model ) In [87]: # Notice we can programatically recover the shape of a model's output layer m = bagged_model [ 0 ] out_dim = m . layers [ - 1 ] . output_shape [ - 1 ] print ( out_dim ) 3 In [88]: def get_bagged_pred ( bagged_model , X ): # Number of observations n_obs = X . shape [ 0 ] # Prediction dimensions (here, number of classes) pred_dim = bagged_model [ 0 ] . layers [ - 1 ] . output_shape [ - 1 ] # Number of models in the bagged ensemble n_models = len ( bagged_model ) # 3D tensor to store predictions from each bootstrapped model # n_observations x n_classes x n_models boot_preds = np . zeros (( n_obs , pred_dim , n_models )) # Store all predictions in the tensor for i , model in enumerate ( bagged_model ): boot_preds [:,:, i ] = model . predict ( X ) # Average the predictions across models bag_pred = boot_preds . mean ( axis =- 1 ) return bag_pred , boot_preds In [89]: # Get aggregated and unaggregated ensemble predictions bag_pred , boot_preds = get_bagged_pred ( bagged_model , X_test ) In [90]: # Example of aggregated predictions bag_pred [: 3 ] Out[90]: array([[0.0202006 , 0.96481654, 0.01498286], [0.02863034, 0.95705401, 0.01431565], [0.03919064, 0.94526246, 0.01554691]]) In [91]: # Shape of unaggregated ensemble predictions tensor boot_preds . shape Out[91]: (171, 3, 30) In [92]: # Calculate bagged accuracy bag_acc = sum ( bag_pred . argmax ( axis =- 1 ) == y_test . argmax ( axis =- 1 )) / bag_pred . shape [ 0 ] print ( 'Bagged Acc:' , bag_acc ) Bagged Acc: 0.9941520467836257 🏋🏻‍♂️ Optional Take-home Challenges Bagged NN Custom Python Class It would be nice if we could interact with our bagged model like any other keras model, passing Create a custom Bagged_NN class with its own build , compile , fit , eval , and predict methods! Use Bootstraped Predictions To Quantify Uncertainty In your HW you'll use bootstrapping to quantify uncertainty on predictions of a binary variable using Posterior Predictive Ratio (PPR). How might you do something similar with categorical bootstrapped predictions like we have here? In [93]: # Might something like entropy be useful? from scipy.stats import entropy entropy ([ 0.25 , 0.25 ], base = 2 ), entropy ([ 0.8 , 0.2 ], base = 2 ), entropy ([ 1 , 0 , 0 , 0 ], base = 2 ) Out[93]: (1.0, 0.7219280948873623, 0.0) An Image Classification Example The 2nd half of your HW asks you to classifying images. Let's try soemthing similar now using the famouse MNIST dataset of handwritten digits.\\ We can load the dataset directly from Tensorflow/Keras! You can read more about TensorFlow's datasets here . In [94]: from tensorflow.keras.datasets import mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () print ( x_train . shape , y_train . shape , x_test . shape , y_test . shape ) (60000, 28, 28) (60000,) (10000, 28, 28) (10000,) In [95]: # Unique response variable values set ( y_test ) Out[95]: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9} Each observation is an 28x28 pixel image.\\ There are 60,000 training examples and 10,000 test images.\\ The $y$ values corresponde to which of the digit the image represents, 0-9. This is how each image is represented numerically. In [96]: np . set_printoptions ( edgeitems = 30 , linewidth = 100000 , formatter = dict ( float = lambda x : \" %.3g \" % x )) x_train [ 10 ] Out[96]: array([[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 118, 219, 166, 118, 118, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 103, 242, 254, 254, 254, 254, 254, 66, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 232, 254, 254, 254, 254, 254, 238, 70, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 104, 244, 254, 224, 254, 254, 254, 141, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 207, 254, 210, 254, 254, 254, 34, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 206, 254, 254, 254, 254, 41, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 209, 254, 254, 254, 171, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 91, 137, 253, 254, 254, 254, 112, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 40, 214, 250, 254, 254, 254, 254, 254, 34, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 81, 247, 254, 254, 254, 254, 254, 254, 146, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 110, 246, 254, 254, 254, 254, 254, 171, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 73, 89, 89, 93, 240, 254, 171, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 128, 254, 219, 31, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 254, 254, 214, 28, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 138, 254, 254, 116, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 19, 177, 90, 0, 0, 0, 0, 0, 25, 240, 254, 254, 34, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 164, 254, 215, 63, 36, 0, 51, 89, 206, 254, 254, 139, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 57, 197, 254, 254, 222, 180, 241, 254, 254, 253, 213, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 140, 105, 254, 254, 254, 254, 254, 254, 236, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 7, 117, 117, 165, 254, 254, 239, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8) The values represent pixel intensity and range from 0-255.\\ We can use plt.imshow or ax.imshow to display it as an image. In [97]: # Display and example observation as an image print ( 'This picture belongs to the class for number' , y_train [ 10 ]) ax = plt . gca () ax . grid ( 'off' ) ax . imshow ( x_train [ 10 ], cmap = 'gray' ); This picture belongs to the class for number 3 (Just a Little) Preprocessing Flattening We don't know how to feed a 2D input into our neural networks (yet!). So we will simply flatted each image to a length 28x28 = 784 array. In [98]: # Flatten image data x_train = x_train . reshape ( x_train . shape [ 0 ], 784 ) x_test = x_test . reshape ( x_test . shape [ 0 ], 784 ) # check if the shapes are ok print ( x_train . shape , y_train . shape , x_test . shape , y_test . shape ) (60000, 784) (60000,) (10000, 784) (10000,) Normalizing Let's confirm what we said about pixel values ranging from 0-255 and then normalize them to the range [0,1]. In [99]: # checking the min and max of x_train and x_test print ( x_train . min (), x_train . max (), x_test . min (), x_test . max ()) 0 255 0 255 In [100]: # Normalize x_train = x_train / 255 x_test = x_test / 255 print ( x_train . min (), x_train . max (), x_test . min (), x_test . max ()) 0.0 1.0 0.0 1.0 Build & Compile Here we use a little trick with the 'sparse_categorical_crossentropy' loss. Basically, the saves us from having to turn our response variable into a categorical one! We can just leave them as integers. We'll also cheat a bit here and use the Adam optimizer. We'll learn more about this and other optimizers in the coming lectures and advanced section. Notice too how a sequential Keras model can also be defined as a list passed to the Sequential constructor rather than by repeatedly using the add method. In future labs, we'll look at the functional Keras API, which is an alternative approach to sequential which is more flexible, allowing for more complex architectures. In [101]: from tensorflow.keras.losses import sparse_categorical_crossentropy In [102]: # Build MNIST model model_mnist = tf . keras . models . Sequential ([ tf . keras . layers . Input ( shape = ( 784 ,)), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dense ( 10 , activation = 'softmax' ) ]) model_mnist . compile ( loss = 'sparse_categorical_crossentropy' , optimizer = tf . keras . optimizers . Adam ( 0.001 ), metrics = [ 'accuracy' ] ) Fit In [103]: # Fit the MNIST model trained_mnist = model_mnist . fit ( x_train , y_train , epochs = 6 , batch_size = 128 , validation_data = ( x_test , y_test )) Epoch 1/6 469/469 [==============================] - 7s 14ms/step - loss: 0.6048 - accuracy: 0.8335 - val_loss: 0.1983 - val_accuracy: 0.9430 Epoch 2/6 469/469 [==============================] - 8s 16ms/step - loss: 0.1823 - accuracy: 0.9478 - val_loss: 0.1383 - val_accuracy: 0.9609 Epoch 3/6 469/469 [==============================] - 7s 15ms/step - loss: 0.1200 - accuracy: 0.9658 - val_loss: 0.1072 - val_accuracy: 0.9696 Epoch 4/6 469/469 [==============================] - 9s 19ms/step - loss: 0.0910 - accuracy: 0.9731 - val_loss: 0.0936 - val_accuracy: 0.9717 Epoch 5/6 469/469 [==============================] - 7s 15ms/step - loss: 0.0695 - accuracy: 0.9806 - val_loss: 0.0881 - val_accuracy: 0.9751 Epoch 6/6 469/469 [==============================] - 8s 18ms/step - loss: 0.0577 - accuracy: 0.9839 - val_loss: 0.0798 - val_accuracy: 0.9759 In [104]: # Helper function for plotting training history def plot_accuracy_loss ( model_history ): plt . figure ( figsize = [ 12 , 4 ]) plt . subplot ( 1 , 2 , 1 ) plt . semilogx ( model_history . history [ 'accuracy' ], label = 'train_acc' , linewidth = 4 ) plt . semilogx ( model_history . history [ 'val_accuracy' ], label = 'val_acc' , linewidth = 4 , alpha = .7 ) plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Accuracy' ) plt . legend () plt . subplot ( 1 , 2 , 2 ) plt . loglog ( model_history . history [ 'loss' ], label = 'train_loss' , linewidth = 4 ) plt . loglog ( model_history . history [ 'val_loss' ], label = 'val_loss' , linewidth = 4 , alpha = .7 ) plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Loss' ) plt . legend () plt . tight_layout () In [105]: # Plot MNIST training history plot_accuracy_loss ( trained_mnist ) Not bad! But do see some overfitting as the validation accuracy starts to diverge from the training accuracy in later epochs. The same general trend can also be seen in the plot of the losses. In the next lecture we'll look at methods for dealing with overfitting in neural networks. Visually Inspecting Model Performance A great benefit of working with image date is that you can often (but not always) simply look at an observation to see if your model's prediction make sense or not. Let's try that now! In [106]: # Make a single prediction and validate it def example_NN_prediction ( dataset = x_test , model_ = model_mnist ): \"\"\" This tests our MNist FFNN by examining a single prediction on the test set and checking if it matches the real label. Arguments: n: if you select n then you will choose the nth test set \"\"\" mnist_preds = model_mnist . predict ( x_test ) all_predictions = np . argmax ( mnist_preds , axis = 1 ) n = np . random . choice ( 784 ) digit = x_test [ n ,:] actual_label = y_test [ n ] plt . imshow ( digit . reshape ( - 1 , 28 )) prediction_array = model_ . predict ( digit . reshape ( 1 , - 1 )) prediction = np . argmax ( prediction_array ) if prediction == y_test [ n ]: print ( \"The Mnist model correctly predicted:\" , prediction ) else : print ( \"The true label was\" , actual_label ) print ( \"The Mnist model incorrectly predicted:\" , prediction ) #################################################### # Make a many predictions and validate them ################################################### def example_NN_predictions ( model_ , dataset_ = x_test , response_ = y_test , get_incorrect = False ): \"\"\" This tests our MNist FFNN by examining 3 images and checking if our nueral network can correctly classify them. Arguments: model_ : the mnist model you want to check predictions for. get_incorrect (boolean): if True, the model will find 3 examples where the model made a mistake. Otherwise it just select randomly. \"\"\" dataset = dataset_ . copy () response = response_ . copy () # If get_incorrect is True, then get an example of incorrect predictions. # Otherwise get random predictions. if not get_incorrect : n = np . random . choice ( dataset . shape [ 0 ], size = 3 ) digits = dataset [ n ,:] actual_label = response [ n ] else : # Determine where the model is making mistakes: mnist_preds = model_mnist . predict ( dataset ) all_predictions = np . argmax ( mnist_preds , axis = 1 ) incorrect_index = all_predictions != response incorrect = x_test [ incorrect_index , :] # Randomly select a mistake to show: n = np . random . choice ( incorrect . shape [ 0 ], size = 3 ) digits = incorrect [ n ,:] # determine the correct label labels = response [ incorrect_index ] actual_label = labels [ n ] #get the predictions and make the plot: fig , ax = plt . subplots ( 1 , 3 , figsize = ( 12 , 4 )) ax = ax . flatten () for i in range ( 3 ): #show the digit: digit = digits [ i ,:] ax [ i ] . imshow ( digit . reshape ( 28 , - 1 )) #reshape the image to 28 by 28 for viewing # reshape the input correctly and get the prediction: prediction_array = model_ . predict ( digit . reshape ( 1 , - 1 )) prediction = np . argmax ( prediction_array ) #Properly label the prediction (correct vs incorrect): if prediction == actual_label [ i ]: ax [ i ] . set_title ( \"Correct Prediction: \" + str ( prediction )) else : ax [ i ] . set_title ( 'Incorrect Prediction: {} (True label: {} )' . format ( prediction , actual_label [ i ])) plt . tight_layout () In [107]: # Here's a random prediction example example_NN_prediction () The Mnist model correctly predicted: 1 In [108]: # Correct predictions example_NN_predictions ( model_ = model_mnist , get_incorrect = False ) Let's see some examples where the network makes the wrong prediction. In [109]: # Incorrect Predictions example_NN_predictions ( model_ = model_mnist , get_incorrect = True ) Oh my. That is some bad handwriting. In [ ]:","tags":"labs","url":"labs/lab05/notebook/"},{"title":"Lab 5: FFNN 2","text":"CS109A Introduction to Data Science Lab 5: Feed Forward Neural Networks 2 (Training, Evaluation, & Interogation) Harvard University Spring 2022 Instructors : Mark Glickman & Pavlos Protopapas Lab Team : Eleni Kaxiras, Marios Mattheakis, Chris Gumb, Shivas Jayaram In [ ]: #RUN THIS CELL import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Table of Contents Building a NN /w Keras Quick Review Learning weights from Data Evaluating a Keras Model Inspecting Training History Multi-class Classification Example (Tabular Data) Interpreting Our Black Box NN Bagging Review Image Classification Example In [ ]: import pandas as pd import matplotlib.pyplot as plt import numpy as np Let's revisit the toy dataset from the first NN lab. In [ ]: # Load toy data toydata = pd . read_csv ( 'data/toyDataSet_1.csv' ) x_toy = toydata [ 'x' ] . values . reshape ( - 1 , 1 ) y_toy = toydata [ 'y' ] . values . reshape ( - 1 , 1 ) In [ ]: # Plot toy data ax = plt . gca () ax . scatter ( x_toy , y_toy ) ax . set_xlabel ( 'x' ) ax . set_ylabel ( 'y' ) ax . set_title ( 'Toy Dataset' ); In [ ]: from tensorflow.keras import models , layers , activations from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense , Input # Input \"layer\"?!? Here we construct a sequential Keras model with one Dense hidden layer containing only a single neuron with a relu activation.\\ The output layer will be of size 1 and have no activation (e.g., 'linear'). In [ ]: # Instantiate sequential Keras model and give it a name toy_model = Sequential ( name = 'toy_model' ) # Despite designation in Keras, Input is not a true layer # It only specifies the shape of the input toy_model . add ( Input ( shape = ( 1 ,))) # hidden layer with 1 neurons (or nodes) toy_model . add ( Dense ( 1 , activation = 'relu' )) # output layer, one neuron toy_model . add ( Dense ( 1 , activation = 'linear' )) toy_model . summary () Compiling the NN model.compile(optimizer, loss, metrics, **kwargs) optimizer - defines how the weights are updated (we'll use SGD)\\ loss - what the model is trying to minimize\\ metric - list of metrics to report during training process compile is used to configure a NN model be for it can be fit. We aren't ready to fit just yet, but we are compiling here because doing so reinitilizes the model weights. We are going to manually set our weights before training so we need to to the compilation first. Q: Why do I want metrics if I already have a loss? In [ ]: import tensorflow as tf from tensorflow.keras import optimizers , losses , metrics from tensorflow.keras.optimizers import SGD from tensorflow.keras.losses import mse In [ ]: toy_model . compile ( optimizer = SGD ( learning_rate = 1e-1 ), loss = 'mse' , metrics = []) A little nudge... Our toy model is very simply. It only has 4 weights. But the problem there are only 4 possible weight values that would make this a good fit. That is like finding a needle in a haystack. So we will cheat a bit and initialize our weights in the 'neighborhood' of the true weights which generated the data. Our future models will be complex enough that they won't need to worry about finding a specific combination of weights: some local minima (but not all) will do the job just fine. In [ ]: # A FUNCTION THAT READS AND PRINTS OUT THE MODEL WEIGHTS/BIASES def print_weights ( model ): weights = model . get_weights () print ( dict ( zip ([ \"w1\" , \"b1\" , \"w2\" , \"b2\" ], [ weight . flatten ()[ 0 ] for weight in weights ]))) # MANUALLY SETTING THE WEIGHTS/BIASES ## True weights from data generating function # w1 = 2 # b1 = 0.0 # w2 = 1 # b2 = 0.5 # Initialize weights to that 'neighborhood' w1 = 1.85 b1 = - 0.5 w2 = 0.9 b2 = 0.4 # Store current weight data structure weights = toy_model . get_weights () # hidden layer weights [ 0 ][ 0 ] = np . array ([ w1 ]) #weights weights [ 1 ] = np . array ([ b1 ]) # biases # output layer weights [ 2 ] = np . array ([[ w2 ]]) # weights weights [ 3 ] = np . array ([ b2 ]) # bias # hidden layer # Set the weights toy_model . set_weights ( weights ) print ( 'Manually Initialized Weights:' ) print_weights ( toy_model ) Forward Pass Review Input, Hidden Layers, and Output Layers The forward pass through an FFNN is a sequence of linear (affine) and nonlinear operations (activation). We use the model's predict method to execut the forward pass with a linspace spanning the range of the x data as input. In [ ]: # Predict x_lin = np . linspace ( x_toy . min (), x_toy . max (), 500 ) y_hat = toy_model . predict ( x_lin ) # Plot plt . scatter ( x_toy , y_toy , alpha = 0.4 , lw = 4 , label = 'data' ) plt . plot ( x_lin , y_hat , label = 'NN' , ls = '--' , c = 'r' ) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . title ( 'Predictions with Manually Set Weights' ) plt . legend (); We'll let back propogation and stochastic gradient descent take it from here. Back Propogation & SGD Review The backward pass is the training. It is based on the chain rule of calculus, and it calculates the gradient of the loss w.r.t. the weights. This gradient is used by the optimizer to update the weights to minimize the loss function. Batching, stochastic gradient descent, and epochs Shuffle and partition the dataset in mini-batches to help escape from local minima. Each batch is seen once per epoch. And thus each observation is also seen once per epoch. We can train the network for as many epochs as we like. Reproducibility There is a lot of stochasticity in the training of neural networks, from weight initilizations, to shuffling of data between epochs.\\ Below is some code that appears to be working for me to get reproducible results. Though I think some of the steps taken may be purely superstitious. In [ ]: # Advice gleaned from: https://deeplizard.com/learn/video/HcW0DeWRggs import os import random as rn os . environ [ 'PYTHONHASHSEED' ] = '0' os . environ [ 'CUDA_VISIBLE_DEVICES' ] = '' tf . random . set_seed ( 109 ) np . random . seed ( 109 ) rn . seed ( 109 ) Fitting the NN Model.fit(x=None, y=None, batch_size=None, epochs=1, verbose=\"auto\", validation_split=0.0, validation_data=None, shuffle=True, **kwargs) batch_size - number of observations overwhich the loss is calculated before each weight update\\ epochs - number of times the complete dataset is seen in the fitting process\\ verbose - you can silence the training output by setting this to 0 \\ validation_split - splits off a portion of the x and y training data to be used as validation (see warning below)\\ validation_data - tuple designating a seperate x_val and y_val dataset\\ shuffle - whether to shuffle the training data before each epoch We fit the model for 100 epochs and set batch_size to 64. The results of fit() are then stored in a variable called history . In [ ]: # Fit model and store training histry history = toy_model . fit ( x_toy , y_toy , epochs = 100 , batch_size = 64 , verbose = 1 ) Plot Training History history.history is a dictionary which contains information from each training epoch (no, I don't know the rationale behind the double name). Use it to plot the loss across epochs. Don't for get those labels! In [ ]: # Plot training history plt . plot ( history . history [ 'loss' ], c = 'r' ) plt . ylabel ( 'MSE loss' ) plt . xlabel ( 'epoch' ) plt . title ( 'NN Training History' ); In [ ]: # Weights learned for the data print_weights ( toy_model ) We can see we've moved much closer to the original weights after fitting. But visualizing our model's predictions will make this more clear. Predict & Plot We use the model's predict method on a linspace, x_lin , which we construct to span the range of the dataset's $x$ values. We save the resulting predictions in y_hat In [ ]: # Predict x_lin = np . linspace ( x_toy . min (), x_toy . max (), 500 ) y_hat = toy_model . predict ( x_lin ) # Plot plt . scatter ( x_toy , y_toy , alpha = 0.4 , lw = 4 , label = 'data' ) plt . plot ( x_lin , y_hat , label = 'NN' , ls = '--' , c = 'r' ) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . title ( 'Predictions After Training' ) plt . legend (); Much better! But perhaps you are not impressed yet? An Ugly Function In [ ]: def ugly_function ( x ): if x < 0 : return np . exp ( - ( x ** 2 )) / 2 + 1 + np . exp ( - (( 10 * x ) ** 2 )) else : return np . exp ( - ( x ** 2 )) + np . exp ( - (( 10 * x ) ** 2 )) How do you feel about the prospect of manually setting the weights to approximate this beauty? In [ ]: # Generate data x_ugly = np . linspace ( - 3 , 3 , 1500 ) # create x-values for input y_ugly = np . array ( list ( map ( ugly_function , x_ugly ))) # Plot data plt . plot ( x_ugly , y_ugly ); plt . title ( 'An Ugly Function' ) plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ); And here we don't even have the option of cheating by initializing our weights strategically! 🏋🏻‍♂️ TEAM ACTIVITY: We're Gonna Need a Bigger Model... Complete the build_nn function for quickly constructing different NN architectures. Use build_nn to construct an NN to approximate the ugly function Compile the model & print its summary Tip: Remember, if it is the last line of the cell, Jupyter will display the return value without an explicit call to print() required. In fact, Jupyter uses its own display() function which often results in prettier output for tables Fit the model Plot the training history Hyperparameters to play with: Architecture Number of hidden layers Number of neurons in each hidden layer Hidden layers' activation function Training SGD 's learning_rate batch_size epochs NN Build Function \\ Arguments: name : str - A name for your NN. input_shape : tuple - number of predictors in input (remember the trailing ','!) hidden_dims : list of int - specifies the number of neurons in each hidden layer Ex: [2,4,8] would mean 3 hidden layers with 2, 4, and 8 neurons respectively hidden_act : str (or Keras activation object) - activation function used by all hidden layers out_dim : int - number of output neurons a.k.a 'ouput units' out_act : str (or Keras activation object) - activation function used by output layer Hint: We will reuse this function throughout the notebook in different settings, but you should go ahead and set some sensible defaults for all of the arguments. In [ ]: def build_NN ( name = 'NN' , input_shape = ( 1 ,), hidden_dims = [ 2 ], hidden_act = 'relu' , out_dim = 1 , out_act = 'linear' ): # your code here # end your code here return model In [ ]: # %load ../solutions/sol1_1.py Build & Print Model Summary Use build_NN to construct your model and store it in a variable called ugly_model .\\ You can play with hidden_dims and hidden_act . In [ ]: # your code here Compile \\ Use the SGD optimizer and 'mse' as your loss.\\ You can expermiment with SGD 's learning_rate . In [ ]: # Compile # your code here Fit \\ Fit ugly_model on x_ugly and y_ugly and story the results in a variable called history .\\ You can experiment with epochs and batch_size . In [ ]: # Fit # your code here Plot Training History Plot the model's training history. Don't forget your axis labels!\\ Hint: Remember, fit returns a history object which itself has a history dictionary attribute. Because this (2nd object) is a dictionary, you can always use its keys method if you don't know what's in it. You can also access the history from the model itself. Ex: ugly_model.history.history In [ ]: # Plot History # your code here Get Predictions \\ Similar to sklearn models, keras models have a predict method. Use your model's predict method to predict on x_ugly and store the results in a variable called y_hat_ugly . In [ ]: # Predict # your code here Plot Predictions \\ Run the cell below to compare your model's predictions to the true (ugly) function. Still not quite right? Try tweaking some of the hyperparameters above and re-run the cells in this section to see if you can improve. In [ ]: # Plot predictions plt . plot ( x_ugly , y_ugly , alpha = 0.4 , lw = 4 , label = 'true function' ) plt . plot ( x_ugly , y_hat_ugly , label = 'NN' , ls = '--' , c = 'r' ) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . legend (); End of Team Activity Multi-class Classification with Keras In [ ]: import seaborn as sns So far we've only used our new Keras powers for toy regression problems. We'll move to classification next... but with 3 classes! This example will use seaborn 's penguins dataset (a worthy successor to the connonical iris dataset.) We'll build a model to identify a penguin's species from its other features. In the process we'll dust off our Python skills with a quick run through a basic model building workflow. In [ ]: # Bring on the penguins! penguins = sns . load_dataset ( 'penguins' ) penguins . head () We have 3 species of penguins living across 3 different islands. There are measurements of bill length, bill depth, flipper length, and body mass. We also have categorcial variable for each penguin's sex giving us a total of 7 features. Here's a plot that tries to show too much at once. But you can ignore the marker shapes and sizes. The bill and flipper length alone ($x$ and $y$ axes) seem too already provide a fair amount of information about the species (color). In [ ]: # Plot penguins with too much info sns . relplot ( data = penguins , x = 'flipper_length_mm' , y = 'bill_length_mm' , hue = 'species' , style = 'sex' , size = 'body_mass_g' , height = 6 ); plt . title ( 'Penguins!' , fontdict = { 'color' : 'teal' , 'size' : 20 , 'weight' : 'bold' , 'family' : 'serif' }); You may have noticed some pesky NaN s when we displayed the beginning of the DataFrame.\\ We should investigate further. Missingness In [ ]: # How many missing values in each column? penguins . isna () . sum () Let's take a look at them first all the rows with missing data. In [ ]: # Rows with missingness penguins [ penguins . isna () . any ( axis = 1 )] Yikes! There are two observations where all predictors except species and island are missing.\\ These rows won't be of any use to us. We see that dropping rows missing body_mass_g will take care of most our missingness. In [ ]: # Drop the bad rows identified above penguins = penguins . dropna ( subset = [ 'body_mass_g' ]) # Check state of missingness after dropping penguins . isna () . sum () It looks like there are 9 rows where sex is missing. We can try to impute these values.\\ But first, take a look at our DataFrame again. In [ ]: penguins . head () Notice how the indices go from 2 to 4 . What happened to 3 ?\\ It was one of the rows we dropped! This issue with the indices can cause headaches later on (think loc / iloc distinction). But we can make things good as new using the reset_index method. Just be sure to set drop=True , otherwise the old indices will be added to the DataFrame as a new column. In [ ]: # Reset index penguins = penguins . reset_index ( drop = True ) penguins . head () Much better!\\ Now, on to imputing the missing sex values. Let's take a look at the value_counts . In [ ]: # Counts of each unique value in the dataset penguins . sex . value_counts () It's almost an even split. We'll impute the mode because it's a quick fix. In [ ]: # The mode here should match the value with the most counts above sex_mode = penguins . sex . mode ()[ 0 ] sex_mode Finally, we use fillna to replace the remaining NaN s with the sex_mode and confirm that there are no more missing values in the DataFrame. In [ ]: # Replace missing values with most common value (i.e, mode) penguins = penguins . fillna ( sex_mode ) penguins . isna () . sum () Q: Imputing the mode here was very easy, but does this approach make you a bit nervous? Why? Is there some other way we could have imputed this values? Preprocessing We can't just throw this DataFrame at a neural network as it is. There's some work we need to do first. Separate predictors from response variable In [ ]: # Isolate response from predictors response = 'species' X = penguins . drop ( response , axis = 1 ) y = penguins [ response ] Encode Categorical Predictor Variables In [ ]: # Check the predictor data types X . dtypes Both island and sex are categotical. We can use pd.get_dummies to one-hot-encode them (don't forget to drop_first !). In [ ]: # Identify the categorical columns cat_cols = [ 'island' , 'sex' ] In [ ]: # one-hot encode the categorical columns X_design = pd . get_dummies ( X , columns = cat_cols , drop_first = True ) X_design . head ( 1 ) From the remaining columns we can infer that the 'reference' values for our categorical variables are island = 'Biscoe' , and sex = 'Female' . Feature Scaling We should take a closer look at the range of values our predictors take on. In [ ]: # Summary stats of predictors X_design . describe () Our features are not on the same scale. Just compare the min/max of bill_depth_mm and body_mass_g for example.\\ This can slow down neural network training for reasons we'll see in an upcoming lecture. Let's make use of sklearn 's StandardScaler to standardize the data, centering each predictor at 0 and setting their standard deviations to 1. In [ ]: from sklearn.preprocessing import StandardScaler In [ ]: # Remember the column names for later; we'll lose them when we scale X_cols = X_design . columns # Saving the scaler object in a variable allows us to reverse the transformation later scaler = StandardScaler () X_scaled = scaler . fit_transform ( X_design ) In [ ]: # The scaler was passed a pandas DataFrame but returns a numpy array type ( X_scaled ), X_scaled . shape In [ ]: # We can always add the column names back later if we need to pd . DataFrame ( X_scaled , columns = X_cols ) . head ( 3 ) Encoding the Response Variable In [ ]: # Take a look at our response y Our response variable is still a string . We need to turn it into some numerical representation for our neural network.\\ We could to this ourselves with a few list comprehensions, but sklearn 's LabelEncoder makes this very easy. In [ ]: from sklearn.preprocessing import LabelEncoder In [ ]: # Encode string labels as integers # LabelEncoder uses the familar fit/transform methods we saw with StandardScaler labenc = LabelEncoder () . fit ( y ) y_enc = labenc . transform ( y ) y_enc In [ ]: # We can recover the class labels from the encoder object later labenc . classes_ This gets us part of the way there. But the penguin species are categorical not ordinal . Keeping the labels as integers implies that species 2 is twice as \"different\" from species 1 as it is from species 0 . We want to perform a conversion here similar to the one-hot encoding above, except will will not 'drop' one of the values. This is where Keras's to_categorical utility function comes in. In [ ]: from tensorflow.keras.utils import to_categorical In [ ]: y_cat = to_categorical ( y_enc ) y_cat Perfect! Q: If this is what our array of response variables looks like, what will this mean for the output layer of our neural network? Train-test Split In [ ]: from sklearn.model_selection import train_test_split You may be familiar with using train_test_split to split the X and y arrays themselves. But here we will using it to create a set of train and test indices . We'll see later that being able to determine which rows in the original X and y ended up in train or test will be helpful. Q: But couldn't we just sample integers to get random indices? Why use train_test_split ? A: Because train_test_split allows for stratified splitting! Here we use a trick to stratify on both the sex and island variables by concatinating their values together. This gives us a total of 6 possible values (2 sexs x 3 islands). By stratifying on this column we help ensure that each of the 6 possible sex/island combinations is equally represented in both train and test. In [ ]: # Concatenate categorical columns; use this for stratified splitting strat_col = penguins [ 'sex' ] . astype ( 'str' ) + penguins [ 'island' ] . astype ( 'str' ) strat_col In [ ]: # Create train/test indices train_idx , test_idx = train_test_split ( np . arange ( X_scaled . shape [ 0 ]), test_size = 0.5 , random_state = 109 , stratify = strat_col ) In [ ]: # Index into X_scaled and y_cat to create the train and test sets X_train = X_scaled [ train_idx ] y_train = y_cat [ train_idx ] X_test = X_scaled [ test_idx ] y_test = y_cat [ test_idx ] In [ ]: # Sanity check on the resulting shapes X_train . shape , y_train . shape , X_test . shape , y_test . shape Validation Split Here is where those indices we saved come in handy.\\ We also want to also ensure equal representation across train and validation. In [ ]: # Subset original stratify column using saved train split indices strat_col2 = strat_col . iloc [ train_idx ] strat_col2 . shape In [ ]: # Create train and validation splits from original train split X_train , X_val , y_train , y_val = train_test_split ( X_train , y_train , test_size = 0.5 , random_state = 109 , stratify = strat_col2 ) 🏋🏻‍♂️ TEAM ACTIVITY: Classify Those Penguins! In [ ]: from tensorflow.keras.losses import categorical_crossentropy from tensorflow.keras.metrics import Accuracy , AUC from tensorflow.keras.activations import softmax Build Construct your NN penguin classifier. You can make use of your build_NN function from earlier. What output activation should you use? Hint: try to programaticlaly determin the input and output shape from your data rather than hard coding those values. In [ ]: # Construct your NN and print the model summary # your code here In [ ]: # %load solutions/sol2_1.py Compile Again, let's use SGD as our optimizer. You can fiddle with the learning_rate .\\ What loss and metric(s) do you think are appropriate? In [ ]: # Compile # youre code here Fit Fit your model and store the results in a variable called history .\\ Feel free to play with batch_size and epochs . Don't forget to include the validation_data ! In [ ]: # Fit # your code here Plot Finally, write some code to visualize your loss and metric(s) across the training epochs. You should include both train and validation scores. This is where a legend is very important! Note: If you load the solutions they may not run for you unless you have selected the same metric(s) In [ ]: # Plot training history # your code here In [ ]: #%load solutions/sol2_2.py End of Team Activity Evaluating the Model First, let's see how well we could to by simply predicting the majority class in the training data for all observations. In [ ]: naive_acc = y_train . mean ( axis = 0 ) . max () print ( 'Naive Accuracy:' , naive_acc ) In [ ]: # Train model . evaluate ( X_train , y_train ) In [ ]: # Validation model . evaluate ( X_val , y_val ) In [ ]: # Test model . evaluate ( X_test , y_test ) Black Box Interpretation Proxy Model In [ ]: from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import cross_validate , GridSearchCV In [ ]: # Create train & test response variables for proxy model y_train_bb = model . predict ( X_train ) . argmax ( - 1 ) y_test_bb = model . predict ( X_test ) . argmax ( - 1 ) In [ ]: # Use cross-validation to tune proxy's hyperparameters parameters = { 'max_depth' : range ( 1 , 10 ), 'criterion' : [ 'gini' , 'entropy' ]} clf = DecisionTreeClassifier ( random_state = 42 ) grid = GridSearchCV ( clf , parameters , cv = 5 ) # fit using same train but NN's predictions as response grid . fit ( X_train , y_train_bb ) print ( 'Best Score:' , grid . best_score_ ) print ( 'Best Params:' , grid . best_params_ ) In [ ]: # Retrieve best estimator from the grid object proxy = grid . best_estimator_ bb_test_score = sum ( y_test_bb == y_test . argmax ( - 1 )) / len ( y_test_bb ) proxy_test_score = proxy . score ( X_test , y_test_bb ) print ( 'Black Box Test Score:' , bb_test_score ) print ( 'Proxy Model Test Score:' , proxy_test_score ) Feature Importance In [ ]: feature_importances = proxy . feature_importances_ feature_importances In [ ]: sort_idx = np . argsort ( feature_importances )[:: - 1 ] In [ ]: ax = sns . barplot ( x = feature_importances [ sort_idx ], y = X_cols [ sort_idx ], color = 'purple' , orient = 'h' ) for index , val in enumerate ( feature_importances [ sort_idx ]): ax . text ( val / 3 , index , round ( val , 2 ), color = 'white' , weight = 'bold' , va = 'center' ) ax . set_title ( 'NN Feature Importance According to DTree Proxy' ) sns . despine ( right = True ) Fixing All But One Predictor We can alo try to see how a predictor affects the NN's output by fixing all the other to some \"reasonable\" values (e.g., mean, mode) and then only varying the predictor of interest. Based on the results above, let's explore how bill_length_mm effects the NN's output. Construct 'Average' Observation In [ ]: # Review data types X_design . dtypes In [ ]: # Take means for continous means = X_scaled [:,: 4 ] . mean ( axis = 0 ) # And modes for catgoricals modes = pd . DataFrame ( X_scaled [:, 4 :]) . mode () . values . reshape ( - 1 ) In [ ]: # Shape Sanity Check means . shape , modes . shape In [ ]: # Concatenate these two back together avg_obs = np . concatenate ([ means , modes ]) In [ ]: # And stick it back in a DataFrame avg_obs = pd . DataFrame ( avg_obs ) . transpose () avg_obs . columns = X_design . columns avg_obs In [ ]: # Identify column in our array that corresponds to bill length bill_col = np . argmax ( X_design . columns == 'bill_length_mm' ) # Find the min and max bill length stdevs in the data set bill_min_std = np . min ( X_scaled [:, bill_col ]) bill_max_std = np . max ( X_scaled [:, bill_col ]) # Create 100 evenly spaced values within that range bill_lengths = np . linspace ( bill_min_std , bill_max_std , 100 ) # Create 100 duplicates of the average observation avg_df = pd . concat ([ avg_obs ] * bill_lengths . size , ignore_index = True ) # Set the bill length column to then linspace we just created avg_df [ 'bill_length_mm' ] = bill_lengths Notice now that all rows are identical except for bill_length_mm which slowly covers the entire range of values observed in the dataset. In [ ]: avg_df . head () Return Predictor to Original Scale When we visualize our results we'll want to do so back in the original scale for better interpretability. Here we make use of our scaler object from way back when as it stores the means and standard deviations of the original, unscaled predictors. In [ ]: # Recover the feature of interest on the original scale bill_std = np . sqrt ( scaler . var_ [ bill_col ]) bill_mean = scaler . mean_ [ bill_col ] bill_lengths_original = ( bill_std * bill_lengths ) + bill_mean We can sanity check out inverse transformation by confirming we recovereed the same min and max bill length from our very first DataFrame! In [ ]: # Min Sanity Check bill_lengths_original . min (), penguins . bill_length_mm . min () In [ ]: # Max Sanity Check bill_lengths_original . max (), penguins . bill_length_mm . max () Now we are ready to plot an approximation of how bill_length_mm affects the NN's predictions. In [ ]: # Plot predicted class probabilities as a function of bill length (approx) avg_pred = model . predict ( avg_df ) fig , ax = plt . subplots () for idx , species in enumerate ( labenc . classes_ ): plt . plot ( bill_lengths_original , avg_pred [:, idx ], label = species ) ax . set_ylabel ( 'predicted probability' ) ax . set_xlabel ( 'bill length in mm' ) ax . set_title ( 'NN Predictions varying only bill length, holding all other predictors at mean/mode' ) ax . legend (); If you know your penguins this should be too surprising. Gentoo penguins are the 3rd largest species after the emperor and king penguins (not represented in our dataset). Q: Why is this only an approximation of how bill_length_mm affects the NN's predictions? Bagging You'll be using bagging (\"bootstrap aggregating\") in your HW so let's take a minute to review the idea and see how it would work with a Kerass model. The idea is to similuate multiple datasets by sampling our current one with replacement and fitting a model on this sample. The process is repeated multiple times until we have an ensemble of fitted models, all trained on slightly different datasets. We can then treat the ensemble as a singled 'bagged' model. When it is time to predict, each model in the ensemble makes its own predictions. These predictions can then be aggregated across models, for example, by taking the average or through majority voting. We may also be interested in looking at the distribution of the predictions for a given observation as this may help us quanity our uncertainty in a way in which we could not with a single model's predictions (even if that model outputs a probability!) In [ ]: # Set sup parameters for the bagging process learning_rate = 1e-1 epochs = 50 batch_size = 64 n_boot = 30 bagged_model = [] np . random . seed ( 109 ) for n in range ( n_boot ): # Bootstrap boot_idx = np . random . choice ( X_train . shape [ 0 ], size = X_train . shape [ 0 ], replace = True ) X_train_boot = X_train [ boot_idx ] y_train_boot = y_train [ boot_idx ] # Build boot_model = build_NN ( name = f 'penguins_ { n } ' , input_shape = ( X_train_boot . shape [ 1 ],), hidden_dims = [ 8 , 16 , 32 ], hidden_act = 'relu' , out_dim = 3 , out_act = 'softmax' ) # Compile boot_model . compile ( optimizer = SGD ( learning_rate = learning_rate ), loss = 'categorical_crossentropy' , metrics = [ 'acc' , 'AUC' ]) # Fit boot_model . fit ( X_train_boot , y_train_boot , batch_size = batch_size , epochs = epochs , verbose = 0 ) # Store bootstrapped model's probability predictions bagged_model . append ( boot_model ) In [ ]: # Notice we can programatically recover the shape of a model's output layer m = bagged_model [ 0 ] out_dim = m . layers [ - 1 ] . output_shape [ - 1 ] print ( out_dim ) In [ ]: def get_bagged_pred ( bagged_model , X ): # Number of observations n_obs = X . shape [ 0 ] # Prediction dimensions (here, number of classes) pred_dim = bagged_model [ 0 ] . layers [ - 1 ] . output_shape [ - 1 ] # Number of models in the bagged ensemble n_models = len ( bagged_model ) # 3D tensor to store predictions from each bootstrapped model # n_observations x n_classes x n_models boot_preds = np . zeros (( n_obs , pred_dim , n_models )) # Store all predictions in the tensor for i , model in enumerate ( bagged_model ): boot_preds [:,:, i ] = model . predict ( X ) # Average the predictions across models bag_pred = boot_preds . mean ( axis =- 1 ) return bag_pred , boot_preds In [ ]: # Get aggregated and unaggregated ensemble predictions bag_pred , boot_preds = get_bagged_pred ( bagged_model , X_test ) In [ ]: # Example of aggregated predictions bag_pred [: 3 ] In [ ]: # Shape of unaggregated ensemble predictions tensor boot_preds . shape In [ ]: # Calculate bagged accuracy bag_acc = sum ( bag_pred . argmax ( axis =- 1 ) == y_test . argmax ( axis =- 1 )) / bag_pred . shape [ 0 ] print ( 'Bagged Acc:' , bag_acc ) 🏋🏻‍♂️ Optional Take-home Challenges Bagged NN Custom Python Class It would be nice if we could interact with our bagged model like any other keras model, passing Create a custom Bagged_NN class with its own build , compile , fit , eval , and predict methods! Use Bootstraped Predictions To Quantify Uncertainty In your HW you'll use bootstrapping to quantify uncertainty on predictions of a binary variable using Posterior Predictive Ratio (PPR). How might you do something similar with categorical bootstrapped predictions like we have here? In [ ]: # Might something like entropy be useful? from scipy.stats import entropy entropy ([ 0.25 , 0.25 ], base = 2 ), entropy ([ 0.8 , 0.2 ], base = 2 ), entropy ([ 1 , 0 , 0 , 0 ], base = 2 ) An Image Classification Example The 2nd half of your HW asks you to classifying images. Let's try soemthing similar now using the famouse MNIST dataset of handwritten digits.\\ We can load the dataset directly from Tensorflow/Keras! You can read more about TensorFlow's datasets here . In [ ]: from tensorflow.keras.datasets import mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () print ( x_train . shape , y_train . shape , x_test . shape , y_test . shape ) In [ ]: # Unique response variable values set ( y_test ) Each observation is an 28x28 pixel image.\\ There are 60,000 training examples and 10,000 test images.\\ The $y$ values corresponde to which of the digit the image represents, 0-9. This is how each image is represented numerically. In [ ]: np . set_printoptions ( edgeitems = 30 , linewidth = 100000 , formatter = dict ( float = lambda x : \" %.3g \" % x )) x_train [ 10 ] The values represent pixel intensity and range from 0-255.\\ We can use plt.imshow or ax.imshow to display it as an image. In [ ]: # Display and example observation as an image print ( 'This picture belongs to the class for number' , y_train [ 10 ]) ax = plt . gca () ax . grid ( 'off' ) ax . imshow ( x_train [ 10 ], cmap = 'gray' ); (Just a Little) Preprocessing Flattening We don't know how to feed a 2D input into our neural networks (yet!). So we will simply flatted each image to a length 28x28 = 784 array. In [ ]: # Flatten image data x_train = x_train . reshape ( x_train . shape [ 0 ], 784 ) x_test = x_test . reshape ( x_test . shape [ 0 ], 784 ) # check if the shapes are ok print ( x_train . shape , y_train . shape , x_test . shape , y_test . shape ) Normalizing Let's confirm what we said about pixel values ranging from 0-255 and then normalize them to the range [0,1]. In [ ]: # checking the min and max of x_train and x_test print ( x_train . min (), x_train . max (), x_test . min (), x_test . max ()) In [ ]: # Normalize x_train = x_train / 255 x_test = x_test / 255 print ( x_train . min (), x_train . max (), x_test . min (), x_test . max ()) Build & Compile Here we use a little trick with the 'sparse_categorical_crossentropy' loss. Basically, the saves us from having to turn our response variable into a categorical one! We can just leave them as integers. We'll also cheat a bit here and use the Adam optimizer. We'll learn more about this and other optimizers in the coming lectures and advanced section. Notice too how a sequential Keras model can also be defined as a list passed to the Sequential constructor rather than by repeatedly using the add method. In future labs, we'll look at the functional Keras API, which is an alternative approach to sequential which is more flexible, allowing for more complex architectures. In [ ]: from tensorflow.keras.losses import sparse_categorical_crossentropy In [ ]: # Build MNIST model model_mnist = tf . keras . models . Sequential ([ tf . keras . layers . Input ( shape = ( 784 ,)), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dense ( 10 , activation = 'softmax' ) ]) model_mnist . compile ( loss = 'sparse_categorical_crossentropy' , optimizer = tf . keras . optimizers . Adam ( 0.001 ), metrics = [ 'accuracy' ] ) Fit In [ ]: # Fit the MNIST model trained_mnist = model_mnist . fit ( x_train , y_train , epochs = 6 , batch_size = 128 , validation_data = ( x_test , y_test )) In [ ]: # Helper function for plotting training history def plot_accuracy_loss ( model_history ): plt . figure ( figsize = [ 12 , 4 ]) plt . subplot ( 1 , 2 , 1 ) plt . semilogx ( model_history . history [ 'accuracy' ], label = 'train_acc' , linewidth = 4 ) plt . semilogx ( model_history . history [ 'val_accuracy' ], label = 'val_acc' , linewidth = 4 , alpha = .7 ) plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Accuracy' ) plt . legend () plt . subplot ( 1 , 2 , 2 ) plt . loglog ( model_history . history [ 'loss' ], label = 'train_loss' , linewidth = 4 ) plt . loglog ( model_history . history [ 'val_loss' ], label = 'val_loss' , linewidth = 4 , alpha = .7 ) plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Loss' ) plt . legend () plt . tight_layout () In [ ]: # Plot MNIST training history plot_accuracy_loss ( trained_mnist ) Not bad! But do see some overfitting as the validation accuracy starts to diverge from the training accuracy in later epochs. The same general trend can also be seen in the plot of the losses. In the next lecture we'll look at methods for dealing with overfitting in neural networks. Visually Inspecting Model Performance A great benefit of working with image date is that you can often (but not always) simply look at an observation to see if your model's prediction make sense or not. Let's try that now! In [ ]: # Make a single prediction and validate it def example_NN_prediction ( dataset = x_test , model_ = model_mnist ): \"\"\" This tests our MNist FFNN by examining a single prediction on the test set and checking if it matches the real label. Arguments: n: if you select n then you will choose the nth test set \"\"\" mnist_preds = model_mnist . predict ( x_test ) all_predictions = np . argmax ( mnist_preds , axis = 1 ) n = np . random . choice ( 784 ) digit = x_test [ n ,:] actual_label = y_test [ n ] plt . imshow ( digit . reshape ( - 1 , 28 )) prediction_array = model_ . predict ( digit . reshape ( 1 , - 1 )) prediction = np . argmax ( prediction_array ) if prediction == y_test [ n ]: print ( \"The Mnist model correctly predicted:\" , prediction ) else : print ( \"The true label was\" , actual_label ) print ( \"The Mnist model incorrectly predicted:\" , prediction ) #################################################### # Make a many predictions and validate them ################################################### def example_NN_predictions ( model_ , dataset_ = x_test , response_ = y_test , get_incorrect = False ): \"\"\" This tests our MNist FFNN by examining 3 images and checking if our nueral network can correctly classify them. Arguments: model_ : the mnist model you want to check predictions for. get_incorrect (boolean): if True, the model will find 3 examples where the model made a mistake. Otherwise it just select randomly. \"\"\" dataset = dataset_ . copy () response = response_ . copy () # If get_incorrect is True, then get an example of incorrect predictions. # Otherwise get random predictions. if not get_incorrect : n = np . random . choice ( dataset . shape [ 0 ], size = 3 ) digits = dataset [ n ,:] actual_label = response [ n ] else : # Determine where the model is making mistakes: mnist_preds = model_mnist . predict ( dataset ) all_predictions = np . argmax ( mnist_preds , axis = 1 ) incorrect_index = all_predictions != response incorrect = x_test [ incorrect_index , :] # Randomly select a mistake to show: n = np . random . choice ( incorrect . shape [ 0 ], size = 3 ) digits = incorrect [ n ,:] # determine the correct label labels = response [ incorrect_index ] actual_label = labels [ n ] #get the predictions and make the plot: fig , ax = plt . subplots ( 1 , 3 , figsize = ( 12 , 4 )) ax = ax . flatten () for i in range ( 3 ): #show the digit: digit = digits [ i ,:] ax [ i ] . imshow ( digit . reshape ( 28 , - 1 )) #reshape the image to 28 by 28 for viewing # reshape the input correctly and get the prediction: prediction_array = model_ . predict ( digit . reshape ( 1 , - 1 )) prediction = np . argmax ( prediction_array ) #Properly label the prediction (correct vs incorrect): if prediction == actual_label [ i ]: ax [ i ] . set_title ( \"Correct Prediction: \" + str ( prediction )) else : ax [ i ] . set_title ( 'Incorrect Prediction: {} (True label: {} )' . format ( prediction , actual_label [ i ])) plt . tight_layout () In [ ]: # Here's a random prediction example example_NN_prediction () In [ ]: # Correct predictions example_NN_predictions ( model_ = model_mnist , get_incorrect = False ) Let's see some examples where the network makes the wrong prediction. In [ ]: # Incorrect Predictions example_NN_predictions ( model_ = model_mnist , get_incorrect = True ) Oh my. That is some bad handwriting. In [ ]:","tags":"labs","url":"labs/lab05/notebook0/"},{"title":"Lab 6: FFNN 3","text":"Notebooks Lab 6 - FFNN 3","tags":"labs","url":"labs/lab06/"},{"title":"Lab 6: FFNN 3","text":"CS109B Introduction to Data Science Lab 6: Feed Forward Neural Networks 3 - Optimizers and Regularization Harvard University Spring 2022 Instructors : Mark Glickman & Pavlos Protopapas Lab Team : Eleni Kaxiras, Marios Mattheakis, Chris Gumb, Shivas Jayaram In [1]: #RUN THIS CELL import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: Table of Contents JupyterHub Keras Functional API Weight Initializers Optimizers SGD Adagrad RMProp Adam Batchnorm Regularization (15) Early Stopping Norm Penalties Dropout Data Augmentation Data Generators (15) Activity 2 (15) In [2]: % matplotlib inline import copy import operator import numpy as np import pandas as pd import tensorflow as tf import random as rn import os import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split from PIL import Image from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error , r2_score In [3]: # You can adjust the notebook width if you like from IPython.display import display , HTML display ( HTML ( \" \" )) In [4]: # Recite the sacred reproducibility incantation os . environ [ 'PYTHONHASHSEED' ] = '0' os . environ [ 'CUDA_VISIBLE_DEVICES' ] = '' tf . random . set_seed ( 109 ) np . random . seed ( 109 ) rn . seed ( 109 ) JupyterHub JupyterHub-GPU is a Jupyter Notebook server with all packages defined in the cs109b.yml installed. Instances running on this server have access to GPU acceleration, making training neural networks much faster! Launching JupyterHub-GPU Select \"JupyterHub-GPU\" from the navigation bar on the Canvas course page. Wait a minute form your server to start up... You'll then be greeted with the familar, classic Jupyter Notebook file browsers. From here you can: Create folders and naviate the file structure Launch Jupyter Notebooks Upload files manually Launch a new terminal which is helpful for: unziping files running git commands or executing arbitrary shell commands To run this lab on JupyterHub, simply launch a new terminal and clone the public course repo with the command: git clone https://github.com/Harvard-IACS/2022-CS109B If you want to run the cells in the 1st half of this lab, they run much faster with the GPU acceleration.\\ One weakness of the JupyterHub server is slow disk read & write speeds. So if you decide to use the Datagenerators at the end of the notebook (which requires files be read from disk each batch) then you may find it runs just as fast (if not faster) on your local machine. We'll see an alternative to Datagenerators that circumvent this issue in lab 7. Part 1 - Regression with Neural Networks Let's fit a difficult function where polynomial regression fails. The dielectric function of many optical materials depends on the frequency and is given by the Lorentz model as: $$ \\varepsilon(\\omega) = 1 - \\frac{\\omega_0&#94;2}{\\omega_0&#94;2-\\omega&#94;2 +i\\omega\\Gamma},$$ where $\\omega$ is the frequency, $\\omega_0$ is the resonance frequency of the bound electrons, and $\\Gamma$ is the electron damping. In many situations, we measure the real part of the dielectric function in the lab and then we fit these observations. Let's assume that we perform an experiment and the observations came from a Lorentz model. In [5]: df = pd . read_csv ( 'data/lorentz_set.csv' ) . sample ( frac = 1 , random_state = 109 ) # shuffle DataFrame! x_train , x_test , y_train , y_test = train_test_split ( df . x , df . y , train_size = 0.7 , random_state = 109 ) In [6]: def plot_lorentz ( df , test_idx , ax = None ): if ax is None : ax = plt . gca () train_mask = np . ones ( df . shape [ 0 ], dtype = bool ) train_mask [ test_idx ] = False ax . scatter ( df . x [ train_mask ], df . y [ train_mask ], c = 'b' , label = 'train data' ) ax . scatter ( df . x [ ~ train_mask ], df . y [ ~ train_mask ], c = 'orange' , marker = '&#94;' , label = 'test data' ) ax . set_xlabel ( '$\\omega$' ) ax . set_ylabel ( '$\\epsilon$' ) ax . legend () plt . figure ( figsize = ( 12 , 5 )); plot_lorentz ( df , x_test . index ); In [7]: polynomial_features = PolynomialFeatures ( degree = 25 ) x_poly_train = polynomial_features . fit_transform ( x_train . values . reshape ( - 1 , 1 )) x_poly_test = polynomial_features . fit_transform ( x_test . values . reshape ( - 1 , 1 )) model = LinearRegression () model . fit ( x_poly_train , y_train ) y_poly_train = model . predict ( x_poly_train ) y_poly_test = model . predict ( x_poly_test ) mse_train_poly = mean_squared_error ( y_train , y_poly_train ) mse_test_poly = mean_squared_error ( y_test , y_poly_test ) print ( 'MSE on training set: ' , mse_train_poly ) print ( 'MSE on testing set: ' , mse_test_poly ) x_lin = np . linspace ( x_train . min (), x_train . max (), 1000 ) x_lin_poly = polynomial_features . fit_transform ( x_lin . reshape ( - 1 , 1 )) y_poly_pred = model . predict ( x_lin_poly ) plt . figure ( figsize = ( 12 , 5 )) ax = plt . gca () plot_lorentz ( df , x_test . index , ax = ax ); # plt.plot(x_train, y_train,'ob',label='train data') # plt.scatter(x_test, y_test,c='orange', marker='&#94;',label='train data') ax . plot ( x_lin , y_poly_pred , color = 'm' , linewidth = 2 , label = 'polynomial model train' ) ax . set_title ( \"The Lorentz Equation: Polynomial Fit, $R&#94;2$ score = {} \" . format ( round ( r2_score ( y_train , y_poly_train ), 4 ))); MSE on training set: 1.106055824984082 MSE on testing set: 1.9999920914096725 As expected, we get some wile behavior because we need a very high polynomial degree to begin to approximate this function.\\ Let's see if we can do better using a feed forward neural network . We'll again be using Tensorflow's high-level Keras API. But this time we'll construct our NNs a bit differently... Keras Functional API In [8]: from tensorflow import keras from tensorflow.keras.layers import Input , Dense from tensorflow.keras.models import Model Official Keras Guide The Keras functional API is a way to create models that are more flexible than the tf.keras.Sequential API . The functional API can handle models with non-linear topology, shared layers, and even multiple inputs or outputs. The main idea is that a deep learning model is usually a directed acyclic graph (DAG) of layers. So the functional API is a way to build graphs of layers. You create a node in the graph by instantiating a layer object. A directed connection is created between nodes by passing one node as input to another: {python} inputs = Dense(...) hidden1 = Dense(...)(inputs) hidden2 = Dense(...)(hidden1) output = Dense(...)(hidden2) The model is created with the tf.keras.Model constructor, which requires args inputs and outputs , defining where the graph begins and where it ends. {python} model = tf.keras.Model(inputs=inputs, outputs=outputs) This flexibility allows you to have forking and merging paths in your network. We'll see more complex graph structures like later in the course! In [9]: inputs = Input ( shape = ( 1 ,)) x = Dense ( 50 , activation = 'tanh' )( inputs ) x = Dense ( 50 , activation = 'tanh' )( x ) outputs = Dense ( 1 , activation = 'linear' )( x ) NN1 = Model ( inputs = inputs , outputs = outputs , name = 'NN1' ) NN1 . save_weights ( 'NN1_init.h5' ) NN1 . summary () Model: \"NN1\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 1)] 0 dense (Dense) (None, 50) 100 dense_1 (Dense) (None, 50) 2550 dense_2 (Dense) (None, 1) 51 ================================================================= Total params: 2,701 Trainable params: 2,701 Non-trainable params: 0 _________________________________________________________________ 2022-03-04 12:07:46.626352: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected 2022-03-04 12:07:46.626419: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ip-10-10-197-240.ec2.internal 2022-03-04 12:07:46.626428: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ip-10-10-197-240.ec2.internal 2022-03-04 12:07:46.626603: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.57.2 2022-03-04 12:07:46.626655: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.57.2 2022-03-04 12:07:46.626664: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.57.2 2022-03-04 12:07:46.627053: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Weight Initilizers tf.keras.initializers In [10]: from tensorflow.keras import initializers from tensorflow.keras.initializers import glorot_normal , glorot_uniform , he_uniform , random_normal The keyword arguments used for passing initializers to layers depends on the layer. Usually, it is simply kernel_initializer and bias_initializer . As we've already seen with Keras objects like optimizers, losses, and metrics, all built-in initializers can also be passed via their string identifier (though this requires that you accept their default parameters if they have any). The standard initializers you might expect are: Normal Truncated Normal Uniform Ones Zeros Some more exotic initializers include: Glorot (a.k.a Xavier) normal initializer Draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor. Glorot et al., 2010 Glorot (a.k.a. Xavier) uniform initializer Similar idea, except now the limits of the uniform are a function of fan_in and fan_out HeNormal Just like Glorot normal except it only uses fan_in He et al., 2015 HeUniform See above As with everything (except possibly early stopping!) there is no free lunch here. The initializer is just another hyperparameter for you to experiment with. And they can indeed have an effect on your model's training, and therefore, its performance as well. In [11]: # Helper function for plotting training history def plot_history ( model , title = None , ax = None ): if ax is None : ax = plt . gca () ax . plot ( model . history . history [ 'loss' ], label = 'train' ) ax . plot ( model . history . history [ 'val_loss' ], label = 'validation' ) ax . set_xlabel ( 'epoch' ) ax . set_ylabel ( 'MSE' ) best_loss = np . nanmin ( model . history . history [ 'val_loss' ]) ax . axvline ( np . nanargmin ( model . history . history [ 'val_loss' ]), c = 'k' , ls = '--' , label = f 'best val loss = { best_loss : .2f } ' ) ax . legend () ax . set_title ( title ) Let's take a look at a few initializers and the resulting weight distributions in the first layer before and after training. In [12]: %%time fig , axs = plt . subplots ( 4 , 4 , figsize = ( 16 , 10 )); kernal_initilizers = [ 'glorot_normal' , 'glorot_uniform' , 'he_uniform' , 'random_normal' ] for i , kernel_initializer in enumerate ( kernal_initilizers ): # Build Model inputs = Input ( shape = ( 1 ,)) x = Dense ( 200 , activation = 'tanh' , kernel_initializer = kernel_initializer )( inputs ) x = Dense ( 50 , activation = 'tanh' , kernel_initializer = kernel_initializer )( x ) x = Dense ( 5 , activation = 'tanh' , kernel_initializer = kernel_initializer )( x ) outputs = Dense ( 1 , activation = 'linear' , kernel_initializer = kernel_initializer )( x ) NN1 = Model ( inputs = inputs , outputs = outputs , name = 'NN1' ) # Compile NN1 . compile ( tf . keras . optimizers . Adam ( learning_rate = 0.01 ), 'mse' ) # Plot Initial Weights first_layer_weights = NN1 . get_weights ()[ 0 ][ 0 ] axs [ i , 2 ] . hist ( first_layer_weights , density = True ) sns . kdeplot ( first_layer_weights , ax = axs [ i , 2 ]) axs [ i , 2 ] . set_xlabel ( 'weight' ) axs [ i , 2 ] . set_title ( 'Initialization' ) # Fit NN1 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), batch_size = 32 , epochs = 100 , verbose = 0 ) # Plot # History plot_history ( NN1 , ax = axs [ i , 0 ], title = kernel_initializer ) # Predictions y_hat = NN1 . predict ( x_test ) mse = mean_squared_error ( y_test , y_hat ) x_lin = np . linspace ( df . x . min (), df . x . max (), ( 500 )) . reshape ( - 1 , 1 ) y_hat_lin = NN1 . predict ( x_lin ) axs [ i , 1 ] . plot ( x_lin , y_hat_lin ); plot_lorentz ( df , test_idx = x_test . index , ax = axs [ i , 1 ]) axs [ i , 1 ] . set_title ( f 'Test MSE = { mse : .2f } ' ) # Plot Weights after fitting first_layer_weights = NN1 . get_weights ()[ 0 ][ 0 ] axs [ i , 3 ] . hist ( first_layer_weights , density = True ) sns . kdeplot ( first_layer_weights , ax = axs [ i , 3 ]); axs [ i , 3 ] . set_xlabel ( 'weight' ) axs [ i , 3 ] . set_title ( 'After Training' ) fig . tight_layout (); CPU times: user 12.9 s, sys: 1.66 s, total: 14.5 s Wall time: 11.5 s The distributions of the weights in the first layer after fitting seem to become multi-modal, with peaks on either side of 0. Why might this be the case? In any event, we can tell from the plots of the model predictions that we still have a lot of room for improvement. All these models seem to be underfit . More epochs could help, but let's first explore different options with our optimizer . Optimizers Recall that backpropogation uses the chain rule to calculate the gradient of the loss with respect to the weights. But it is gradient descent that actually updates the model weights. How this update is performe is defined by the optimizer. In [13]: from tensorflow.keras import optimizers from tensorflow.keras.optimizers import SGD , Adagrad , RMSprop , Adam SGD Gradient descent (with momentum) optimizer. {python} tf.keras.optimizers.SGD( learning_rate=0.01, momentum=0.0, nesterov=False, name=\"SGD\", **kwargs ) For background on nesterov momentum: Sutskever et al., 2013 In SGD gradients are averaged across all $m$ observations in a mini-batch. $g = \\frac{1}{m} \\sum_i \\nabla_W L(f(x_i; W), y_i)$ The comes the update step where the weights are actually adjusted. $W&#94;* = W - \\eta g$ Here, $\\eta$ is our learning rate. Because the gradient is calculated using a random sample (mini-batch), the it is a 'noisy' approximation which can allow us to escape local minima. Here will start with the default parameters to the SGD optimizer. In [14]: inputs = Input ( shape = ( 1 ,)) x = Dense ( 50 , activation = 'tanh' )( inputs ) x = Dense ( 50 , activation = 'tanh' )( x ) outputs = Dense ( 1 , activation = 'linear' )( x ) NN1 = Model ( inputs = inputs , outputs = outputs , name = 'NN1' ) NN1 . save_weights ( 'NN1_init.h5' ) NN1 . summary () Model: \"NN1\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_6 (InputLayer) [(None, 1)] 0 dense_19 (Dense) (None, 50) 100 dense_20 (Dense) (None, 50) 2550 dense_21 (Dense) (None, 1) 51 ================================================================= Total params: 2,701 Trainable params: 2,701 Non-trainable params: 0 _________________________________________________________________ In [15]: NN1 . load_weights ( 'NN1_init.h5' ) NN1 . compile ( optimizer = SGD ( learning_rate = 0.01 , momentum = 0 , nesterov = False , clipnorm = 1 ), loss = 'mse' ) NN1 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), batch_size = 32 , epochs = 50 , verbose = 0 ); In [16]: plot_history ( NN1 ) When performing stochastic gradient descent, batch size is another hyperparameter. It is, after, what makes the optimizer \"stochastic!\" The smaller the batches, the noiser the approximations of the gradient. Note that rather than re-creating the original network each time, we simply load the weights that were stored upon initialization. Less code to type! Batch Size In [17]: %%time fig , axs = plt . subplots ( 2 , 2 , figsize = ( 10 , 8 ), sharex = True , sharey = True ) axs = axs . flatten () batch_sizes = [ 128 , 32 , 8 , 1 ] for ax , batch_size in zip ( axs , batch_sizes ): NN1 . load_weights ( 'NN1_init.h5' ) NN1 . compile ( optimizer = SGD ( learning_rate = 0.01 , momentum = 0 , nesterov = False ), loss = 'mse' ) NN1 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), batch_size = batch_size , epochs = 200 , verbose = 0 ) plot_history ( NN1 , title = f 'Batch Size = { batch_size } ' , ax = ax ) plt . tight_layout () CPU times: user 41 s, sys: 4.79 s, total: 45.8 s Wall time: 33.3 s Learning Rate Vanilla SGD has a fixed learning rate. Let's see how adjusting it as a hyperparameter affects the performance of our model. In [18]: %%time fig , axs = plt . subplots ( 2 , 2 , figsize = ( 10 , 8 ), sharex = True , sharey = True ) axs = axs . flatten () lrs = [ 1e-4 , 1e-3 , 1e-2 , 0.1 ] for ax , lr in zip ( axs , lrs ): NN1 . load_weights ( 'NN1_init.h5' ) NN1 . compile ( optimizer = SGD ( learning_rate = lr , momentum = 0 , nesterov = False ), loss = 'mse' ) NN1 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), batch_size = 32 , epochs = 200 , verbose = 0 ) plot_history ( NN1 , title = f 'Learning Rate = { lr } ' , ax = ax ) plt . tight_layout () CPU times: user 17.9 s, sys: 1.72 s, total: 19.6 s Wall time: 16.1 s Clearly some of these learning rates are too low, causing the loss to decrease very slowly as the weight updates are being scaled down considerably. The learning rate of 0.1 reaches a much lower loss, but it starts to oscillate wildly, apparently bouncing in and out of minima because of the high learning rate. Learning rate decay / scheduling You can use a learning rate schedule to modulate how the learning rate of your optimizer changes over time. You can find more info on schedulers in the Keras documentation Here we will use ExponentialDecay to shrink the learning rate over the course of training. In [19]: from tensorflow.keras.optimizers.schedules import ExponentialDecay In [20]: lr_schedule = ExponentialDecay ( initial_learning_rate = 0.1 , decay_steps = 100 , decay_rate = 0.85 ) NN1 . load_weights ( 'NN1_init.h5' ) NN1 . compile ( optimizer = SGD ( learning_rate = lr_schedule ), loss = 'mse' ) NN1 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), batch_size = 32 , epochs = 400 , verbose = 0 ); plot_history ( NN1 , title = f 'Exponential LR Decay' ) If we were to incorporate information about past updates , might that help us converge faster? Momentum Momentum is the first method for using such past information in our updates. This helps us move along with the general trajectory we've taken so far, with oscillations cancelling themselves out. $$L(W)$$ Updates are made using a weighted average of the current gradient, $g$, and the average \"trend\" seen so far, $v$.\\ $v = \\alpha v + (1-\\alpha)g$ $W&#94;* = W - \\eta v$ In [21]: %%time fig , axs = plt . subplots ( 2 , 2 , figsize = ( 10 , 8 ), sharex = True ) axs = axs . flatten () lrs = [ 1e-4 , 1e-3 , 1e-2 , 0.1 ] for ax , lr in zip ( axs , lrs ): NN1 . load_weights ( 'NN1_init.h5' ) NN1 . compile ( optimizer = SGD ( learning_rate = lr , momentum = 0.9 , nesterov = False ), loss = 'mse' ) NN1 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), batch_size = 32 , epochs = 200 , verbose = 0 ) plot_history ( NN1 , title = f 'Learning Rate = { lr } + Momentum' , ax = ax ) plt . tight_layout () CPU times: user 18 s, sys: 1.69 s, total: 19.6 s Wall time: 16 s What happened in the 4th example above? Let's train again with those parameters and look at the detailed output from each epoch. In [22]: # Inspect strange behavior in 4th plot above NN1 . load_weights ( 'NN1_init.h5' ) NN1 . compile ( optimizer = SGD ( learning_rate = 0.1 , momentum = 0.9 , nesterov = False ), loss = 'mse' ) NN1 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), batch_size = 32 , epochs = 15 , verbose = 1 ) Epoch 1/15 3/3 [==============================] - 0s 43ms/step - loss: 7.0072 - val_loss: 6.4655 Epoch 2/15 3/3 [==============================] - 0s 9ms/step - loss: 5.5418 - val_loss: 5.9347 Epoch 3/15 3/3 [==============================] - 0s 9ms/step - loss: 10.0591 - val_loss: 6.1523 Epoch 4/15 3/3 [==============================] - 0s 9ms/step - loss: 15.9397 - val_loss: 188.1030 Epoch 5/15 3/3 [==============================] - 0s 9ms/step - loss: 65836.7266 - val_loss: 5894116.0000 Epoch 6/15 3/3 [==============================] - 0s 9ms/step - loss: 3131821824.0000 - val_loss: 731959853056.0000 Epoch 7/15 3/3 [==============================] - 0s 9ms/step - loss: 943046036291584.0000 - val_loss: 220918583536910336.0000 Epoch 8/15 3/3 [==============================] - 0s 9ms/step - loss: 284624769660978462720.0000 - val_loss: 66676391461970633555968.0000 Epoch 9/15 3/3 [==============================] - 0s 9ms/step - loss: 85903609459189883185659904.0000 - val_loss: 20123812249870497329675501568.0000 Epoch 10/15 3/3 [==============================] - 0s 9ms/step - loss: 25926867330470899435841941143552.0000 - val_loss: 6073642079803857688343553324875776.0000 Epoch 11/15 3/3 [==============================] - 0s 9ms/step - loss: inf - val_loss: inf Epoch 12/15 3/3 [==============================] - 0s 9ms/step - loss: inf - val_loss: nan Epoch 13/15 3/3 [==============================] - 0s 9ms/step - loss: nan - val_loss: nan Epoch 14/15 3/3 [==============================] - 0s 9ms/step - loss: nan - val_loss: nan Epoch 15/15 3/3 [==============================] - 0s 9ms/step - loss: nan - val_loss: nan Out[22]: It looks like the combination of high learning rate and momentum caused a numerical overflow, likely from hitting very 'steep' parts of the loss surface. It is bad news once you get a nan in your computations. As you can see, the network weights have all become nan as well! In [23]: # Ouch! Numerical overflow did in our network NN1 . get_weights () Out[23]: [array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], dtype=float32), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32), array([[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]], dtype=float32), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32), array([[nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan], [nan]], dtype=float32), array([nan], dtype=float32)] We take steps to prevent outcomes like this though. Gradient Clipping The Keras optimizers have clipping aguments 'hidden in the *kwargs designation of their docstrings. Here we use clipnorm prevent giant gradients (and thus giant weight updates) from causing problems. In [24]: NN1 . load_weights ( 'NN1_init.h5' ) NN1 . compile ( optimizer = SGD ( learning_rate = 0.1 , momentum = 0.9 , nesterov = False , clipnorm = .1 ), loss = 'mse' ) NN1 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), batch_size = 32 , epochs = 200 , verbose = 0 ); plot_history ( NN1 , title = f 'LR=0.1; momentum=0.9; clipnorm=0.1' ) So far, the learning rate in our optimizer has been treating all weights equally. But this doesn't have to be the case. Adagrad Adagrad is an optimizer with parameter-specific learning rates, which are adapted relative to how frequently a parameter gets updated during training. The more updates a parameter receives, the smaller the updates. A weight's learning rate is inversely proportional to the root of its accumulated squared component of the gradient seen thus far. $r&#94;*_i = r_i + g&#94;2_i$ $W&#94;*_i = W_i - \\frac{\\epsilon}{\\delta + \\sqrt{r_i}}g_i$ {python} tf.keras.optimizers.Adagrad( learning_rate=0.001, initial_accumulator_value=0.1, epsilon=1e-07, name=\"Adagrad\", **kwargs ) Duchi et al., 2011 In [25]: NN1 . load_weights ( 'NN1_init.h5' ) NN1 . compile ( optimizer = Adagrad ( learning_rate = 0.01 ), loss = 'mse' ) NN1 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), batch_size = 32 , epochs = 200 , verbose = 0 ) plot_history ( NN1 ) As you can see, there is a problem with this approach. The accumulated gradients can quickly shrink the learning rates to the point where the network is not longer learning anything. But we have a fix for this! RMSPromp RMSprop maintains a moving (discounted) average of the square of gradients and divides the current gradient by the root of this average (RMS = root mean square). $r&#94;*_i = \\rho r_i + (1-\\rho)g&#94;2_i$ $W&#94;*_i = W_i - \\frac{\\epsilon}{\\delta + \\sqrt{r_i}}g_i$ {python} tf.keras.optimizers.RMSprop( learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False, name=\"RMSprop\", **kwargs ) Hinton, 2012 In [26]: NN1 . load_weights ( 'NN1_init.h5' ) NN1 . compile ( optimizer = RMSprop ( learning_rate = 0.01 , rho = 0.9 , momentum = 0.0 ), loss = 'mse' ) NN1 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), batch_size = 32 , epochs = 200 , verbose = 0 ) plot_history ( NN1 ) This works well, but can we also have it use information about the general trajectory or \"trend\" like momentum? Adam Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments (Basically, momentum + RMSProp). {python} tf.keras.optimizers.Adam( learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name=\"Adam\", **kwargs ) Kingma et al., 2014 In [27]: NN1 . load_weights ( 'NN1_init.h5' ) NN1 . compile ( optimizer = Adam ( learning_rate = 0.01 , beta_1 = 0.9 , beta_2 = 0.999 ), loss = 'mse' ) NN1 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), batch_size = 32 , epochs = 200 , verbose = 0 ) plot_history ( NN1 ) Very nice. Perhaps the lowest MSE we've seen so far. How does the fit look? In [28]: plot_lorentz ( df , test_idx = x_test . index ) ax = plt . gca () x_lin = np . linspace ( df . x . min (), df . x . max (), ( 500 )) . reshape ( - 1 , 1 ) y_hat = NN1 . predict ( x_lin ) ax . plot ( x_lin , y_hat , c = 'm' ); Much better than our original polynomial model! Other Optimizer Implementations Available in Keras: Adadelta Improvement on Adagrad. Adapts learning rates based on a moving window instead of accumulating all past gradients. Adamax Variant of Adam based on infinity norm (i.e., max). Nadam Variant of Adam using nesterov momentum Ftrl Implements the \" Follow The Regularized Leader \" algorithm. Overfitting So far we've been looking at an oversimplified dataset where all the datapoints are right on top of the true generating function. But the real world is noisy! In [29]: df2 = pd . read_csv ( 'data/lorentz_noise_set2.csv' ) df2 = df2 . sample ( frac = 1 , random_state = 109 ) # shuffle DataFrame! x_train , x_test , y_train , y_test = train_test_split ( df2 . x , df2 . y , train_size = 0.7 , random_state = 109 ) In [30]: plot_lorentz ( df2 , x_test . index ) How does our previously best performing model fair on this more realistic dataset? In [31]: %%time inputs = Input ( shape = ( 1 ,)) x = Dense ( 50 , activation = 'tanh' )( inputs ) x = Dense ( 50 , activation = 'tanh' )( x ) outputs = Dense ( 1 , activation = 'linear' )( x ) NN2 = Model ( inputs = inputs , outputs = outputs ) NN2 . compile ( optimizer = Adam ( learning_rate = 0.01 , beta_1 = 0.9 , beta_2 = 0.999 ), loss = 'mse' ) NN2 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), batch_size = 32 , epochs = 2000 , verbose = 0 ) plot_history ( NN2 ); CPU times: user 42 s, sys: 3.12 s, total: 45.1 s Wall time: 37.7 s There are clear signs of overfitting as the validation error starts to diverge from the train error. Any \"improvement\" seen with respect to the training data after a certain point no longer generalizes. And after a while, we actually start to see the validation loss increasing. In [32]: # Plot the overfit predictions plot_lorentz ( df2 , test_idx = x_test . index ) ax = plt . gca () x_lin = np . linspace ( df2 . x . min (), df2 . x . max (), ( 500 )) . reshape ( - 1 , 1 ) y_hat = NN2 . predict ( x_lin ) ax . plot ( x_lin , y_hat ); And here we can see the model predictions jump around as it tries to fit the sparse and noisy points in the training data.\\ Luckily we have several tools at our disposal for addressing overfitting in neural networks. Regularization Early Stopping (Keras Callbacks) Training a NN can take a long time. We should checkpoint our model so we don't lose progress and stop early if we don't see improvement to help save on training time. This motivates Keras Callbacks . A callback is an object that can perform actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc). In [33]: from tensorflow.keras.callbacks import EarlyStopping , ModelCheckpoint , LambdaCallback EarlyStopping allows us to end the training process if we haven't seen an improvment in some loss or metric of our choice ( monitor ) for some number of epochs ( patience ). This even gives us the option of reverting back to the best state of the network (according to the chosen metric) when training ends. In [34]: es = EarlyStopping ( monitor = 'val_loss' , patience = 75 , restore_best_weights = True , verbose = 1 ) ModelCheckpoint let's us save our models to disk periodically. For example, you might want to update your saved model file each time there is an improvement. Just image hwo sad it would be if your kernel diedd or there was some other failure thousands of epochs into the training process. Checkpointing can prevent these tragedies. In [35]: # checkpointing actually slows down training # just add it to the call back list if you'd like to include it mc = ModelCheckpoint ( 'data/models' , monitor = 'val_loss' , save_best_only = True , save_weights_only = False ) We can also execute arbitrary functions at different the start/end of a batch/eepoch using the LambdaCallback . In [36]: lcall = LambdaCallback ( on_epoch_end = lambda epoch , logs : print ( f 'epoch { epoch } ' ) if epoch % 50 == 0 else None ) Any callbacks we wish to use during training are passed as a list to the fit function's callbacks argument. In [37]: %%time inputs = Input ( shape = ( 1 ,)) x = Dense ( 50 , activation = 'tanh' )( inputs ) x = Dense ( 50 , activation = 'tanh' )( x ) outputs = Dense ( 1 , activation = 'linear' )( x ) NN_es = Model ( inputs = inputs , outputs = outputs ) NN_es . compile ( optimizer = Adam ( learning_rate = 0.01 , beta_1 = 0.9 , beta_2 = 0.999 ), loss = 'mse' ) NN_es . fit ( x_train , y_train , validation_data = ( x_test , y_test ), batch_size = 32 , epochs = 2000 , callbacks = [ es , lcall ], verbose = 0 ) plot_history ( NN_es , title = \"Early Stopping\" ) epoch 0 epoch 50 epoch 100 epoch 150 epoch 200 epoch 250 Restoring model weights from the end of the best epoch: 218. Epoch 293: early stopping CPU times: user 6.52 s, sys: 472 ms, total: 6.99 s Wall time: 5.92 s In [38]: plot_lorentz ( df2 , test_idx = x_test . index ) ax = plt . gca () x_lin = np . linspace ( df2 . x . min (), df2 . x . max (), ( 500 )) . reshape ( - 1 , 1 ) y_hat = NN_es . predict ( x_lin ) ax . plot ( x_lin , y_hat ); Weight Decay In [39]: from tensorflow.keras.regularizers import L1 , L2 We can also and a penalty term to our loss function that penalizes the model based on the magnitutes of its weights. This forces the network to balance goodness-of-fit to the training data and model complexity, reducing the possibility of overfitting. $L_2$ Regularization Penalty based on the square of network weights. In [40]: %%time kernel_regularizer = L2 ( 0.005 ) bias_regularizer = L2 ( 0.005 ) inputs = Input ( shape = ( 1 ,)) x = Dense ( 50 , activation = 'tanh' , kernel_regularizer = kernel_regularizer , bias_regularizer = bias_regularizer )( inputs ) x = Dense ( 50 , activation = 'tanh' , kernel_regularizer = kernel_regularizer , bias_regularizer = bias_regularizer )( x ) outputs = Dense ( 1 , activation = 'linear' )( x ) NN1 = Model ( inputs = inputs , outputs = outputs ) NN1 . compile ( optimizer = Adam ( learning_rate = 0.01 , beta_1 = 0.9 , beta_2 = 0.999 ), loss = 'mse' ) NN1 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), batch_size = 32 , epochs = 2000 , # callbacks=[es, lcall], verbose = 0 ) plot_history ( NN1 ) CPU times: user 43.3 s, sys: 2.98 s, total: 46.3 s Wall time: 38.4 s $L_1$ Regularization Penalty based on the absolute value of model weights. Like LASSO in linear regression, this tends to push some weights to zero, resulting in \"sparsity\" (i.e., some nodes are \"off\" in the network). In [41]: %%time kernel_regularizer = L1 ( 0.005 ) bias_regularizer = L1 ( 0.005 ) inputs = Input ( shape = ( 1 ,)) x = Dense ( 50 , activation = 'tanh' , kernel_regularizer = kernel_regularizer , bias_regularizer = bias_regularizer )( inputs ) x = Dense ( 50 , activation = 'tanh' , kernel_regularizer = kernel_regularizer , bias_regularizer = bias_regularizer )( x ) outputs = Dense ( 1 , activation = 'linear' )( x ) NN_L1 = Model ( inputs = inputs , outputs = outputs ) NN_L1 . compile ( optimizer = Adam ( learning_rate = 0.01 , beta_1 = 0.9 , beta_2 = 0.999 ), loss = 'mse' ) NN_L1 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), batch_size = 32 , epochs = 2000 , # callbacks=[es, lcall], verbose = 0 ) plot_history ( NN_L1 , title = 'L1 Regularized Model' ) CPU times: user 44.4 s, sys: 3.13 s, total: 47.5 s Wall time: 39.3 s Dropout In [42]: from tensorflow.keras.layers import Dropout The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged. Note that the Dropout layer only applies during training and no values are dropped during inference. Even if individual weight magnitudes are constrained with methods like weight decay above, many weights can \"conspire\" within a network to collectively have a large effect. Dropout helps prevent this because no weight can \"rely\" on any other for its affect as it may be turned off. You can also think of dropout as turning your network into a aggregated ensemble of smaller neural networks! In [43]: %%time dropout_rate = 0.2 inputs = Input ( shape = ( 1 ,)) x = Dense ( 50 , activation = 'tanh' )( inputs ) x = Dropout ( dropout_rate )( x ) x = Dense ( 50 , activation = 'tanh' )( x ) x = Dropout ( dropout_rate )( x ) outputs = Dense ( 1 , activation = 'linear' )( x ) NN_do = Model ( inputs = inputs , outputs = outputs ) NN_do . compile ( optimizer = Adam ( learning_rate = 0.01 , beta_1 = 0.9 , beta_2 = 0.999 ), loss = 'mse' ) NN_do . fit ( x_train , y_train , validation_data = ( x_test , y_test ), batch_size = 32 , epochs = 2000 , # callbacks=[es, lcall], verbose = 0 ) plot_history ( NN_do , title = 'Dropout Model' ) CPU times: user 44.9 s, sys: 3.11 s, total: 48 s Wall time: 40 s Batch Normalization (Batchnorm) When we have inputs on different scales we are advised to standardize them to put everything on the same scale.\\ But now consider that from the perspective of the 2nd layer in your network, the outputs of the first layer are its input features. Should these be standardized for the same reasons we standardize the input to the first layer? Batch Normalization extends the idea of feature rescaling to (potential) all the layers in your network. The \"batch\" in the name refers to the fact the the mean and standard deviation used for the standardization are \"noisy\" approximations calculated from the current mini-batch. This \"noisiness\" can have a slight regularization effect. More importantly, it helps keep the distribution of input values going into a given layer from shifting around. Your layer is trying to learn weights to map its input to the appropriate input, but this is made very difficult if the input values look wildly different after weights in the previous layers are also updated; it's like trying to hit a moving target. Batchnorm is normally placed between the affine transformation of a neuron and its activation function. A batchnorm layer has two trainable parameters which allow the network to learn what the ideal mean and standard deviation are for the data to possess before being passed through the activation function. Consider why this might be important for certain activation functions like relu or sigmoid. If we want to put a batchnorm layer before our activation functions then we can forego specifying an activation in the Dense layer and instead use an Activation layer! In [44]: from tensorflow.keras.layers import Activation , BatchNormalization In [45]: %%time n_nodes = 20 noise_stddev = 0.2 dropout_rate = 0.20 activation = 'relu' inputs = Input ( shape = ( 1 ,)) x = Dense ( n_nodes , activation = None )( inputs ) x = BatchNormalization ()( x ) x = Activation ( activation )( x ) x = Dropout ( dropout_rate , seed = 109 )( x ) for i in range ( 10 ): x = Dense ( n_nodes , activation = None )( x ) x = BatchNormalization ()( x ) x = Activation ( activation )( x ) x = Dropout ( dropout_rate , seed = 209 + i )( x ) outputs = Dense ( 1 , activation = 'linear' )( x ) NN_bn = Model ( inputs = inputs , outputs = outputs ) display ( NN_bn . summary ()) NN_bn . compile ( optimizer = Adam ( learning_rate = 0.01 , beta_1 = 0.9 , beta_2 = 0.999 ), loss = 'mse' ) NN_bn . fit ( x_train , y_train , validation_data = ( x_test , y_test ), batch_size = 512 , epochs = 2000 , callbacks = [ EarlyStopping ( monitor = 'val_loss' , patience = 250 , restore_best_weights = True , verbose = 1 ), lcall ], verbose = 0 ) plot_history ( NN_bn , title = 'Batchnorm Model' ) Model: \"model_5\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_12 (InputLayer) [(None, 1)] 0 dense_37 (Dense) (None, 20) 40 batch_normalization (BatchN (None, 20) 80 ormalization) activation (Activation) (None, 20) 0 dropout_2 (Dropout) (None, 20) 0 dense_38 (Dense) (None, 20) 420 batch_normalization_1 (Batc (None, 20) 80 hNormalization) activation_1 (Activation) (None, 20) 0 dropout_3 (Dropout) (None, 20) 0 dense_39 (Dense) (None, 20) 420 batch_normalization_2 (Batc (None, 20) 80 hNormalization) activation_2 (Activation) (None, 20) 0 dropout_4 (Dropout) (None, 20) 0 dense_40 (Dense) (None, 20) 420 batch_normalization_3 (Batc (None, 20) 80 hNormalization) activation_3 (Activation) (None, 20) 0 dropout_5 (Dropout) (None, 20) 0 dense_41 (Dense) (None, 20) 420 batch_normalization_4 (Batc (None, 20) 80 hNormalization) activation_4 (Activation) (None, 20) 0 dropout_6 (Dropout) (None, 20) 0 dense_42 (Dense) (None, 20) 420 batch_normalization_5 (Batc (None, 20) 80 hNormalization) activation_5 (Activation) (None, 20) 0 dropout_7 (Dropout) (None, 20) 0 dense_43 (Dense) (None, 20) 420 batch_normalization_6 (Batc (None, 20) 80 hNormalization) activation_6 (Activation) (None, 20) 0 dropout_8 (Dropout) (None, 20) 0 dense_44 (Dense) (None, 20) 420 batch_normalization_7 (Batc (None, 20) 80 hNormalization) activation_7 (Activation) (None, 20) 0 dropout_9 (Dropout) (None, 20) 0 dense_45 (Dense) (None, 20) 420 batch_normalization_8 (Batc (None, 20) 80 hNormalization) activation_8 (Activation) (None, 20) 0 dropout_10 (Dropout) (None, 20) 0 dense_46 (Dense) (None, 20) 420 batch_normalization_9 (Batc (None, 20) 80 hNormalization) activation_9 (Activation) (None, 20) 0 dropout_11 (Dropout) (None, 20) 0 dense_47 (Dense) (None, 20) 420 batch_normalization_10 (Bat (None, 20) 80 chNormalization) activation_10 (Activation) (None, 20) 0 dropout_12 (Dropout) (None, 20) 0 dense_48 (Dense) (None, 1) 21 ================================================================= Total params: 5,141 Trainable params: 4,701 Non-trainable params: 440 _________________________________________________________________ None epoch 0 epoch 50 epoch 100 epoch 150 epoch 200 epoch 250 epoch 300 epoch 350 epoch 400 epoch 450 epoch 500 epoch 550 epoch 600 epoch 650 epoch 700 epoch 750 Restoring model weights from the end of the best epoch: 504. Epoch 754: early stopping CPU times: user 22.2 s, sys: 1.66 s, total: 23.9 s Wall time: 19.1 s Data Augmentation In [46]: from tensorflow.keras.layers import GaussianNoise The easiest way to fix overfitting is to use more data. This isn't always an option, but we can often \"fake it\" with some success. The idea behind data augmentation is to produce variations of your existing data that are still close enough to real examples. In this way the model becomes less sensitive to any of the idiosyncratic features of your particular training examples and instead will learn what is essential and common across all your generated data. Perhaps the easiest form of data augmentation is simply adding a bit of gaussian noise to your data points.\\ This is made very easy with Keras's GaussianNoise layer. Just specify a stddev for the noise distribution. And keep in mind that it can take a random seed! In [47]: %%time noise_stddev = 0.025 inputs = Input ( shape = ( 1 ,)) x = GaussianNoise ( stddev = noise_stddev , seed = 109 )( inputs ) x = Dense ( 50 , activation = 'tanh' )( x ) x = Dense ( 50 , activation = 'tanh' )( x ) outputs = Dense ( 1 , activation = 'linear' )( x ) NN_da = Model ( inputs = inputs , outputs = outputs ) NN_da . compile ( optimizer = Adam ( learning_rate = 0.01 , beta_1 = 0.9 , beta_2 = 0.999 ), loss = 'mse' ) NN_da . fit ( x_train , y_train , validation_data = ( x_test , y_test ), batch_size = 32 , epochs = 2000 , # callbacks=[es, lcall], verbose = 0 ) plot_history ( NN_da , title = 'Model /w Data Augmentation (Gaussian Noise)' ) CPU times: user 43.6 s, sys: 3.12 s, total: 46.7 s Wall time: 39.2 s Part 2 - Classification with Neural Networks In this 2nd half of the lab we will be working with the Fashion-MNIST dataset . Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits. We will be using a very small fraction of the dataset. The goal is to first overfit a NN to this classification task and then use the tools explored in the 1st half of the notebook to regularize and improve the model through hyperparameter tweaking. But first, we will explore a new option for image data augmentation through the use of Keras's ImageDataGenerator ojects which may be useful to us. Datagenerators in Keras This section shows how to build data generators from a collection of image files to be used for training a Keras model. The process of creating the file structure required by data generators is normally quite tedious. The code below can be adapted and reused for your own projects to save you a lot of time! (keras ImageGenerator documentation) Fashion-MNIST and other pre-loaded dataset are formatted in a way that is almost ready for feeding into a model. But what about your average image files? They should be appropriately preprocessed into floating-point tensors before being fed into the network. Imagine you have a bunch of dogs and cat images you want to prepare for a Keras model using a ImageDataGenerator We need to create: train, validation, and test directories, each containing a subset of the images. separate cat and dog directories within train, validation, and test Number 2 is necessary because the Keras ImageDataGenerator infers the class label from the subdirectory the image resides in. The directory structure should then look like this: dogs_vs_cats ├── test │ ├── cats │ └── dogs ├── train | ├── cats | └── dogs └── validation ├── cats └── dogs This is what we will do for the Fashion-MNIST data. It would be a lot of work to do it manually, especially with 10 classes! But it isn't to difficult to take care of all of this programatically. In [48]: # Load entire Fashion-MNIST dataset from Tensorflow ( x_train , y_train ), ( x_test , y_test ) = tf . keras . datasets . fashion_mnist . load_data () These are the 10 class labels and the integer they map to in y_train and y_test . In [49]: # Fashion-MNIST class labels labels = [ 'T-shirt/top' , 'Trouser' , 'Pullover' , 'Dress' , 'Coat' , 'Sandal' , 'Shirt' , 'Sneaker' , 'Bag' , 'Ankle boot' ] label2idx = { label : idx for idx , label in enumerate ( labels )} label2idx Out[49]: {'T-shirt/top': 0, 'Trouser': 1, 'Pullover': 2, 'Dress': 3, 'Coat': 4, 'Sandal': 5, 'Shirt': 6, 'Sneaker': 7, 'Bag': 8, 'Ankle boot': 9} Here's some examples from the dataset. In [50]: # Fashion-MNIST examples fig , axs = plt . subplots ( 3 , 5 , figsize = ( 9 , 7 )) for i , ax in enumerate ( axs . ravel ()): ax . imshow ( x_train [ i ], cmap = plt . cm . gray ) ax . set_xticks (()) ax . set_yticks (()) ax . set_title ( labels [ y_train [ i ]]) plt . tight_layout () In [51]: # Dataset size x_train . shape , x_test . shape Out[51]: ((60000, 28, 28), (10000, 28, 28)) Because our goal is to first overfit (and have a model we can train multiple times as we experiment in a 75 minute lab!) we will discard most of the training data. We do stratified splits to avoid biasing our sample. The classes are equally represented in the original dataset. In [52]: # Throw away most of the training data to make things more difficult # We'll also toss most of the test data (writing it all to disk is slow on JupyterHub!) _ , x_test , _ , y_test = train_test_split ( x_test , y_test , test_size = 0.10 , stratify = y_test ) x_train , _ , y_train , _ = train_test_split ( x_train , y_train , train_size = 0.02 , stratify = y_train ) x_train , x_val , y_train , y_val = train_test_split ( x_train , y_train , test_size = 0.2 , stratify = y_train ) In [53]: # Sample size x_train . shape , x_val . shape , x_test . shape Out[53]: ((960, 28, 28), (240, 28, 28), (1000, 28, 28)) The PIL module (Python Image Library) is useful for taking the numpy arrays that currently represent the images and writing them to disk as jpeg files. In [54]: from PIL import Image In [55]: DATA_DIR = 'data/fashionMNIST' # create path name for data In [56]: data = { 'train' : ( x_train , y_train ), 'validation' : ( x_val , y_val ), 'test' : ( x_test , y_test )} In [57]: %%time # Construct directory structure and write image files to disk if not os . path . isdir ( DATA_DIR ): os . makedirs ( DATA_DIR , exist_ok = True ) for split , ( x , y ) in data . items (): for label in labels : label_s = label . replace ( '/' , '_' ) new_dir = os . path . join ( DATA_DIR , split , label_s ) os . makedirs ( new_dir , exist_ok = True ) target = label2idx [ label ] cur_images = x [ y == target ] for i in range ( cur_images . shape [ 0 ]): im = Image . fromarray ( cur_images [ i ]) . convert ( \"L\" ) filename = f \" { label_s } _ { split } _ { i : 04d } \" im . save ( f \" { DATA_DIR } / { split } / { label_s } / { filename } .jpeg\" ) CPU times: user 172 µs, sys: 15 µs, total: 187 µs Wall time: 9.64 ms In [58]: # number of images in each subdir for dir_name in data . keys (): print ( dir_name ) for label in labels : label_dir = label . replace ( '/' , '_' ) print ( ' \\t ' + label_dir , len ( os . listdir ( DATA_DIR + '/' + dir_name + '/' + label_dir ))) train T-shirt_top 96 Trouser 96 Pullover 96 Dress 96 Coat 96 Sandal 96 Shirt 96 Sneaker 96 Bag 96 Ankle boot 96 validation T-shirt_top 24 Trouser 24 Pullover 24 Dress 24 Coat 24 Sandal 24 Shirt 24 Sneaker 24 Bag 24 Ankle boot 24 test T-shirt_top 100 Trouser 100 Pullover 100 Dress 100 Coat 100 Sandal 100 Shirt 100 Sneaker 100 Bag 100 Ankle boot 100 Create the Generators Now that we have the data in the correct directory structure we can create the data generators. Yes, that's correct. We will have multiple generators, one for each split directory. First we create a a main data generator object, datagen . This can be a given a wide range of arguments which can be used to preprocess the images it generates. For right now we will only use the rescale argument to normalize all pixel values to between 0 and 1 (remamber that 255 is the max pixel value). In [59]: from tensorflow.keras.preprocessing.image import ImageDataGenerator The ImageDataGenerator can automate the scaling of normalizing the image pixel values to [0,1]. THis is just a first glipse of how these objects can help with a preprocessing and training pipeline! In [60]: datagen = ImageDataGenerator ( rescale = 1. / 255 ) Now we use datagen 's flow_from_directory method to create the 3 generators: traingen , valgen , and testgen . The need to be given the directory which they will use as their image source, a target_size to resize all images to (e.g., (14,14)), a batch_size , and class_mode to instruct the generator on how to interpret the label folders. We should probably also set shuffle = False in the test generator so it produces the same images in the same order everytime it is used. We also set color_mode='grayscale' . Otherwise the default is to output color images which would be of shape (28,28,3) because they have 3 color channels (more on that next lab!). In [61]: batch_size = 32 target_size = ( 28 , 28 ) # generator can resize all images if we want # this is the augmentation configuration we will use for training traingen = datagen . flow_from_directory ( DATA_DIR + '/train' , # this is the target directory target_size = target_size , # all images will be resized to 150x150 batch_size = batch_size , class_mode = 'categorical' , # since we use categorical_crossentropy loss, we need binary labels color_mode = 'grayscale' ) valgen = datagen . flow_from_directory ( DATA_DIR + '/validation' , target_size = target_size , batch_size = batch_size , class_mode = 'categorical' , color_mode = 'grayscale' ) testgen = datagen . flow_from_directory ( DATA_DIR + '/test' , target_size = target_size , batch_size = batch_size , shuffle = False , class_mode = 'categorical' , color_mode = 'grayscale' ) Found 960 images belonging to 10 classes. Found 240 images belonging to 10 classes. Found 1000 images belonging to 10 classes. Let's look at the output of one of these generators to see how target_size and batch_size affects the output. Note that the generator yields these batches indefinitely: it loops endlessly over the images in the target folder. For this reason, you need to break the iteration loop at some point if using a for loop. Better still, you can use the built in next function to return a signle element from the generator. In [62]: data_batch , labels_batch = next ( traingen ) print ( 'data batch shape:' , data_batch . shape ) print ( 'labels batch shape:' , labels_batch . shape ) data batch shape: (32, 28, 28, 1) labels batch shape: (32, 10) Construct a Classifier NN Let's built our first attempt at a clothing classifier and try to overfit. Note that Keras has a Flatten layer! We can use this to automatically turm input images into 1D arrays.\\ (We'll see how to handle 2D input in future lectures and labs) In [63]: import tensorflow as tf from tensorflow.keras.models import Model from tensorflow.keras.layers import Activation , Input , BatchNormalization , Dense , Dropout , Flatten , GaussianNoise from tensorflow.keras.optimizers import Adam , SGD In [64]: # Overfit Fashion-MNIST Classifier input_dim = data_batch . shape [ 1 :] n_classes = labels_batch . shape [ - 1 ] inputs = Input ( shape = ( input_dim )) flat = Flatten ()( inputs ) x = Dense ( 256 , activation = None , kernel_initializer = 'he_uniform' )( flat ) x = BatchNormalization ()( x ) x = Activation ( 'relu' )( x ) x = Dense ( 128 , activation = None , kernel_initializer = 'he_uniform' )( x ) x = BatchNormalization ()( x ) x = Activation ( 'relu' )( x ) x = Dense ( 64 , activation = None , kernel_initializer = 'he_uniform' )( x ) x = BatchNormalization ()( x ) x = Activation ( 'relu' )( x ) x = Dense ( 32 , activation = None , kernel_initializer = 'he_uniform' )( x ) x = BatchNormalization ()( x ) x = Activation ( 'relu' )( x ) x = Dense ( n_classes , activation = None , kernel_initializer = 'he_uniform' )( x ) x = BatchNormalization ()( x ) outputs = Activation ( 'softmax' )( x ) NN = Model ( inputs , outputs ) NN . compile ( optimizer = Adam (), loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ]) NN . summary () Model: \"model_7\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_14 (InputLayer) [(None, 28, 28, 1)] 0 flatten (Flatten) (None, 784) 0 dense_52 (Dense) (None, 256) 200960 batch_normalization_11 (Bat (None, 256) 1024 chNormalization) activation_11 (Activation) (None, 256) 0 dense_53 (Dense) (None, 128) 32896 batch_normalization_12 (Bat (None, 128) 512 chNormalization) activation_12 (Activation) (None, 128) 0 dense_54 (Dense) (None, 64) 8256 batch_normalization_13 (Bat (None, 64) 256 chNormalization) activation_13 (Activation) (None, 64) 0 dense_55 (Dense) (None, 32) 2080 batch_normalization_14 (Bat (None, 32) 128 chNormalization) activation_14 (Activation) (None, 32) 0 dense_56 (Dense) (None, 10) 330 batch_normalization_15 (Bat (None, 10) 40 chNormalization) activation_15 (Activation) (None, 10) 0 ================================================================= Total params: 246,482 Trainable params: 245,502 Non-trainable params: 980 _________________________________________________________________ Keras Callbacks We'll use early stopping with short patience so we can call it quits early when it looks like we are overfitting so we can save time. In [65]: from tensorflow.keras.callbacks import EarlyStopping , ModelCheckpoint , LambdaCallback es = EarlyStopping ( monitor = 'val_loss' , patience = 5 , restore_best_weights = True ) callbacks = [ es ] Fit Model with Generator Let's fit the model to the data using the generator. You can use fit as before but this time you will pass it generators rather than dataframes or numpy arrays. Because the data is being generated endlessly, the Keras model needs to know how many samples to draw from the generator before declaring an epoch over. This is the role of the steps_per_epoch argument: after having drawn steps_per_epoch batches from the generator—that is, after having run for steps_per_epoch gradient descent steps - the fitting process will go to the next epoch. When using fit , you can pass a validation_data argument, much as with the fit method. It's important to note that this argument is allowed to be a data generator, but it could also be a tuple of Numpy arrays. If you pass a generator as validation_data, then this generator is expected to yield batches of validation data endlessly; thus you should also specify the validation_steps argument, which tells the process how many batches to draw from the validation generator for evaluation In [66]: %%time history = NN . fit ( traingen , steps_per_epoch = traingen . samples // traingen . batch_size , epochs = 30 , validation_data = valgen , validation_steps = valgen . samples // valgen . batch_size , callbacks = callbacks , workers =- 1 , use_multiprocessing = True , verbose = 1 ) Epoch 1/30 30/30 [==============================] - 4s 89ms/step - loss: 1.5283 - accuracy: 0.5250 - val_loss: 1.7427 - val_accuracy: 0.3304 Epoch 2/30 30/30 [==============================] - 1s 49ms/step - loss: 1.0572 - accuracy: 0.7292 - val_loss: 1.4761 - val_accuracy: 0.5938 Epoch 3/30 30/30 [==============================] - 1s 47ms/step - loss: 0.8686 - accuracy: 0.8125 - val_loss: 1.3258 - val_accuracy: 0.6830 Epoch 4/30 30/30 [==============================] - 1s 48ms/step - loss: 0.7378 - accuracy: 0.8594 - val_loss: 1.1504 - val_accuracy: 0.7143 Epoch 5/30 30/30 [==============================] - 1s 48ms/step - loss: 0.7061 - accuracy: 0.8573 - val_loss: 1.0881 - val_accuracy: 0.7009 Epoch 6/30 30/30 [==============================] - 1s 47ms/step - loss: 0.6047 - accuracy: 0.9094 - val_loss: 0.8503 - val_accuracy: 0.8080 Epoch 7/30 30/30 [==============================] - 2s 56ms/step - loss: 0.5776 - accuracy: 0.9146 - val_loss: 0.8590 - val_accuracy: 0.7812 Epoch 8/30 30/30 [==============================] - 1s 48ms/step - loss: 0.5076 - accuracy: 0.9240 - val_loss: 0.9329 - val_accuracy: 0.7455 Epoch 9/30 30/30 [==============================] - 1s 49ms/step - loss: 0.4559 - accuracy: 0.9448 - val_loss: 0.8668 - val_accuracy: 0.7366 Epoch 10/30 30/30 [==============================] - 2s 50ms/step - loss: 0.4175 - accuracy: 0.9521 - val_loss: 0.9613 - val_accuracy: 0.7455 Epoch 11/30 30/30 [==============================] - 1s 49ms/step - loss: 0.4058 - accuracy: 0.9490 - val_loss: 0.8339 - val_accuracy: 0.7455 Epoch 12/30 30/30 [==============================] - 1s 48ms/step - loss: 0.4080 - accuracy: 0.9479 - val_loss: 0.8559 - val_accuracy: 0.7589 Epoch 13/30 30/30 [==============================] - 1s 49ms/step - loss: 0.3778 - accuracy: 0.9521 - val_loss: 0.7683 - val_accuracy: 0.8080 Epoch 14/30 30/30 [==============================] - 1s 48ms/step - loss: 0.3366 - accuracy: 0.9656 - val_loss: 0.7713 - val_accuracy: 0.7991 Epoch 15/30 30/30 [==============================] - 1s 49ms/step - loss: 0.3215 - accuracy: 0.9656 - val_loss: 0.8330 - val_accuracy: 0.7812 Epoch 16/30 30/30 [==============================] - 1s 49ms/step - loss: 0.3106 - accuracy: 0.9708 - val_loss: 0.7864 - val_accuracy: 0.7500 Epoch 17/30 30/30 [==============================] - 1s 48ms/step - loss: 0.2934 - accuracy: 0.9719 - val_loss: 0.7535 - val_accuracy: 0.7812 Epoch 18/30 30/30 [==============================] - 1s 49ms/step - loss: 0.3124 - accuracy: 0.9698 - val_loss: 0.7927 - val_accuracy: 0.7589 Epoch 19/30 30/30 [==============================] - 1s 49ms/step - loss: 0.2677 - accuracy: 0.9781 - val_loss: 0.7035 - val_accuracy: 0.7902 Epoch 20/30 30/30 [==============================] - 2s 51ms/step - loss: 0.2690 - accuracy: 0.9698 - val_loss: 0.7851 - val_accuracy: 0.7679 Epoch 21/30 30/30 [==============================] - 2s 50ms/step - loss: 0.2608 - accuracy: 0.9760 - val_loss: 0.8236 - val_accuracy: 0.7455 Epoch 22/30 30/30 [==============================] - 1s 48ms/step - loss: 0.2589 - accuracy: 0.9729 - val_loss: 0.7968 - val_accuracy: 0.7812 Epoch 23/30 30/30 [==============================] - 1s 49ms/step - loss: 0.2334 - accuracy: 0.9823 - val_loss: 0.7767 - val_accuracy: 0.7768 Epoch 24/30 30/30 [==============================] - 1s 49ms/step - loss: 0.2530 - accuracy: 0.9667 - val_loss: 0.8587 - val_accuracy: 0.7500 CPU times: user 14.3 s, sys: 1.65 s, total: 16 s Wall time: 37.8 s Evaluate the Model In [67]: NN . evaluate ( traingen ) 30/30 [==============================] - 1s 35ms/step - loss: 0.1512 - accuracy: 0.9948 Out[67]: [0.15120545029640198, 0.9947916865348816] In [68]: NN . evaluate ( valgen ) 8/8 [==============================] - 0s 33ms/step - loss: 0.7008 - accuracy: 0.7917 Out[68]: [0.7008152604103088, 0.7916666865348816] In [69]: NN . evaluate ( testgen ) 32/32 [==============================] - 2s 62ms/step - loss: 0.7146 - accuracy: 0.7980 Out[69]: [0.7146075367927551, 0.7979999780654907] Let's plot the loss and accuracy of the model over the training and validation data during training: Plot the Training History In [70]: fig , axs = plt . subplots ( 1 , 2 , figsize = ( 10 , 4 )) print ( f \"Final Val Acc: { history . history [ 'val_accuracy' ][ - 1 ] : .2f } \" ) axs [ 0 ] . plot ( history . history [ 'accuracy' ]) axs [ 0 ] . plot ( history . history [ 'val_accuracy' ]) axs [ 0 ] . set_title ( 'model accuracy' ) axs [ 0 ] . set_ylabel ( 'accuracy' ) axs [ 0 ] . set_xlabel ( 'epoch' ) axs [ 0 ] . legend ([ 'train' , 'validation' ], loc = 'upper left' ) # summarize history for loss axs [ 1 ] . plot ( history . history [ 'loss' ]) axs [ 1 ] . plot ( history . history [ 'val_loss' ]) axs [ 1 ] . set_title ( 'model loss' ) axs [ 1 ] . set_ylabel ( 'loss' ) axs [ 1 ] . set_xlabel ( 'epoch' ) axs [ 1 ] . legend ([ 'train' , 'validation' ], loc = 'upper left' ); Final Val Acc: 0.75 Part 2: Now with Data Augmentation We saw in the lecture on regularization that we can slightly alter our training data to similuate a larger dataset. These means our model is less likely to overfit as there is more it would have to 'memorize.' Take a look again at the ImageDataGenerator Documentation to see what choices we have for changing our data. Are all changes appropriate here? Create a new data generator datagen_aug that preprocesses images with your chosen changes. Then we only need to make a new traingen_aug using flow_from_directory . The preprocessing_function argument can be passed an arbitrary function to act on images pulled from the generator. Here we provide an example of a function to add Gaussian noise. Create Augmenting Generator In [71]: # your code here def add_noise ( img ): VARIABILITY = 20 # customize this deviation = VARIABILITY * np . random . random () noise = np . random . normal ( 0 , deviation , img . shape ) img += noise img = np . clip ( img , 0. , 255. ) return img datagen_aug = ImageDataGenerator ( rescale = 1. / 255 , ## customize these and other parameters # rotation_range=0, # width_shift_range=0, # height_shift_range=0, # shear_range=0, # zoom_range=0, preprocessing_function = add_noise , # horizontal_flip=True, # fill_mode='nearest', ) # We don't want to augment the validation (or test) data traingen_aug = datagen_aug . flow_from_directory ( DATA_DIR + '/train' , target_size = target_size , batch_size = batch_size , class_mode = 'categorical' , color_mode = 'grayscale' ) Found 960 images belonging to 10 classes. These are just a few of the options available (for more, see the Keras documentation). Let's quickly go over this code: rotation_range is a value in degrees (0–180), a range within which to randomly rotate pictures. width_shift and height_shift are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally. shear_range is for randomly applying shearing transformations. zoom_range is for randomly zooming inside pictures. horizontal_flip is for randomly flipping half the images horizontally—relevant when there are no assumptions of - horizontal asymmetry (for example, real-world pictures). fill_mode is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift. Let's look at some augmented images generated from a single source image. In [72]: # Examples of augmented images from tensorflow.keras.preprocessing import image train_gb_dir = os . path . join ( DATA_DIR , 'train' , 'Shirt' ) fnames = [ os . path . join ( train_gb_dir , fname ) for fname in os . listdir ( train_gb_dir )] img_path = fnames [ 3 ] # Chooses one image to augment img = image . load_img ( img_path , target_size = target_size ) # Reads the image and resizes it x = image . img_to_array ( img ) # Converts it to a Numpy array x = x . reshape (( 1 ,) + x . shape ) # Reshapes it i = 0 for batch in datagen_aug . flow ( x , batch_size = 1 ): plt . figure ( i ) imgplot = plt . imshow ( image . array_to_img ( batch [ 0 ]) . convert ( 'L' ), cmap = 'gray' ) i += 1 if i % 4 == 0 : break plt . show () Here you can generate a random augemented image. In [73]: # Sample from augmented data generator example = next ( traingen_aug )[ 0 ][ 0 ] print ( example . shape ) plt . imshow ( example , cmap = 'gray' ); (28, 28, 1) If you train a new network using this data-augmentation configuration, the network will never see the same input twice. But the inputs it sees are obviously very related, because they come from a small number of original images—you can't produce new information, you can only remix existing information. As such, this may not be enough to completely get rid of overfitting. To further fight overfitting and improve the model, use all the tool at your disposal: activation functions, initializers, optimizers, regularization techniques, callbacks, and batchnorm! 2nd FFN Model To see the affect of your choices, you should make several attempts in which you do not change the overall model architecture (i.e., number of layers and nodes). In [ ]: # your code here data_batch , labels_batch = next ( traingen_aug ) print ( 'data batch shape:' , data_batch . shape ) print ( 'labels batch shape:' , labels_batch . shape ) input_dim = data_batch . shape [ 1 :] n_classes = labels_batch . shape [ - 1 ] inputs = Input ( shape = ( input_dim )) flat = Flatten ()( inputs ) x = Dense ( 256 , activation = None , ** params )( flat ) x = Activation ( 'relu' )( x ) x = Dense ( 128 , activation = None , ** params )( x ) x = Activation ( 'relu' )( x ) x = Dense ( 64 , activation = None , ** params )( x ) x = Activation ( 'relu' )( x ) x = Dense ( 32 , activation = None , ** params )( x ) x = Activation ( 'relu' )( x ) x = Dense ( n_classes , activation = None , ** params )( x ) outputs = Activation ( 'softmax' )( x ) NN_aug = Model ( inputs , outputs ) NN_aug . compile ( optimizer = Adam (), loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ]) NN_aug . summary () In [ ]: %%time # your code here history_aug = NN_aug . fit ( traingen_aug , steps_per_epoch = traingen_aug . samples // traingen_aug . batch_size , epochs = 100 , validation_data = valgen , validation_steps = valgen . samples // valgen . batch_size , # callbacks=___, verbose = 0 ) # save model if needed NN_aug . save ( 'data/gen_model_aug.h5' ) In [ ]: fig , axs = plt . subplots ( 1 , 2 , figsize = ( 10 , 4 )) print ( f \"Final Val Acc: { history_aug . history [ 'val_accuracy' ][ - 1 ] : .2f } \" ) axs [ 0 ] . plot ( history_aug . history [ 'accuracy' ]) axs [ 0 ] . plot ( history_aug . history [ 'val_accuracy' ]) axs [ 0 ] . set_title ( 'model accuracy' ) axs [ 0 ] . set_ylabel ( 'accuracy' ) axs [ 0 ] . set_xlabel ( 'epoch' ) axs [ 0 ] . legend ([ 'train' , 'validation' ], loc = 'upper left' ) # summarize history for loss axs [ 1 ] . plot ( history_aug . history [ 'loss' ]) axs [ 1 ] . plot ( history_aug . history [ 'val_loss' ]) axs [ 1 ] . set_title ( 'model loss' ) axs [ 1 ] . set_ylabel ( 'loss' ) axs [ 1 ] . set_xlabel ( 'epoch' ) axs [ 1 ] . legend ([ 'train' , 'validation' ], loc = 'upper left' ); In [ ]: NN_aug . evaluate ( traingen ) In [ ]: NN_aug . evaluate ( valgen ) In [ ]: NN_aug . evaluate ( testgen ) How high are you able to get the validation accuracy? (Can you beat 83%?) Does it translate well to test accuracy after all your optimization and tuning? In [ ]:","tags":"labs","url":"labs/lab06/notebook/"},{"title":"Lab 7: CNNs","text":"Notebooks Lab 7 - CNNs","tags":"labs","url":"labs/lab07/"},{"title":"Lab 7: CNNs","text":"CS109B Introduction to Data Science Lab 7: Convolutional Neural Networks Harvard University Spring 2022 Instructors : Mark Glickman & Pavlos Protopapas Lab Team : Eleni Kaxiras, Marios Mattheakis, Chris Gumb, and Shivas Jayaram Authors : Cedric Flamant, Chris Gumb, Hayden Joy, Eleni Kaxiras, and Pavlos Protopapas In [1]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: Learning Objectives By the end of this section, you should understand how images are represented in Python, have a strong understanding of the 'convolution' operation performed by CNNs, and be able to construct, train, and evaluate a CNN using Keras! Notebook Contents Using SEAS Jupyter Hub {3 min} Working with Images in Python {10 min) Image as Tensors An Image from Scratch Preprocessing: Padding Preprocessing: Normalization An Implementation of \"Convolution\" {20 min} Setting Convolution Parameters Defining Filters: Kernels and Biases Calculating Output Dimensions Convolution Implementation Multiple Convolution Layers Tensorflow Datasets {15 min} Loading Datasets The Dataset Object Take, Cardinality, & Batch Cache, Prefetch, & Shuffle Preprocessing with Datasets Data Augmentation Keras {25 min} Layers of a CNN in Keras Excercise: Defining a Model Training Plotting Training History Evaluating Callbacks Exercise: Improving on Baseline Model Using SEAS JupyterHub-GPU &#94; PLEASE READ : Instructions for Using SEAS JupyterHub SEAS and FAS are providing you with a Jupyter computing environment to use for your CS109B course work. It is accessible from 'JupyterHub-GPU' in the menu on the Canvas course page. The GPU available on these instances allow for much faster NN training. The libraries defined in cs109b.yml (keras, tensorflow, pandas, etc.) are all pre-installed. NOTE : This service funded by SEAS and FAS for the purposes of the class. NOTE NOTE NOTE: You are only to use JupyterHub-GPU for purposes directly related to CS109B coursework. Help us keep this service: Make sure you stop your instance as soon as you do not need it. In [112]: import matplotlib.pyplot as plt import numpy as np from scipy.signal import convolve2d , correlate2d from sklearn.datasets import load_sample_image import tensorflow as tf import tensorflow_addons as tfa import tensorflow_datasets as tfds import os import random as rn Working with Images in Python &#94; Convolutional neural networks were developed for working with image data, having been inspiried by the study of biological vision[ 1 ][ 2 ]. Before we dive into CNNs themselves, we should first learn a bit about how images are represented digitally. Images as Tensors The matplotlib.pyplot module has several functions we can use to work with images in Python: imread , imshow , and imsave . Let's begin by loading an example image using imread and displaying it with imshow . (painting by Mark Rothko ) In [111]: import matplotlib.pyplot as plt import numpy as np In [4]: img = plt . imread ( 'data/rothko.jpg' , ) plt . axis ( 'off' ) plt . imshow ( img ); What more can we learn about this img object that we created by reading in our image file? In [5]: print ( 'Image Type:' , type ( img )) print ( 'Image Shape:' , img . shape ) print ( 'Array Type:' , img . dtype ) print ( f 'Value Range: [ { img . min () } , { img . max () } ]' ) Image Type: Image Shape: (981, 770, 3) Array Type: uint8 Value Range: [0, 238] The image is represented as a 3-dimensional numpy array, or tensor . The 1st dimension is height, the 2nd is width, and the 3rd is the color channel . Each element, or pixel, in the array is an 8-bit, unsigned integer. $2&#94;8 = 256$ so a pixel can take on values from 0 and 255. Because the images a numpy array we can use familiar Python indexing to specify certain sections. Let's explore the image's color channels. In [6]: fig , axs = plt . subplots ( 1 , 3 ) rgb_list = [ 'Reds' , 'Greens' , 'Blues' ] for i , ax in enumerate ( axs . ravel ()): ax . imshow ( img [:,:, i ], cmap = rgb_list [ i ]) ax . axis ( 'off' ) ax . set_title ( rgb_list [ i ]) plt . tight_layout () When we pass a tensor with 3 channels to imshow it assumes these are red , green , and blue . However, for inputs with a different number of channels we need to use the cmap argument to specify the color gradient to map the pixel values onto. For example, there is nothing intrinsically green about the 2nd color channel plucked from our original image. It's just a 2-D array of integers where each integer represents the intensity of the pixel, but it contains no information about the color. If we want imshow to interpret it as green, or any other color, then we need to make that explicit. Note: Some image formats such as png can have a 4th channel referred to as alpha which controls transparency. imshow knows how to handle these 4-channel images, but it is important to keep this in mind lest the extra channel catch you by surprise. An Image from Scratch We can use what we now know about an image's digital representation to build an image from scratch by creating and manipulating a numpy array. Let's create a 5x5 RGB image made from scratch using numpy and call it toy_img . The red channel will contain a diagonal line starting in the upper right corner \\\\ The green channel will contain a 'backwards' diagonal line starting in the lower right corner **/** The blue channel will have a vertical line in the middle of the image **|** To keep it simple, all pixels in each channel will be either fully on or off We'll use 1 for all the 'on' pixels and 0 for 'off' pixels. np.zeros() can create an image of all 'off' pixels; we can then 'turn some on' using slicing/indexing np.eye(N) creates an identity NxN matrix np.flipud(a) flips the array a upsidedown In [7]: toy_img = np . zeros (( 5 , 5 , 3 )) # blank image of all 'off' pixels # RED toy_img [:,:, 0 ] = np . eye ( 5 ) # diagonal of red # GREEN toy_img [:,:, 1 ] = np . flipud ( np . eye ( 5 )) # backwards diagonal of green # BLUE toy_img [:, 2 , 2 ] = 1 # vertical line of blue in the middle plt . imshow ( toy_img ); Looking good! Note how the center pixel where all 3 channels are set to 1 is white. But if we are thinking of toy_img as a 3-channel image then something unintuitive happens when we try and display the image as a 3d numpy array (i.e., tensor). In [8]: # Display 5x5x3 toy image tensor toy_img Out[8]: array([[[1., 0., 0.], [0., 0., 0.], [0., 0., 1.], [0., 0., 0.], [0., 1., 0.]], [[0., 0., 0.], [1., 0., 0.], [0., 0., 1.], [0., 1., 0.], [0., 0., 0.]], [[0., 0., 0.], [0., 0., 0.], [1., 1., 1.], [0., 0., 0.], [0., 0., 0.]], [[0., 0., 0.], [0., 1., 0.], [0., 0., 1.], [1., 0., 0.], [0., 0., 0.]], [[0., 1., 0.], [0., 0., 0.], [0., 0., 1.], [0., 0., 0.], [1., 0., 0.]]]) What's going on here! We see 5 matrices, each 5x3. It is hard to tell by looking at this just what is going on in each color channel. The problem is that numpy is splitting the display based on the first dimension (by row). You can think of this as treating the first dimension as though that were the channel. But, as long as we are thinking about this tensor as an an image with the last diemension specifying the color channel, we would prefer to split the display by this last dimension. The show_channels() helper function does just that by swaping the first and last axes in the tensor and then printing the result. In [9]: # Helper function to display image tensor split into color channels def show_channels ( tensor ): channel_first = np . rollaxis ( tensor , 2 , 0 ) print ( channel_first ) In [10]: # Display color channels as matrices show_channels ( toy_img ) [[[1. 0. 0. 0. 0.] [0. 1. 0. 0. 0.] [0. 0. 1. 0. 0.] [0. 0. 0. 1. 0.] [0. 0. 0. 0. 1.]] [[0. 0. 0. 0. 1.] [0. 0. 0. 1. 0.] [0. 0. 1. 0. 0.] [0. 1. 0. 0. 0.] [1. 0. 0. 0. 0.]] [[0. 0. 1. 0. 0.] [0. 0. 1. 0. 0.] [0. 0. 1. 0. 0.] [0. 0. 1. 0. 0.] [0. 0. 1. 0. 0.]]] Now we can clearly see our 3 color channels. Preprocessing: Padding Most networks you'll encounter expect inputs of a fixed dimension . This can make life difficult if you want to train your CNN on a set of images of varying sizes. One solution would be to crop all images down to the same size, but this forces us to throw away what could be useful information. A more palatable option is to pad our images by adding a border of zeros. Previously, in CS109A, we learned how to add a column of ones to our NN's input design matrix with np.ones() . This allowed for multiplication with the bias terms during the forward pass. We could use a similar approach to add a padding of zeros around our image using np.zeros but that would be tedious! Luckily there is np.pad() : https://numpy.org/doc/stable/reference/generated/numpy.pad.html But the use of this function can be a bit confusing at first, especially if you want to very the amount of padding on each side of the image, or if you are working with higher dimensional tensors. First, let's practice by creating a 3x3 matrix of ones and give it a padding of 2 on all sides. In [11]: # original a = np . ones (( 3 , 3 )) a Out[11]: array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]) In [12]: # padded np . pad ( a , 2 ) Out[12]: array([[0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0.], [0., 0., 1., 1., 1., 0., 0.], [0., 0., 1., 1., 1., 0., 0.], [0., 0., 1., 1., 1., 0., 0.], [0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0.]]) Now for some more complicated padding: 1 padding on top 2 badding on bottom 3 padding on left 4 padding on right Display the resulting matrix Note: Don't forget to look at the np.pad() documentation, either with Shift+Tab in Jupyter , or by following link above. The pad_width argument takes a $n$-tuple of 2-tuples, where $n$ is the number of dimensions and the 2-tuples describe how much padding to add to the beginning and end of that dimension. So in this case our pad_width looks like ((top, bottom), (left, right)) In [13]: # padding a 2-D array np . pad ( a , pad_width = (( 1 , 2 ),( 3 , 4 ))) Out[13]: array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 1., 1., 1., 0., 0., 0., 0.], [0., 0., 0., 1., 1., 1., 0., 0., 0., 0.], [0., 0., 0., 1., 1., 1., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) Now we add the same padding above to toy_img and store the result in toy_img_pad . Note: Consider the dimensions of toy_img . We need another 2-tuple for the channel dimension, but we wouldn't want to pad this dimension! In [14]: # padding a 3-D tensor toy_img_pad = np . pad ( toy_img , (( 1 , 2 ), ( 3 , 4 ), ( 0 , 0 ))) In [15]: # display the tensor by channel show_channels ( toy_img_pad ) [[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]] In [16]: # display as an image plt . imshow ( toy_img_pad ); Preprocessing: Normalizing If we're going to be using our images as input to a neural network we should normalize all pixel values to between 0 and 1. The helps keeps our gradients 'well behaved.' We saw earlier that the pixels are unsigned 8-bit integers and so can take on values between 0 and 255. Now that we know the max possible value normalization is easy. We simply divide the tensor representing the image by 255! imgshow() is clever enough to display normalized images exaclty the same as it displays the unnormalized version. In [17]: img_norm = img / 255. fig , axs = plt . subplots ( 1 , 2 ) for ax , image , title in zip ( axs , [ img , img_norm ], [ 'unnormalized' , 'normalized' ]): ax . imshow ( image ) ax . set_title ( title ) ax . axis ( 'off' ) We can take what we've learned to make a 'forgery' of the original Rothko painting! 🤖🎨🖌️ 🖼️ In [18]: # our blank 'canvas' of black (i.e., zeros) rothko2 = np . zeros (( 8 , 5 , 3 )) # some red horizontal lines rothko2 [: 3 ,:, 0 ] = 1 # some green rothko2 [ 4 ,:,: 2 ] = 1 # and some blue rothko2 [ 6 :,:, 1 :] = 1 # add a black border rothko2 = np . pad ( rothko2 ,(( 1 , 1 ),( 1 , 1 ),( 0 , 0 ))) # And voilà! plt . imshow ( rothko2 ); Not too bad! And we see just what we'd expect when view the individual color channels. In [19]: fig , axs = plt . subplots ( 1 , 3 ) rgb_list = [ 'Reds' , 'Greens' , 'Blues' ] for i , ax in enumerate ( axs . ravel ()): ax . imshow ( rothko2 [:,:, i ], cmap = rgb_list [ i ]) ax . axis ( 'off' ) ax . set_title ( rgb_list [ i ]) In [20]: show_channels ( rothko2 ) [[[0. 0. 0. 0. 0. 0. 0.] [0. 1. 1. 1. 1. 1. 0.] [0. 1. 1. 1. 1. 1. 0.] [0. 1. 1. 1. 1. 1. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 1. 1. 1. 1. 1. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 1. 1. 1. 1. 1. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 1. 1. 1. 1. 1. 0.] [0. 1. 1. 1. 1. 1. 0.] [0. 0. 0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 1. 1. 1. 1. 1. 0.] [0. 1. 1. 1. 1. 1. 0.] [0. 0. 0. 0. 0. 0. 0.]]] Finally, let's save our work as a PNG for posterity with imgsave and ensure that we've done so correctly by reading and displaying it again. In [21]: plt . imsave ( 'data/rothko2.png' , rothko2 , vmin = 0 , vmax = 1 , format = 'PNG' ) test_load = plt . imread ( 'data/rothko2.png' ) plt . imshow ( test_load ); But what do we see if we inspect the representation of the loaded PNG? It's that tricky 4th \"color\" channel, alpha! In [22]: show_channels ( test_load ) [[[0. 0. 0. 0. 0. 0. 0.] [0. 1. 1. 1. 1. 1. 0.] [0. 1. 1. 1. 1. 1. 0.] [0. 1. 1. 1. 1. 1. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 1. 1. 1. 1. 1. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 1. 1. 1. 1. 1. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 1. 1. 1. 1. 1. 0.] [0. 1. 1. 1. 1. 1. 0.] [0. 0. 0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 1. 1. 1. 1. 1. 0.] [0. 1. 1. 1. 1. 1. 0.] [0. 0. 0. 0. 0. 0. 0.]] [[1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1.]]] An Implementation of \"Convolution\" &#94; A convolutional layer is composed of filters which are composed of kernels which are themselves composed of weights . Each filter also has a bias term though it is often not depicted in diagrams (it is exluded in the one above for example). We learn the weights and biases from our data. Each conv layer also has an associated activation function such as ReLU or sigmoid. In Keras, the number of filters and the height and width of the kernels of which they consist are set by the filters (int) and kernel_size (int or tuple) arguments respectively. The depth of the filters is fixed by the depth (i.e., number of 'channels' or 'filter maps') of the input to the conv layer. The output of the conv layer, if we have multiple filters, is is a 3D tensor which is a set of feature maps . Each feature map is itself the output of one of the layer's filters \"convolving\" on the input. The height and width of the output feature map tensor is a function of the input size, kernel_size , padding , and stride . The depth of the output tensor (i.e, number of feature maps) is equal to the number of filters in the layer. The best way to build intuition for what the covolution layers in Keras are (roughly) doing is to step through a basic implementation of our own. We'll construct a basic implementation and explore the affect of some different filters and parameters on the output. For this section we'll use this lovely photograph of a pagoda that comes with sklearn . In [23]: from sklearn.datasets import load_sample_image # normalized image img = load_sample_image ( 'china.jpg' ) / 255. plt . axis ( 'off' ) plt . imshow ( img ); Convolution Parameters We'll need to set some basic parameters for our convolution. Number of Filters : determines the number of output feature maps Kernel Dimensions : height and width of the filters (depth determined by input depth) Padding : extra pixels added to the border of the input. Typically these are zeros. Common paddings settings are 'valid' (sometimes called 'full') which ensures every pixel of the input is visited the same number of tiems by the filter, and 'same' which ensures the output is the same size (height and width) as the input. Stride : how large of a 'step' the filter takes when convolving. Almost anything will work here and we can come back to play with these values later. But let's start simple. In [24]: # basic convolution parameters n_filters = 3 # number of filters (each producing a feature map) k_dim = ( 3 , 3 ) # kernel height and width padding = 1 # zeros to add around border strides = 1 # size of step when convolving Defining Filters: Kernels and Biases We will store the weights of our filters in a 4D tensor (which is also how Keras stores the weights in its Conv2D layers as we'll see later). We'll call our weight tensor filters . The dimensions are: (number of filters) x (kernel height) x (kernel width) x (filter depth) For our example, filters will contain 3 identical filters, each composed of 3 identical kernels, and the values in each filter will sum to 1. Each filter also has a bias term. We'll create biases to be a numpy array of zeros (you can also try changing it later to see what affect it has). In [25]: # filters is a 4D tensor # Number of filters x kernel height x kernel width x filter depth filters = np . ones (( n_filters , * k_dim , img . shape [ - 1 ])) / np . prod ([ * k_dim , img . shape [ - 1 ]]) # biases is a 1d array; there is one bias for each filter biases = np . zeros ( n_filters ) Q: Why is it important that the weights in each kernel sum to one? What would happen if they are larger or smaller? 🤔 Q: Why might we want 3 identical filters here? 🤔 Calculating Output Dimensions Calculating the output dimensions based on our input image and parameters will be essential for implementing convolution. The function should throw an error if the dimensions are not integers. We can refer to the equation: $$O = \\frac{W - K + 2P}{S} + 1$$ Where $O$ is the output dim, $W$ is the input dim, $K$ is the filter size, $P$ is padding, and $S$ is the stride. In [26]: # Calculates output shape based on input and conv parameters def output_dim ( img : np . array , filters : np . array , padding : int , strides : int ): w = np . array ( img . shape [: - 1 ]) k = filters [ 0 ] . shape [: - 1 ] out_dim = ( w - k + 2 * padding ) / strides + 1 assert ( out_dim % 1 ) . all () == 0 , \"Calculated output dimensions not integer valued\" return out_dim . astype ( int ) In [27]: out_dim = output_dim ( img , filters , padding , strides ) out_dim Out[27]: array([427, 640]) \"Convolution\" Here we'll implement the \"convolution\" (as that term is used in CNNs) algorithm as a function. We'll make use of out_dim and what we learned about np.pad . We won't implement 'valid' or 'same' padding, only a uniform amount of padding provided to all sides of the input image. In [28]: def convolution ( img , filters , biases = None , padding = 1 , strides = 1 ): # pad input image img = np . pad ( img , (( padding , padding ), ( padding , padding ), ( 0 , 0 ))) # get new input dimensions after padding input_dim = np . array ( img . shape ) # determine output dimensions filter_map_dim = output_dim ( img , filters , padding , strides ) # find dimensions of individual filter k_dim = filters . shape [ 1 :] # if no biases specified, set them to zero (num biases == num kernels) if biases is None : biases = np . zeros ( filters . shape [ 0 ]) # create empty array to store the convolution results filter_maps = np . ones ( shape = ( * filter_map_dim , len ( filters ))) # loop over filters for f in range ( filters . shape [ 0 ]): # init y pos of upper left corner of filter on input image to 0 input_y = filter_maps_y = 0 # loop over rows until we run off the bottom while input_y + k_dim [ 0 ] <= input_dim [ 0 ]: # init x pos of upper left corner of filter on input image to 0 input_x = filter_maps_x = 0 # loop over columns until we run off the side while input_x + k_dim [ 1 ] <= input_dim [ 1 ]: # x & y coordinates of input that filter currently 'sees' xs = slice ( input_x , input_x + k_dim [ 0 ]) ys = slice ( input_y , input_y + k_dim [ 1 ]) # elementwise multiplication, add bias, sum, and store in output filter_maps [ filter_maps_y , filter_maps_x , f ] = np . sum ( img [ ys , xs , :] * filters [ f ]) + biases [ f ] # advance column based on stride input_x += strides # NOTE: movement over output img NOT based on stride! filter_maps_x += 1 # advance row based on stride input_y += strides # NOTE: movement over output img NOT based on stride! filter_maps_y += 1 return filter_maps In [ ]: %%time out = convolution ( img , filters , biases , padding , strides ) def show_result ( img , out ): fig , axs = plt . subplots ( 1 , 2 , figsize = ( 20 , 20 )) out = np . clip ( out , 0. , 1. ) imgs = [ img , out ] titles = [ 'original' , 'output' ] for i , title in enumerate ( titles ): for spine in axs [ i ] . spines . values (): spine . set_visible ( False ) axs [ i ] . imshow ( imgs [ i ]) axs [ i ] . set_title ( title ) show_result ( img , out ) We can experiment with different values for the stride and padding and seeing how this affects the output. You can also go back and change k_dim and rerun all the following cells to see how larger/smaller kernels affect the output. In [ ]: %%time # CONFIGURE CONV HERE n_filters = ___ k_dim = ( _ , _ ) padding = ___ strides = ___ filters = np . ones (( n_filters , * k_dim , img . shape [ - 1 ])) / np . prod ([ * k_dim , img . shape [ - 1 ]]) biases = np . zeros ( n_filters ) out = convolution ( img , filters , biases , padding , strides ) show_result ( img , out ) Q: Why is the resulting image black & white? 🤔 Hint: Consider the operation that takes place when were are calculating each pixel value of the output image and think back to the white pixel in the toy_image we made earlier. For variety, we'll make a copy of kernels using np.copy() and save it as new_kernels . Scale down the values in each kernels in new_kernels by a different value for each kernel. Now rerun the convolution. How does this affect the output image? Is it still black & white? If not, what else is wrong? In [31]: # your code here new_filters = filters . copy () new_filters [ 0 ] *= ___ new_filters [ 1 ] *= ___ new_filters [ 2 ] *= ___ In [32]: out2 = convolution ( img , new_filters , biases ) In [ ]: show_result ( img , out2 ) Sharpen Filter & Polychromatic Output Now we'll create a 3D filter from the provided 2D sharp_kernel that will only affect the red color channel and save it as filter_R . Hint: np.pad() and np.reshape() may be useful. In [34]: sharp_kernel = np . array ([[ 0. , - 1. , 0. ], [ - 1. , 5. , - 1. ], [ 0. , - 1. , 0. ]]) In [35]: filter_R = np . pad ( sharp_kernel . reshape ( * sharp_kernel . shape , - 1 ), (( 0 , 0 ), ( 0 , 0 ), ( 0 , 2 ))) In [36]: show_channels ( filter_R ) [[[ 0. -1. 0.] [-1. 5. -1.] [ 0. -1. 0.]] [[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]] [[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]]] Again using sharp_filter as a starting point, use a loop to create a 4D tensor sharp_filters where each kernel only operates on one of the 3 color channels. In [37]: sharp_filters = np . zeros (( n_filters , * sharp_kernel . shape , img . shape [ - 1 ])) for i in range ( sharp_filters . shape [ - 1 ]): sharp_filters [ i ,:,:, i ] = sharp_kernel In [38]: for i , color in enumerate ([ 'red' , 'green' , 'blue' ]): print ( color ) show_channels ( sharp_filters [ i ]) print () red [[[ 0. -1. 0.] [-1. 5. -1.] [ 0. -1. 0.]] [[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]] [[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]]] green [[[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]] [[ 0. -1. 0.] [-1. 5. -1.] [ 0. -1. 0.]] [[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]]] blue [[[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]] [[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]] [[ 0. -1. 0.] [-1. 5. -1.] [ 0. -1. 0.]]] Now run the convolution again using your new sharp filters. In [39]: out_sharp = convolution ( img , sharp_filters ) In [ ]: show_result ( img , out_sharp ) Later, on your own, you should also try experimenting with how altering parameters like kernel size and stride can affect the output image when using the sharpen filter. Multiple Convolution Layers The true power of CNNs comes from their 'deep' architecture. That is, when we stack convolution operations one after the other with the output feature maps of one convolutional 'layer' serving as the input to the next convolutional layer. In this next example we'll do just that. Creating a 2 layer CNN and inspecting the feature map we get at as output from the second layer. If each layer can be though of as learning certain features of the image, then layers deeper in the network are learning more complex and abstract features: features of features! 🤯 We'll use a picture of Harvard's Widener Library as our input image: In [41]: img = plt . imread ( \"data/Widener_Library.jpg\" ) # normalize img = img / 255. plt . imshow ( img ); 1st CNN Layer Our first CNN layer will consist of 2 3x3x3 filters : one for vertical edge detection and another for horizontal edge detection . Each filter is composed of 3 identical 3x3 kernels, one for each of the input image's color channels. (see https://en.wikipedia.org/wiki/Kernel_(image_processing ) for kernel examples) We apply each filter to the input image like a CNN would: Convolution Add Bias Activation Function We'll use the ReLU activation function where $\\textrm{ReLU}(x) = \\textrm{max}(0,x)$ In [42]: def relu ( x ): return np . clip ( x , a_min = 0. , a_max = None ) x = np . linspace ( - 1 , 1 ) plt . plot ( x , relu ( x )) plt . title ( 'ReLU Activation' ); In [43]: from scipy.signal import convolve2d , correlate2d Let's use the vertical edge detection filter first, saving the resulting feature map as vedges . Implementation Notes: The version of convolution we implemented in the previous section may served to illustrate the operation, but with its nested loop structure it is far from efficient. For the following section we'll use scipy.signal.convolve2d for our convolution. mode='same' pads the input to ensure the output has the same height and width. Because all the kernels in the filter are identical, we can just define one kernel and convolve it over each color channel in turn. We are adding a bias of 0 which has no effect. We only include it to replicate the operations of a CNN layer. In [44]: # Apply a Vertical Edge detection filter vedge_kernel = np . array ( [[ 1 , 0 , - 1 ], [ 2 , 0 , - 2 ], [ 1 , 0 , - 1 ]]) / 3. # to store our output feature map vedges = np . zeros_like ( img ) # elementwise multiplication for i in range ( 3 ): # vedges[:,:,i] = convolve2d(img[:,:,i], vedge_kernel, mode='same') vedges [:,:, i ] = correlate2d ( img [:,:, i ], vedge_kernel , mode = 'same' ) # sum results across channels, producing a flat feature map vedges = vedges . sum ( - 1 ) # add a bias bias = 0 vedges += bias # ReLu activation vedges = relu ( vedges ) We then perform the same operation with the horizontal edge filter on img , saving the resulting feature map as hedges . In [45]: # Apply a Horizontal Edge detection filter hedge_kernel = np . array ( [[ 1 , 2 , 1 ], [ 0 , 0 , 0 ], [ - 1 , - 2 , - 1 ]]) / 3. hedges = np . zeros_like ( img ) for i in range ( 3 ): # hedges[:,:,i] = convolve2d(img[:,:,i], hedge_kernel, mode='same') hedges [:,:, i ] = correlate2d ( img [:,:, i ], hedge_kernel , mode = 'same' ) hedges = hedges . sum ( - 1 ) bias = 0 vedges += bias hedges = relu ( hedges ) Finally we visualize the two feature maps. We'll show them in black & white by setting cmap='gray' in our call to imshow . This is just convention; we could use any color map we like. In [46]: # visualize the 2 feature maps fontsize = 20 f , ax = plt . subplots ( 1 , 2 , figsize = ( 13 , 8 )) ax [ 0 ] . imshow ( np . clip ( vedges , a_min = None , a_max = 1. ), cmap = 'gray' ) ax [ 0 ] . set_title ( 'Vertical Edge Detection' , { 'fontsize' : fontsize }) ax [ 0 ] . axis ( 'off' ) ax [ 1 ] . imshow ( np . clip ( hedges , a_min = None , a_max = 1. ), cmap = 'gray' ) ax [ 1 ] . set_title ( 'Horizontal Edge Detection' , { 'fontsize' : fontsize }) ax [ 1 ] . axis ( 'off' ) plt . tight_layout () 2nd CNN Layer Together, vedges and hedges could be the output of the first layer of a CNN with 2 filters. Now we will investigate what can happen when we stack convolution layers. To achieve this, we'll concatenate vedges and hedges in a third dimension, calling the output feature_maps . feature_maps should have dimensions (267, 400, 2). In [47]: # Concatenate the feature maps feature_maps = np . concatenate (( np . expand_dims ( vedges , axis =- 1 ), np . expand_dims ( hedges , axis =- 1 )), axis =- 1 ) print ( 'Feature Map Shape:' , feature_maps . shape ) Feature Map Shape: (267, 400, 2) Now we take the following 3x3x2 identity filter $A_{ijk}$: $$ A_{:,:,1} = A_{:,:,2} = \\left( \\begin{array}{ccc} 0 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\\\ \\end{array} \\right), $$ and apply it to feature_maps . This time, we will use a bias of -2 as it produces a nice contrast in the final image. We then pass result through a ReLU, saving the output in variable outmap . In [48]: # convolve to produce output of 2nd CNN layer corner_kernel = np . array ([[[ 0 , 0 , 0 ], [ 0 , 1 , 0 ], [ 0 , 0 , 0 ]], [[ 0 , 0 , 0 ], [ 0 , 1 , 0 ], [ 0 , 0 , 0 ]]]) outmap = np . zeros ( feature_maps . shape [: 2 ]) for i in range ( 2 ): # outmap[:,:] += convolve2d(feature_maps[:,:,i], corner_kernel[:,:,i], mode='same') outmap [:,:] += correlate2d ( feature_maps [:,:, i ], corner_kernel [:,:, i ], mode = 'same' ) bias = - 2 outmap += bias outmap = relu ( outmap ) Now that the image has passed through 2 CNN layers. Before visualizing you results, what do expect to see? What would you call the kind of feature represented in our final output feature map? (Think about the input feature maps and what our final CNN layer is doing.) In [49]: # plot output of 2nd CNN layer fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) ax . imshow ( np . clip ( outmap , a_min = 0 , a_max = 1. ), cmap = 'gray' ) ax . set_title ( 'What Might This Feature Be?' , { 'fontsize' : fontsize }) ax . axis ( 'off' ) plt . tight_layout () Tensorflow Datasets &#94; TensorFlow Datasets (TFDS) is a collection of datasets ready to use, with TensorFlow or other Python ML frameworks. These datasets are exposed as tf.data.Dataset objects, enabling easy-to-use and high-performance input pipelines. In [50]: import tensorflow as tf import tensorflow_datasets as tfds To improve TensorFlow's performance in our current environment we can run the following commands. Don't fret too much over what all these commands are doing. There are links in the comments if you'd like to learn more. In [ ]: # Enable/Disable Eager Execution # Reference: https://www.tensorflow.org/guide/eager # TensorFlow's eager execution is an imperative programming environment # that evaluates operations immediately, without building graphs tf . compat . v1 . enable_eager_execution () # Better performance with the tf.data API # Reference: https://www.tensorflow.org/guide/datac_performance AUTOTUNE = tf . data . experimental . AUTOTUNE In [113]: # Ensure replicable results import os import random as rn SEED = 109 tf . random . set_seed ( SEED ) os . environ [ 'PYTHONHASHSEED' ] = '0' os . environ [ 'CUDA_VISIBLE_DEVICES' ] = '' tf . random . set_seed ( SEED ) np . random . seed ( SEED ) rn . seed ( SEED ) Loading Datasets TFDS gives us access to dozens of research quality datasets with the simple `tfds.load` method. An extensive catalogue of datasets can be seen here . You can even write your own custom dataset . ### 🐎 or 🧍? But it doesn't mean you should just because you can. The bizzare Horses or Humans dataset may just be an example of this. Our call to tfds.load will use several arguments: name : (str) the dataset to load (you can look these up in the above catalogue) split : (list) some datasets have pre-specified splits; this list defines which splits to load shuffle_files : (bool) files are loaded in random order as_supervised : (bool) loads labels if dataset has them with_info : (bool) also returns an DatasetInfo object with details about the loaded dataset In [52]: tfds . disable_progress_bar () In [ ]: ( ds_train , ds_test ), ds_info = tfds . load ( name = \"horses_or_humans\" , split = [ 'train' , 'test' ], shuffle_files = True , as_supervised = True , with_info = True , ) The ds_info we got from using with_info=True gives us a great overview of some facts about the dataset. In [54]: ds_info Out[54]: tfds.core.DatasetInfo( name='horses_or_humans', full_name='horses_or_humans/3.0.0', description=\"\"\" A large set of images of horses and humans. \"\"\", homepage='http://laurencemoroney.com/horses-or-humans-dataset', data_path='/home/u_10914655/tensorflow_datasets/horses_or_humans/3.0.0', download_size=153.59 MiB, dataset_size=153.53 MiB, features=FeaturesDict({ 'image': Image(shape=(300, 300, 3), dtype=tf.uint8), 'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2), }), supervised_keys=('image', 'label'), disable_shuffling=False, splits={ 'test': , 'train': , }, citation=\"\"\"@ONLINE {horses_or_humans, author = \"Laurence Moroney\", title = \"Horses or Humans Dataset\", month = \"feb\", year = \"2019\", url = \"http://laurencemoroney.com/horses-or-humans-dataset\" }\"\"\", ) The Dataset Object We'll be working with closely with the tf.data.Dataset object so we should learn more about its methods and structure. Some Python Arcana: Iterables & Iterators : The tf.data.Dataset object is an iterable , which means it implements an __iter__ method which returns an iterator object. An iterator is an object that implements a __next__ method which returns the next element in the iterator! In [55]: # create iterator from iterable my_iter = iter ( ds_train ) my_iter Out[55]: In [56]: # get next element from iterator next_one = next ( my_iter ) print ( f 'Each element in the iterator is of type { type ( next_one ) } with length { len ( next_one ) } ' ) Each element in the iterator is of type with length 2 In [57]: # display element from iterator next_one Out[57]: ( , ) Inspecting this tuple we see the 1st element is our image and the 2nd is the lable. Let's visualize the image. In [58]: image , label = next_one plt . imshow ( image ) plt . title ( int ( label )); It appears humans are the positive class. Let's make a dictionary to map class labels to strings. In [59]: # create human interperatable class names class_names = { 0 : 'horse' , 1 : 'human' } Iterating Iterables can be looped over with a for loop. And while you can loop over the Dataset iterable object itself it is more common to first use the as_numpy_iterator() method if running TF in eager mode. In [60]: rows = 3 cols = 5 fig , ax = plt . subplots ( rows , cols , figsize = ( 10 , 6 )) for ax , ( img , label ) in zip ( ax . ravel (), ds_train . as_numpy_iterator ()): # break when no more axes left if ax is None : break ax . imshow ( img ) ax . set_title ( class_names [ label ]) ax . axis ( 'off' ) The above method works, but it would be a nuisance to have to always write out all that code just to inspect our datasets. Luckily, tfds has a much faster way to do this with the show_examples() method. In [61]: # train examples tfds . show_examples ( ds_train , ds_info , rows = rows , cols = cols ); In [62]: # test examples tfds . show_examples ( ds_test , ds_info , rows = rows , cols = cols ); 2022-03-11 12:21:54.906058: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead. Q: Do you notice anything strange about the test examples? 🤔 Take, Cardinality, & Batch Take We can use the take() method to return a subset of the original Dataset object of a desired cardinality . In [63]: # get a subset with cardinality 2 my_subset = ds_train . take ( 2 ) Cardinality It's true that we can check the length of a datset with len() In [64]: # one way to find a Dataset's length len ( my_subset ) Out[64]: 2 But this is inefficient for larger datasets. It is preferable to use the cardinality() method. This returns a Tensor. In [65]: # a better way cardinality = my_subset . cardinality () print ( f 'Cardinality Type: { type ( cardinality ) } ' ) Cardinality Type: It looks a bit strange when displayed. In [66]: # displaying an EagerTensor cardinality Out[66]: But it behaves just like an integer. In [67]: # no surprises here! ( cardinality + 2 ) == 4 Out[67]: And if you really want to, you can always convert it to a numpy.int64 object which prints nicely. In [68]: # convert to nump.int64 cardinality . numpy () Out[68]: 2 Batch To get the benefits of **stochastic gradient descent** (SGD) during training, we'd like to feed elements from the dataset into our model in batches. This is handled by the batch() method. Q: What are some benefits of SGD? 🤔 In [69]: BATCH_SIZE = 32 num_batch = ds_train . cardinality () / BATCH_SIZE print ( f 'Number of Potential Batches of size { BATCH_SIZE } :' , num_batch . numpy ()) num_batched_produced = ds_train . batch ( BATCH_SIZE ) . cardinality () print ( f 'Number of Batches of size { BATCH_SIZE } Produced:' , num_batched_produced . numpy ()) Number of Potential Batches of size 32: 32.09375 Number of Batches of size 32 Produced: 33 Q: Why don't these numbers match? What's going on? 🤔 The batched dataset is itself a Dataset , but now it's an iterable that produces batches. In a supervised situation like ours, each batch is a tuple. In [70]: # inspect first batch my_batch = ds_train . batch ( BATCH_SIZE ) . as_numpy_iterator () . next () print ( f 'Each batch is of type { type ( my_batch ) } with length { len ( my_batch ) } ' ) Each batch is of type with length 2 The 1st element of the tuple are all the images in the batch. In [71]: # batch images my_batch [ 0 ] . shape Out[71]: (32, 300, 300, 3) The 2nd element are all the labels in the batch. In [72]: # batch labels my_batch [ 1 ] . shape Out[72]: (32,) Note: The batch of images and labels above are both numpy arrays. This is because we used as_numpy_iterator() on our batched dataset. This is why we were able to use the shape attribute. here. We can also strengthen our intuition about the structure of the batched dataset by iterating over the batches and displaying the first image in each. In [73]: # display the first image in each batch fig , axs = plt . subplots ( 4 , 8 , figsize = ( 9 , 5 )) axs = axs . ravel () for i , ( img_batch , label_batch ) in enumerate ( ds_train . batch ( BATCH_SIZE , drop_remainder = True )): for ( img , label ) in zip ( img_batch , label_batch ): axs [ i ] . imshow ( img ) axs [ i ] . set_title ( f 'batch { i + 1 } ' ) axs [ i ] . axis ( 'off' ) break plt . tight_layout () Cache, Prefetch & Shuffle There are helpful methods we can use to optimize the training process. Most of these descriptions are adapted from the TensorFlow documentation. As always, the documentation is the best place to go if you'd like a deeper understanding. `Cache` caches a dataset, either in memory or on local storage. This will save some operations (like file opening and data reading) from being executed during each epoch. (perhaps not a good idea for enormous datasets) Prefetching overlaps the preprocessing and model execution of a training step. While the model is executing training steps, the input pipeline is reading the data for step s+1. Doing so reduces the step time to the maximum (as opposed to the sum) of the training and the time it takes to extract the data.\" `Shuffle` Randomly shuffles the elements of this dataset. Note: cache will produce exactly the same elements during each iteration through the dataset. If you wish to randomize the iteration order, make sure to call shuffle after calling cache. And of course we can chain all these commands together! In [74]: images , labels = ds_train . cache () \\ . shuffle ( buffer_size = ds_train . cardinality (), seed = SEED , reshuffle_each_iteration = True ) \\ . batch ( BATCH_SIZE ) . prefetch ( AUTOTUNE ) \\ . as_numpy_iterator () . next () # one batch # show first image in batch plt . imshow ( images [ 0 ]) plt . title ( class_names [ labels [ 0 ]]) plt . axis ( 'off' ); Preprocessing with Datasets Often, we'll want to preprocess our data in some way before feeding it into our model. We can use use the Dataset object's map method to perform arbitrary functions on the elements of the dataset. Because the result of the map operation is itself a dataset object, we can continue to chain these commands one after another. Here we are normalizing and resizing our images as part of the preprocessing stage using functions of our own design. In [75]: H = W = 200 def normalize_img ( img , label ): return tf . cast ( img , tf . float32 ) / 255.0 , label def resize_img ( img , label ): return tf . image . resize ( img , size = [ H , W ]), label ds_train = ds_train . map ( normalize_img , num_parallel_calls = AUTOTUNE ) . map ( resize_img , AUTOTUNE ) ds_test = ds_test . map ( normalize_img , num_parallel_calls = AUTOTUNE ) . map ( resize_img , AUTOTUNE ) Data Augmentation We almost always wish we had more data ! But it can be expensive and time consuming to gather and label new data. So why not simulate new data? We can accomplish this by creating variants of our original data. In the case of images this is very intuitive. Simply rotate your picture of a horse. It's still a horse, but the rotated image is likely different from anything in your data original. As long as the simulated data is not too different from the sort of example's we'd like to learn, this can help our model generalize better to previously unseen examples not in the original dataset. In [76]: import tensorflow_addons as tfa def random_rotate ( image , label ): \"\"\"Dataset pipe that rotates an image, helper function to augment below\"\"\" shape = image . shape deg = tf . random . uniform ([], - 10. , 10. ) image = tfa . image . rotate ( image , deg / 180. * np . pi , interpolation = \"BILINEAR\" ) image . set_shape (( shape )) label . set_shape (()) return image , label def random_zoom ( image , label ): \"\"\"Dataset pipe that zooms an image, helper function to augment below\"\"\" rand_float = tf . random . uniform ([], 10 , 20 ) rand_int = tf . cast ( rand_float , tf . int32 ) image = tf . image . resize_with_crop_or_pad ( image , H + H // rand_int , W + W // rand_int ) image = tf . image . random_crop ( image , size = [ H , W , 3 ]) return image , label def augment ( image , label ): \"\"\"Function that randomly alters an image with flipping, rotation, zoom, and contrast adjustment\"\"\" image = tf . image . random_flip_left_right ( image ) image , label = random_rotate ( image , label ) image , label = random_zoom ( image , label ) image = tf . image . random_contrast ( image , lower = .95 , upper = 1. ) return image , label Here are just a few examples created using the augmentation functions defined above! In [77]: # display a batch of altered images fig , axs = plt . subplots ( 4 , 4 , figsize = ( 6 , 6 )) aug_batch = ds_train . map ( augment , num_parallel_calls = AUTOTUNE ) . take ( 16 ) for ax , ( img , label ) in zip ( axs . ravel (), aug_batch ): ax . imshow ( img ) ax . axis ( 'off' ) Data augmentation is an important topic and it will be revisited several times throughout the course! Creating a Validation Set We can see from ds_info above that this dataset had predefined train and test sets. We loaded both. Some TF datasets also have a validation set. Others have only train. We'd like to use a validation set while training our model. And in situations where your data set only has a train set it will be important to know how to create new splits. Strangely, this seems to be one functionality that tfds does not provide by default. We can write the code ourselves but it is rather inscrutable. You can read more about this solution here . You won't be required to make splits from TF datasets in this section or in the HW. But you may wish to learn more about it on your own. In [78]: # inscrutable splitting code; not for the feint of heart! # 9 train samples for every validation sample train_per_val = 9 def mapping ( * ds ): return ds [ 0 ] if len ( ds ) == 1 else tf . data . Dataset . zip ( ds ) ds_val_example = ds_train . skip ( train_per_val ) . window ( 1 , train_per_val + 1 ) . flat_map ( mapping ) ds_train_example = ds_train . window ( train_per_val , train_per_val + 1 ) . flat_map ( mapping ) This method breaks cardinality! In [79]: # negative cardinality?! Preposterous! ds_val_example . cardinality () . numpy () Out[79]: -2 But slowly counting the elements in each split shows that the original dataset was indeed split correctly. In [80]: # correct sizes displayed by the slow method original_size = ds_train . cardinality () val_size = len ( list ( ds_val_example . as_numpy_iterator ())) train_size = len ( list ( ds_train_example . as_numpy_iterator ())) print ( f 'Original Size: { original_size . numpy () } ' ) print ( f 'Train Split Size: { train_size } ' ) print ( f 'Val Split Size: { val_size } ' ) print ( f 'Original Size = Train Split + Val Split: { ( original_size == val_size + train_size ) . numpy () } ' ) Original Size: 1027 Train Split Size: 925 Val Split Size: 102 Original Size = Train Split + Val Split: True Perhaps you can devise a better method! Keras &#94; From our work in the previous sections of this notebook I'm sure you can now appreciate that implementing a non-trivial CNN by hand would be a pain, and the looping code above was far from optimized. So we will be using Keras to quickly construct our neural networks. The Keras API that sits on top of Tensorflow. It allows user to work at a more intuitive level of abstraction where the basic objects are layers . Layers of a CNN in Keras The following is a list of layers commonly used when building CNNs with Keras. A link to the official documentation for each layer is also provided. Input tf.keras.Input ( shape=None, **kwargs ) The input is not a layer! As Pavlos said in lecture, you shouldn't think of the input to your network as a layer. Unfortunately, in Keras, most components of a network are referred to as 'layers'. Someone must have come to their senses because now Input can be found in the base tf.keras module. While it can still be imported from tf.keras.layers , we are civilized people and shall speak no more of that. The network will be expecting input of fixed shape which must be specified with the shape parameter. You should look at the data you are using to determine this shape. Adding an explicit Input object to your layer is not required as most layers have an input_shape that can be specified if they are the first layer in the network. 2D Convolutional Layers keras.layers.Conv2D (filters, kernel_size, strides=(1, 1), padding='valid', activation=None, use_bias=True, kernel_initializer='glorot_uniform', data_format='channels_last', bias_initializer='zeros') Some quick review if skipping to this section: A convolutional layer is composed of filters , which are composed of kernels which are themselves composed of weights . Each filter also has a bias term though it is often not depicted in diagrams (it is exluded in the one above for example). We learn the weights and biases from our data. Each conv layer also has an associated activation function such as ReLU or sigmoid. The number of filters and the height and width of the kernels of which they consist are set by the filters and kernel_size (a tuple) arguments respectively. The depth of the filters is fixed by the depth (i.e., 'channels' or 'filter maps') of the input to the conv layer. The output of the conv layer is is a 3D tensor which is a set of feature maps . Each feature map is itself the output of one of the layer's filters convolving on the input. The height and width of the feature map tensor is determined by the input size, kernel_size , padding , and stride . The depth of the output tensor (i.e, number of feature maps) is equal to the number of filters in the layer. Keras also has a 1D convolutional layers used for time series data and a 3D convolutional layers used for video. Pooling Layers keras.layers.MaxPool2D (pool_size=(2, 2), strides=None, padding='valid', data_format=None) Pooling layers are also comprised of filters and feature maps. Let's say the pooling layer has a 2x2 receptive field and a stride of 2. This stride results in feature maps that are one half the size of the input feature maps. We can use a max() operation for each receptive field. Dropout Layers tf.keras.layers.Dropout (rate, seed=None) Dropout consists in randomly setting a fraction of input units to 0 at each update during training time. In Keras this fraction is set by the rate parameter. At inference time, trained weights are multipled by $(1 - \\text{rate})$. Dropout often used to help prevent overfitting by limiting the complexity of our model. It can also prevent groups of neurons from 'conspiring' together to have a large affect on the out put, something traditional forms of weight regularization would not catch. Caution: Dropout's behavior is not the same if performed after a convolutional layer! See this post for more information . Q: Why might it make sense to think of dropout as a type of ensemble method? 🤔 References Dropout: A Simple Way to Prevent Neural Networks from Overfitting Flatten Layers keras.layers.Flatten () Like Input and Dropout , Flatten is not a layer in the traditional sense. It has no learned parameters and no parameters other than input_shape . Its only function is to flatten its multi-dimensional input into a flat vector. The flatten layer sits between our final 2D output (either from Conv2D or MaxPool2D) and our first fully connected, Dense layer. Fully Connected Layers. keras.layers.Dense (units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros') Most CNNs have of one or more dense layers at the end with the final layer referred to as the output layer . You'll need to specify the number of units in each layer (sometimes called 'neurons' or 'nodes') as well as the activation . Special care should be taken in deciding on the activation function for the output layer! The correct choice of activation in this final layer depends on the task we are training our model to perform. For example, a linear activation for regression, but a sigmoid for binary classification. Defining a Model Here our task is building a CNN to (surprise!) classify images as either horses are humans. 🤖-- 🐎 or 🧍? You be using the Keras sequential API to create your CNN model and define its architecture. There are two ways to do this. You can create a model object using the Sequential constructor while: passing it a list of parameterized layers passing no layers and then using the newly created model's add() method to attach layers to your model. We will begin with a very simply CNN. It should consist of the following layers: Input /w input shape derived below Conv2D /w 32 filters, 3x3 kernals, default padding & stride, and relu activation MaxPool2D of size 3x3 Conv2D /w 64 filters, 3x3 kernals, default padding & stride, and relu activation MaxPool2D of size 3x3 Conv2D /w 128 filters, 3x3 kernals, default padding & stride, and relu activation MaxPool2D of size 2x2 Flatten Dense /w 64 units and relu activation Dense /w 1 ? units and ? for an activation If you find these instructions a bit vague it is because you are being gently nudged to both look at all the helpful documentation linked above a think a bit about the task you are giving your network. 😉 Please continue on until you reach the end of exercise note. In [81]: from tensorflow import keras from tensorflow.keras.models import Sequential , Model from tensorflow.keras import Input from tensorflow.keras.layers import Conv2D , Dense , Dropout , Flatten , MaxPool2D We'll need to specify an input shape for our model. We can derive this by looking at one of the images in our dataset. In [82]: # Find input shape for element in ds_train . take ( 1 ) . as_numpy_iterator (): image_shape = element [ 0 ] . shape print ( f 'The input shape of each batch is: { BATCH_SIZE } ' ) print ( f 'The input shape of each image is: { image_shape } ' ) The input shape of each batch is: 32 The input shape of each image is: (200, 200, 3) In [83]: # Construct a CNN inputs = Input ( image_shape ) x = Conv2D ( 32 , 3 , padding = 'valid' , activation = \"relu\" )( inputs ) x = MaxPool2D (( 3 , 3 ))( x ) x = Conv2D ( 64 , 3 , padding = 'valid' , activation = \"relu\" )( x ) x = MaxPool2D (( 3 , 3 ))( x ) x = Conv2D ( 128 , 3 , padding = 'valid' , activation = \"relu\" )( x ) x = MaxPool2D (( 3 , 3 ))( x ) x = Flatten ()( x ) x = Dense ( 64 , activation = \"relu\" )( x ) outputs = Dense ( 1 , activation = \"sigmoid\" )( x ) model1 = Model ( inputs = inputs , outputs = outputs , name = \"Model1\" ) Now let's inspect the model with the summary() method. Notice how the dimensions change as the image passes through different stages of the network. Also take note of which layers have the most parameters. In [84]: model1 . summary () Model: \"Model1\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 200, 200, 3)] 0 conv2d (Conv2D) (None, 198, 198, 32) 896 max_pooling2d (MaxPooling2D (None, 66, 66, 32) 0 ) conv2d_1 (Conv2D) (None, 64, 64, 64) 18496 max_pooling2d_1 (MaxPooling (None, 21, 21, 64) 0 2D) conv2d_2 (Conv2D) (None, 19, 19, 128) 73856 max_pooling2d_2 (MaxPooling (None, 6, 6, 128) 0 2D) flatten (Flatten) (None, 4608) 0 dense (Dense) (None, 64) 294976 dense_1 (Dense) (None, 1) 65 ================================================================= Total params: 388,289 Trainable params: 388,289 Non-trainable params: 0 _________________________________________________________________ Compiling the Model Before we can train our model Keras requires use its compile() method to specify a few things: An optimizer which controls how weights are updated A loss function which your model is trying to minimize A list of metrics (optional) which are other functions that can be monitored during training The compile method accepts optimizers, losses, and elements of the metrics list as either as objects from their respective Keras modules (imported below) or their names as strings (with underscores replacing spaces). Compile your model with: optimizer='SGD' meterics = ['acc'] (e.g., accuracy) loss = ? (think about the task and look at the documentation) Note: Remember that metrics is a list. Hint: You can also use Jupyter's Tab aoutcomplete on the losses module to look at your options. In [85]: from tensorflow.keras import losses , metrics , optimizers In [86]: # compile your model # your code here model1 . compile ( optimizer = 'Adam' , loss = 'binary_crossentropy' , metrics = [ 'acc' ]) Train the Model Train your model by calling its fit method. We'll pass in our training data, validation data, and the number of epochs. Note the chaining of methods on the dataset objects! In [87]: history = model1 . fit ( ds_train . cache () \\ . shuffle ( buffer_size = ds_train . cardinality (), seed = SEED , reshuffle_each_iteration = True ) \\ . batch ( BATCH_SIZE ) . prefetch ( AUTOTUNE ), validation_data = ds_test . cache () \\ . shuffle ( buffer_size = ds_test . cardinality (), seed = SEED , reshuffle_each_iteration = True ) \\ . batch ( BATCH_SIZE ) . prefetch ( AUTOTUNE ), epochs = 30 ) Epoch 1/30 2022-03-11 12:22:12.625326: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100 2022-03-11 12:22:13.318032: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory 33/33 [==============================] - 7s 49ms/step - loss: 0.5458 - acc: 0.7342 - val_loss: 0.9787 - val_acc: 0.7891 Epoch 2/30 33/33 [==============================] - 1s 30ms/step - loss: 0.1525 - acc: 0.9445 - val_loss: 0.9291 - val_acc: 0.8477 Epoch 3/30 33/33 [==============================] - 1s 30ms/step - loss: 0.1015 - acc: 0.9542 - val_loss: 1.1325 - val_acc: 0.8203 Epoch 4/30 33/33 [==============================] - 1s 30ms/step - loss: 0.0450 - acc: 0.9834 - val_loss: 1.3926 - val_acc: 0.8438 Epoch 5/30 33/33 [==============================] - 1s 30ms/step - loss: 0.0242 - acc: 0.9932 - val_loss: 1.7921 - val_acc: 0.8320 Epoch 6/30 33/33 [==============================] - 1s 30ms/step - loss: 0.0128 - acc: 0.9942 - val_loss: 2.2007 - val_acc: 0.7969 Epoch 7/30 33/33 [==============================] - 1s 30ms/step - loss: 0.0711 - acc: 0.9776 - val_loss: 1.3172 - val_acc: 0.8398 Epoch 8/30 33/33 [==============================] - 1s 30ms/step - loss: 0.0070 - acc: 1.0000 - val_loss: 1.6625 - val_acc: 0.8320 Epoch 9/30 33/33 [==============================] - 1s 30ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 1.7524 - val_acc: 0.8359 Epoch 10/30 33/33 [==============================] - 1s 30ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 1.8518 - val_acc: 0.8477 Epoch 11/30 33/33 [==============================] - 1s 30ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 1.9251 - val_acc: 0.8477 Epoch 12/30 33/33 [==============================] - 1s 30ms/step - loss: 9.5161e-04 - acc: 1.0000 - val_loss: 2.0194 - val_acc: 0.8438 Epoch 13/30 33/33 [==============================] - 1s 29ms/step - loss: 6.5970e-04 - acc: 1.0000 - val_loss: 2.0287 - val_acc: 0.8438 Epoch 14/30 33/33 [==============================] - 1s 30ms/step - loss: 5.2964e-04 - acc: 1.0000 - val_loss: 2.0026 - val_acc: 0.8516 Epoch 15/30 33/33 [==============================] - 1s 29ms/step - loss: 5.5790e-04 - acc: 1.0000 - val_loss: 2.1803 - val_acc: 0.8359 Epoch 16/30 33/33 [==============================] - 1s 30ms/step - loss: 3.7807e-04 - acc: 1.0000 - val_loss: 2.1208 - val_acc: 0.8516 Epoch 17/30 33/33 [==============================] - 1s 30ms/step - loss: 2.6987e-04 - acc: 1.0000 - val_loss: 2.3153 - val_acc: 0.8359 Epoch 18/30 33/33 [==============================] - 1s 30ms/step - loss: 2.7432e-04 - acc: 1.0000 - val_loss: 2.2940 - val_acc: 0.8438 Epoch 19/30 33/33 [==============================] - 1s 29ms/step - loss: 2.0947e-04 - acc: 1.0000 - val_loss: 2.3206 - val_acc: 0.8438 Epoch 20/30 33/33 [==============================] - 1s 30ms/step - loss: 1.8929e-04 - acc: 1.0000 - val_loss: 2.3678 - val_acc: 0.8438 Epoch 21/30 33/33 [==============================] - 1s 29ms/step - loss: 1.8074e-04 - acc: 1.0000 - val_loss: 2.3236 - val_acc: 0.8516 Epoch 22/30 33/33 [==============================] - 1s 30ms/step - loss: 1.5403e-04 - acc: 1.0000 - val_loss: 2.4256 - val_acc: 0.8438 Epoch 23/30 33/33 [==============================] - 1s 29ms/step - loss: 1.4036e-04 - acc: 1.0000 - val_loss: 2.4043 - val_acc: 0.8438 Epoch 24/30 33/33 [==============================] - 1s 30ms/step - loss: 1.2904e-04 - acc: 1.0000 - val_loss: 2.4234 - val_acc: 0.8438 Epoch 25/30 33/33 [==============================] - 1s 29ms/step - loss: 1.1480e-04 - acc: 1.0000 - val_loss: 2.5149 - val_acc: 0.8438 Epoch 26/30 33/33 [==============================] - 1s 29ms/step - loss: 1.1362e-04 - acc: 1.0000 - val_loss: 2.4854 - val_acc: 0.8438 Epoch 27/30 33/33 [==============================] - 1s 30ms/step - loss: 1.1889e-04 - acc: 1.0000 - val_loss: 2.4818 - val_acc: 0.8516 Epoch 28/30 33/33 [==============================] - 1s 29ms/step - loss: 9.0511e-05 - acc: 1.0000 - val_loss: 2.5336 - val_acc: 0.8438 Epoch 29/30 33/33 [==============================] - 1s 29ms/step - loss: 8.8405e-05 - acc: 1.0000 - val_loss: 2.6447 - val_acc: 0.8359 Epoch 30/30 33/33 [==============================] - 1s 30ms/step - loss: 7.6366e-05 - acc: 1.0000 - val_loss: 2.5894 - val_acc: 0.8438 Plot the Training History In [88]: # helper function to avoid repeated code later def plot_loss ( model_history , out_file = None ): \"\"\" This helper function plots the NN model accuracy and loss. Arguments: model_history: the model history return from fit() out_file: the (optional) path to save the image file to. \"\"\" fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 4 )) history = model_history ax [ 0 ] . plot ( history . history [ 'acc' ]) ax [ 0 ] . plot ( history . history [ 'val_acc' ]) ax [ 0 ] . set_title ( 'model accuracy' ) ax [ 0 ] . set_ylabel ( 'accuracy' ) ax [ 0 ] . set_xlabel ( 'epoch' ) ax [ 0 ] . legend ([ 'train' , 'validation' ], loc = 'upper left' ) # summarize history for loss ax [ 1 ] . plot ( history . history [ 'loss' ]) ax [ 1 ] . plot ( history . history [ 'val_loss' ]) ax [ 1 ] . set_title ( 'model loss' ) ax [ 1 ] . set_ylabel ( 'loss' ) ax [ 1 ] . set_xlabel ( 'epoch' ) ax [ 1 ] . legend ([ 'train' , 'validation' ], loc = 'upper left' ) plt . show () if out_file : plt . savefig ( out_file ) plot_loss ( model_history = history ); Evaluate the Model Let's see how we did using the model's evaluate method. Note: Your model will complain if unless you passing it batches of the same size it saw during training. In [90]: model1 . evaluate ( ds_test . batch ( BATCH_SIZE ) . prefetch ( AUTOTUNE )) 8/8 [==============================] - 0s 14ms/step - loss: 2.5894 - acc: 0.8438 Out[90]: [2.589402437210083, 0.84375] In [91]: model1 . evaluate ( ds_train . batch ( BATCH_SIZE ) . prefetch ( AUTOTUNE )) 33/33 [==============================] - 3s 75ms/step - loss: 7.0608e-05 - acc: 1.0000 Out[91]: [7.060848292894661e-05, 1.0] Saving Your Model It would be nice if we could save and restore our models between sessions. Here's a few methods! First, we can save just the learned weights. Careful though! You will have to first reconstruct the same architecture to load these weights into. This also does not save the optimizer state. In [141]: model1 . save_weights ( \"data/models/model1_weights.ckpt\" ) In [142]: model1 . load_weights ( \"data/models/model1_weights.ckpt\" ); Alternatively, we can save the entire model, including the architecture itself. In [143]: model1 = model1 . save ( 'data/models/model1' ) 2022-03-11 13:29:16.801220: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them. INFO:tensorflow:Assets written to: data/models/model1/assets INFO:tensorflow:Assets written to: data/models/model1/assets In [144]: model1 = keras . models . load_model ( 'data/models/model1' ) In [145]: # confirm reloading worked model1 . evaluate ( ds_train . batch ( BATCH_SIZE ) . prefetch ( AUTOTUNE )) 33/33 [==============================] - 3s 75ms/step - loss: 7.0608e-05 - acc: 1.0000 Out[145]: [7.060849748086184e-05, 1.0] Keras Callbacks Training CNNs can take a long time. We should checkpoint our model so we don't lose progress and stop early if we don't see improvement to help save on training time. Checkpointing also allows us to recover our best performing version of te model from any point during the training process as later version have a tendency to be overfit. Keras Callbacks Documentation Unfortunately, subroutines of many callbacks currently result in some ugly (but harmless) warnings . Until the elves at the Tensorflow workshop put things right we can simply silence them. In [92]: import tensorflow.python.util.deprecation as deprecation deprecation . _PRINT_DEPRECATION_WARNINGS = False In [128]: from tensorflow.keras.callbacks import EarlyStopping , ModelCheckpoint es = EarlyStopping ( monitor = 'val_loss' , patience = 15 , restore_best_weights = True ) # Point the keras.models.load() method at the path given below to reload the checkpointed model mc = ModelCheckpoint ( 'data/models/checkpoints' , monitor = 'val_loss' , save_best_only = True , save_weights_only = False ) Callbacks are used by passing them as a list to a model's fit() method. Example: model.fit(...., callbacks=[es, mc]) Note : If your model trains very quickly then checkpointing may not be worth it. The time it takes to write to disk after each epoch can sometimes take longer than the training itself! Exercise: Improving on Baseline Model Now see if you can improve on the performance of you previous model. You can try: adjusting the architecture more/fewer layers/filters/units adding a dropout(s) layer adding data augmentation tweaking the various transformations included using callbacks like checkpointing or early stopping adjust the number of epochs adjust the batch size Note: What follows is not the 'solution,' but merely one way you could reparameterize your model in an attempt to improve the evaluation on the validation set. Can you get below a validation loss of ~3.4? In [94]: from tensorflow.keras.layers import GaussianNoise , BatchNormalization , Activation In [129]: # try and improve on your first CNN # your code here # end your code here model2 = Model ( inputs = inputs , outputs = outputs , name = \"Model2\" ) In [ ]: model2 . summary () In [131]: model2 . compile ( optimizer = optimizers . Adam (), loss = 'binary_crossentropy' , metrics = [ 'acc' ]) In [ ]: %%time # CUSTOMIZE THESE PARAMETERS BATCH_SIZE2 = 32 history2 = model2 . fit ( ds_train . cache () \\ . shuffle ( buffer_size = ds_train . cardinality (), seed = SEED , reshuffle_each_iteration = True ) \\ . map ( augment , num_parallel_calls = AUTOTUNE ) \\ . batch ( BATCH_SIZE2 ) . prefetch ( AUTOTUNE ), validation_data = ds_test . cache () \\ . shuffle ( buffer_size = ds_test . cardinality (), seed = SEED , reshuffle_each_iteration = True ) \\ . batch ( BATCH_SIZE2 ) . prefetch ( AUTOTUNE ), epochs = 50 , callbacks = [ es ], verbose = 0 ) In [ ]: plot_loss ( history2 ) In [ ]: model2 . evaluate ( ds_test . batch ( BATCH_SIZE ) . prefetch ( AUTOTUNE )) In [ ]: model2 . evaluate ( ds_train . batch ( BATCH_SIZE ) . prefetch ( AUTOTUNE )) In [ ]: In [ ]: In [ ]:","tags":"labs","url":"labs/lab07/notebook/"},{"title":"Lab 3: Hierarchical Bayesian Models","text":"Notebooks Hierarchical Models","tags":"labs","url":"labs/lab03/"},{"title":"Lab 03:","text":"CS109B Data Science 2: Advanced Topics in Data Science Lab 03 - Hierarchical Models in PyMC3 Harvard University Spring 2022 Instructors: Mark Glickman and Pavlos Protopapas Lab instructor and content: Eleni Angelaki Kaxiras In [1]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: In [2]: import warnings import arviz as az import matplotlib.pyplot as plt import numpy as np import pymc3 as pm import theano.tensor as tt from pymc3 import summary from pymc3 import Model , Normal , HalfNormal , model_to_graphviz , HalfCauchy from pymc3 import NUTS , sample , find_MAP from scipy import optimize warnings . simplefilter ( action = \"ignore\" , category = FutureWarning ) warnings . filterwarnings ( 'ignore' ) import numpy as np import matplotlib.pyplot as plt from matplotlib import gridspec import scipy.stats as stats import pandas as pd import seaborn as sns % matplotlib inline import warnings print ( 'Running on PyMC3 v {} ' . format ( pm . __version__ )) Running on PyMC3 v3.8 In [3]: %% javascript IPython . OutputArea . auto_scroll_threshold = 20000 ; Learning Objectives By the end of this lab, you should know how to create Bayesian hierarchical models in PyMC3 This lab maps to Homework 2. Hierarchical Models Gelman et al. famous radon dataset is a classic for hierarchical modeling. In this dataset the amount of the radioactive gas radon has been measured among different households in all county's of several states. Radon gas is known to be the highest cause of lung cancer in non-smokers. It is believed to be more strongly present in households containing a basement and to differ in amount present among types of soil. Here we'll investigate this differences and try to make predictions of radonlevels in different county's based on the county itself and the presence of a basement. In [4]: df = pd . read_csv ( '../data/radon.csv' , index_col = [ 0 ]) df [ 'log_radon' ] = df [ 'log_radon' ] . astype ( 'float' ) county_names = df . county . unique () county_idx = df . county_code . values n_counties = len ( df . county . unique ()) df . head () Out[4]: idnum state state2 stfips zip region typebldg floor room basement ... pcterr adjwt dupflag zipflag cntyfips county fips Uppm county_code log_radon 0 5081.0 MN MN 27.0 55735 5.0 1.0 1.0 3.0 N ... 9.7 1146.499190 1.0 0.0 1.0 AITKIN 27001.0 0.502054 0 0.832909 1 5082.0 MN MN 27.0 55748 5.0 1.0 0.0 4.0 Y ... 14.5 471.366223 0.0 0.0 1.0 AITKIN 27001.0 0.502054 0 0.832909 2 5083.0 MN MN 27.0 55748 5.0 1.0 0.0 4.0 Y ... 9.6 433.316718 0.0 0.0 1.0 AITKIN 27001.0 0.502054 0 1.098612 3 5084.0 MN MN 27.0 56469 5.0 1.0 0.0 4.0 Y ... 24.3 461.623670 0.0 0.0 1.0 AITKIN 27001.0 0.502054 0 0.095310 4 5085.0 MN MN 27.0 55011 3.0 1.0 0.0 4.0 Y ... 13.8 433.316718 0.0 0.0 3.0 ANOKA 27003.0 0.428565 1 1.163151 5 rows × 29 columns Each row in the dataframe represents the radon measurements for one house in a specific county including whether the house has a basement (floor = 0) or not (floor = 1). We are interested in whether having a basement increases the radon measured in the house. To keep things simple let's keep only the following three variables: county , log_radon , and floor In [5]: # keep only these variables data = df [[ 'county' , 'log_radon' , 'floor' ]] data . head () Out[5]: county log_radon floor 0 AITKIN 0.832909 1.0 1 AITKIN 0.832909 0.0 2 AITKIN 1.098612 0.0 3 AITKIN 0.095310 0.0 4 ANOKA 1.163151 0.0 Let's check how many different counties we have. We also notice that they have a different number of houses. Some have a large number of houses measured, some only 1. In [6]: data [ 'county' ] . value_counts () . head ( 5 ) Out[6]: ST LOUIS 116 HENNEPIN 105 DAKOTA 63 ANOKA 52 WASHINGTON 46 Name: county, dtype: int64 In [7]: data [ 'county' ] . value_counts ()[ - 5 :] Out[7]: STEVENS 2 MILLE LACS 2 MAHNOMEN 1 MURRAY 1 WILKIN 1 Name: county, dtype: int64 In [8]: # let's add a column that numbers the counties from 0 to n # raw_ids = np.unique(data['county']) # raw2newid = {x:np.where(raw_ids == x)[0][0] for x in raw_ids} # data['county_id'] = data['county'].map(raw2newid) # data 1 - Pooling: Same Linear Regression for all We can just pool all the data and estimate one big regression to asses the influence of having a basement on radon levels across all counties. Our model would be: \\begin{equation} y_{i} = \\alpha + \\beta*floor_{i} \\end{equation} Where $i$ represents the measurement (house), and floor contains a 0 or 1 if the house has a basement or not. By ignoring the county feature, we do not differenciate on counties. In [9]: with pm . Model () as pooled_model : # common priors for all a = pm . Normal ( 'a' , mu = 0 , sigma = 100 ) b = pm . Normal ( 'b' , mu = 0 , sigma = 100 ) # radon estimate radon_est = a + b * data [ 'floor' ] . values # likelihood after radon observations radon_obs = pm . Normal ( 'radon_obs' , mu = radon_est , observed = data [ 'log_radon' ]) # note here we enter the whole dataset In [10]: from pymc3 import model_to_graphviz model_to_graphviz ( pooled_model ) Out[10]: %3 cluster919 919 b b ~ Normal radon_obs radon_obs ~ Normal b->radon_obs a a ~ Normal a->radon_obs In [11]: with pooled_model : pooled_trace = sample ( 2000 , tune = 1000 , target_accept = 0.9 ) print ( f 'DONE' ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [b, a] Sampling 4 chains, 0 divergences: 100%|██████████| 12000/12000 [00:03<00:00, 3781.83draws/s] DONE In [12]: pm . traceplot ( pooled_trace ); Remember, with the pooled model we have only one intercept, $\\alpha$, and only one slope, $\\beta$ for all the counties. Let's plot the regression lines. In [13]: # plot just a subset of the countries counties = [ 'HENNEPIN' , 'AITKIN' , 'WASHINGTON' , 'MURRAY' , 'YELLOW MEDICINE' , 'MAHNOMEN' ] plt . figure ( figsize = ( 10 , 5 )) rows = 2 gs = gridspec . GridSpec ( rows , len ( counties ) // rows ) for i , county in enumerate ( counties ): county_data = data . loc [ data [ 'county' ] == county ] x = np . linspace ( - 0.2 , 1.2 ) radon_est = pooled_trace [ 'a' ] . mean () + pooled_trace [ 'b' ] . mean () * x subplt = plt . subplot ( gs [ i ]) subplt . set_ylim ( 0. , 4. ) subplt . scatter ( county_data [ 'floor' ], county_data [ 'log_radon' ]) subplt . plot ( x , radon_est , c = 'r' , label = 'pooled line' ); subplt . set_xlabel ( 'floor' , fontsize = 10 ) subplt . set_ylabel ( 'radon level' , fontsize = 10 ) subplt . set_title ( str ( county ) + ' County' ) subplt . legend () plt . tight_layout () 2 - Unpooling: Separate Linear Regression for each county We believe that different counties have different relationships of radon and basements. Our model would be: \\begin{equation} radon_{i,c} = \\alpha_c + \\beta_c*floor_{i,c} \\end{equation} Where $i$ represents the measurement, $c$ the county, and floor contains a 0 or 1 if the house has a basement or not. Notice we have separate coefficients for each county in $a_c$ and $b_c$. They are totally different, they do not even come from the same distribution. We will do this for only one county, as an example. We pick HENNEPIN county. In [14]: # chose a county county = 'MEEKER' county_data = data . loc [ data [ 'county' ] == county ] county_data . head () Out[14]: county log_radon floor 479 MEEKER 0.875469 0.0 480 MEEKER 1.386294 0.0 481 MEEKER 1.987874 0.0 482 MEEKER 0.788457 0.0 483 MEEKER 1.193922 0.0 In [15]: #help(pm.Normal) In [16]: with pm . Model () as unpooled_model : mu_a = pm . Normal ( 'mu_a' , mu = 0. , sigma = 100 ) sigma_a = pm . HalfNormal ( 'sigma_a' , 5. ) mu_b = pm . Normal ( 'mu_b' , mu = 0. , sigma = 100 ) sigma_b = pm . HalfNormal ( 'sigma_b' , 5. ) a = pm . Normal ( 'a' , mu = mu_a , sigma = sigma_a ) b = pm . Normal ( 'b' , mu = mu_b , sigma = sigma_b ) radon_est = a + b * county_data [ 'floor' ] . values radon_obs = pm . Normal ( 'radon_like' , mu = radon_est , observed = county_data [ 'log_radon' ]) In [17]: model_to_graphviz ( unpooled_model ) Out[17]: %3 cluster5 5 a a ~ Normal radon_like radon_like ~ Normal a->radon_like sigma_b sigma_b ~ HalfNormal b b ~ Normal sigma_b->b sigma_a sigma_a ~ HalfNormal sigma_a->a b->radon_like mu_b mu_b ~ Normal mu_b->b mu_a mu_a ~ Normal mu_a->a In [18]: with unpooled_model : unpooled_trace = sample ( 2000 , tune = 1000 , target_accept = 0.9 ) print ( f 'DONE' ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [b, a, sigma_b, mu_b, sigma_a, mu_a] Sampling 4 chains, 465 divergences: 100%|██████████| 12000/12000 [00:58<00:00, 205.25draws/s] There were 94 divergences after tuning. Increase `target_accept` or reparameterize. There were 184 divergences after tuning. Increase `target_accept` or reparameterize. The acceptance probability does not match the target. It is 0.7384761840644513, but should be close to 0.9. Try to increase the number of tuning steps. There were 88 divergences after tuning. Increase `target_accept` or reparameterize. There were 99 divergences after tuning. Increase `target_accept` or reparameterize. The acceptance probability does not match the target. It is 0.8329297259480425, but should be close to 0.9. Try to increase the number of tuning steps. The estimated number of effective samples is smaller than 200 for some parameters. DONE In [19]: pm . traceplot ( unpooled_trace ); Print the regression line for our chosen county alone. In [20]: county = 'MEEKER' county_data = data . loc [ data [ 'county' ] == county ] x = np . arange ( len ( county_data [ 'floor' ] . values )) radon_est_unpooled = unpooled_trace [ 'a' ] . mean () + unpooled_trace [ 'b' ] . mean () * county_data [ 'floor' ] . values xx = np . linspace ( - 0.2 , 1.2 ) radon_est_pooled = pooled_trace [ 'a' ] . mean () + pooled_trace [ 'b' ] . mean () * xx plt . scatter ( county_data [ 'floor' ], county_data [ 'log_radon' ]) plt . xlim ( - 0.1 , 1.1 ) plt . xlabel ( 'floor' , fontsize = 10 ) plt . ylabel ( 'radon level' , fontsize = 10 ) plt . title ( f ' { str ( county ) } county Radon levels' ) plt . plot ( x , radon_est_unpooled , c = 'g' , label = 'unpooled line' ); plt . plot ( xx , radon_est_pooled , c = 'r' , label = 'pooled line' ); plt . legend (); 3 - Partial pooling: Hierarchical Regression (Varying-Coefficients Model) Merely by the fact that all counties are counties, they share similarities, so there is a middle ground to both of these extremes. Specifically, we may assume that while $\\alpha_c$ and $\\beta_c$are different for each county as in the unpooled case, the coefficients are all drawn from the same distribution: \\begin{equation} radon_{i,c} = \\alpha_c + \\beta_c*floor_{i,c} \\end{equation} \\begin{equation} a_c \\sim \\mathcal{N}(\\mu_a,\\,\\sigma_a&#94;{2}) \\end{equation} \\begin{equation} b_c \\sim \\mathcal{N}(\\mu_b,\\,\\sigma_b&#94;{2}) \\end{equation} where the common parameters are: \\begin{eqnarray} \\mu_a \\sim \\mathcal{N}(0,\\,10) \\\\ \\sigma_a&#94;2 \\sim |\\mathcal{N}(0,\\,10)| \\\\ \\mu_b \\sim \\mathcal{N}(0,\\,10) \\\\ \\sigma_b&#94;2 \\sim |\\mathcal{N}(0,\\,10)| \\end{eqnarray} Add math for mu and sigma The different counties are effectively sharing information through the commin priors. We are thus observing what is known as shrinkage; modeling the groups not as independent from each other, neither as a single group but rather as related. NOTES We saw that some counties had only one sample, so if that house is a really old with old lead pipes, our prediction will be that all houses in this county have radon. On the other extreme, if we have a newer house with no radon then again we will have missleading results. In one case, you will overestimate the bad quality and in the other underestimate it. Under a hierarchical model, the miss-estimation of one group will be offset by the information provided by the other groups. As always gathering more data helps if this is an option. Defining the Model for the Hierarchical Model In [21]: with pm . Model () as hierarchical_model : # Hyperpriors for group nodes mu_a = pm . Normal ( 'mu_a' , mu = 0. , sigma = 100 ) sigma_a = pm . HalfNormal ( 'sigma_a' , 5. ) mu_b = pm . Normal ( 'mu_b' , mu = 0. , sigma = 100 ) sigma_b = pm . HalfNormal ( 'sigma_b' , 5. ) # Above we just set mu and sd to a fixed value while here we # plug in a common group distribution for all a and b (which are # vectors of length n_counties). # Intercept for each county, distributed around group mean mu_a a = pm . Normal ( 'a' , mu = mu_a , sigma = sigma_a , shape = n_counties ) # beta for each county, distributed around group mean mu_b b = pm . Normal ( 'b' , mu = mu_b , sigma = sigma_b , shape = n_counties ) # Model error #eps = pm.HalfCauchy('eps', 5.) radon_est = a [ county_idx ] + b [ county_idx ] * data [ 'floor' ] . values # Data likelihood with sigma for random error # radon_like = pm.Normal('radon_like', mu=radon_est, # sigma=eps, observed=data['log_radon']) # Data likelihood with sigma without random error radon_like = pm . Normal ( 'radon_like' , mu = radon_est , #sigma=eps, observed = data [ 'log_radon' ]) In [22]: model_to_graphviz ( hierarchical_model ) Out[22]: %3 cluster85 85 cluster919 919 sigma_b sigma_b ~ HalfNormal b b ~ Normal sigma_b->b mu_b mu_b ~ Normal mu_b->b sigma_a sigma_a ~ HalfNormal a a ~ Normal sigma_a->a mu_a mu_a ~ Normal mu_a->a radon_like radon_like ~ Normal b->radon_like a->radon_like Inference In [23]: with hierarchical_model : hierarchical_trace = pm . sample ( 2000 , tune = 2000 , target_accept = .9 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [b, a, sigma_b, mu_b, sigma_a, mu_a] Sampling 4 chains, 1,822 divergences: 100%|██████████| 16000/16000 [01:02<00:00, 257.47draws/s] There were 8 divergences after tuning. Increase `target_accept` or reparameterize. There were 49 divergences after tuning. Increase `target_accept` or reparameterize. There were 1754 divergences after tuning. Increase `target_accept` or reparameterize. The acceptance probability does not match the target. It is 0.1690165871960057, but should be close to 0.9. Try to increase the number of tuning steps. There were 11 divergences after tuning. Increase `target_accept` or reparameterize. The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge. The estimated number of effective samples is smaller than 200 for some parameters. In [24]: pm . traceplot ( hierarchical_trace , var_names = [ 'mu_a' , 'mu_b' , 'sigma_a' , 'sigma_b' ]); In [25]: results = pm . summary ( hierarchical_trace ) results [: 10 ] Out[25]: mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat mu_a 1.462 0.048 1.374 1.560 0.002 0.001 912.0 912.0 681.0 1786.0 1.06 mu_b -0.624 0.089 -0.824 -0.476 0.006 0.005 215.0 193.0 224.0 379.0 1.09 a[0] 1.334 0.210 0.917 1.758 0.003 0.003 4753.0 2289.0 3168.0 2972.0 1.20 a[1] 1.043 0.119 0.830 1.276 0.017 0.013 47.0 44.0 55.0 2754.0 1.07 a[2] 1.493 0.220 1.027 1.877 0.027 0.019 68.0 68.0 61.0 2436.0 1.09 a[3] 1.504 0.190 1.131 1.893 0.004 0.003 2658.0 2658.0 1129.0 2511.0 1.17 a[4] 1.382 0.251 1.043 1.853 0.073 0.053 12.0 12.0 12.0 378.0 1.22 a[5] 1.545 0.249 1.050 1.909 0.057 0.042 19.0 18.0 21.0 2916.0 1.12 a[6] 1.746 0.179 1.369 2.046 0.021 0.015 72.0 72.0 76.0 3192.0 1.08 a[7] 1.634 0.252 1.126 1.980 0.065 0.048 15.0 14.0 17.0 1853.0 1.16 In [26]: # plot just a subset of the countries counties = [ 'HENNEPIN' , 'AITKIN' , 'WASHINGTON' , 'LAKE OF THE WOODS' , 'YELLOW MEDICINE' , 'ANOKA' ] plt . figure ( figsize = ( 10 , 5 )) rows = 2 gs = gridspec . GridSpec ( rows , len ( counties ) // rows ) for i , county in enumerate ( counties ): county_data = data . loc [ data [ 'county' ] == county ] subplt = plt . subplot ( gs [ i ]) # pooled line (single values coeff for all) xx = np . linspace ( - 0.2 , 1.2 ) radon_est = pooled_trace [ 'a' ] . mean () + pooled_trace [ 'b' ] . mean () * xx radon_est_hier = np . mean ( hierarchical_trace [ 'a' ][ i ]) + \\ np . mean ( hierarchical_trace [ 'b' ][ i ]) * xx # un-pooled (single subject) sns . regplot ( x = 'floor' , y = 'log_radon' , ci = None , label = 'unpooled' , data = county_data ) . set_title ( 'County ' + str ( county )) # hierarchical line subplt . set_ylim ( 0. , 4. ) subplt . scatter ( county_data [ 'floor' ], county_data [ 'log_radon' ]) subplt . plot ( xx , radon_est , c = 'r' , label = 'pooled' ); # plot the hierarchical, varying coefficient model subplt . plot ( xx , radon_est_hier , c = 'g' , label = 'hierarchical' ); subplt . set_xlabel ( 'floor' , fontsize = 10 ) subplt . set_ylabel ( 'radon level' , fontsize = 10 ) subplt . set_title ( str ( county ) + ' County' ) subplt . legend () plt . tight_layout () This tutorial is modified from PyMC3 docs: https://docs.pymc.io/notebooks/GLM-hierarchical.html","tags":"labs","url":"labs/lab03/notebook/"},{"title":"Lab 2: Bayesian Models","text":"Notebook Lab 2: PDFs Lab 2: Bayes","tags":"labs","url":"labs/lab02/"},{"title":"Lab 02:","text":"CS109B Data Science 2: Advanced Topics in Data Science Lab 02 - Bayesian Analysis Harvard University Spring 2022 Instructors: Mark Glickman and Pavlos Protopapas Lab instructor and content: Eleni Angelaki Kaxiras In [1]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: In [2]: import warnings import arviz as az import matplotlib.pyplot as plt import numpy as np import pymc3 as pm import theano.tensor as tt from pymc3 import summary warnings . simplefilter ( action = \"ignore\" , category = FutureWarning ) warnings . filterwarnings ( 'ignore' ) import numpy as np import matplotlib.pyplot as plt from matplotlib import gridspec import scipy.stats as stats import pandas as pd import seaborn as sns % matplotlib inline import warnings print ( 'Running on PyMC3 v {} ' . format ( pm . __version__ )) Running on PyMC3 v3.8 In [3]: %% javascript IPython . OutputArea . auto_scroll_threshold = 20000 ; In [4]: #pandas trick pd . options . display . max_columns = 50 # None -> No Restrictions pd . options . display . max_rows = 200 # None -> Be careful with this pd . options . display . max_colwidth = 100 pd . options . display . precision = 3 Learning Objectives By the end of this lab, you should be able to: Define a probabilistic model in the PyMC3 framework Run a sampling algorithm This lab corresponds to Lectures 3,4, and maps to Homework 2. 1. Bayes Rule We have data that we believe come from an underlying distribution of unknown parameters. If we find those parameters, we know everything about the process that generated this data and we can make inferences (create new data). \\begin{equation} \\label{eq:bayes} P(\\theta|\\textbf{D}) = \\frac{P(\\textbf{D} |\\theta) P(\\theta) }{P(\\textbf{D})} \\end{equation} $P(\\theta|\\textbf{D})$ is the posterior distribution, prob(hypothesis | data) $P(\\textbf{D} |\\theta)$ is the likelihood function, how probable is my data B for different values of the parameters $P(\\theta)$ is the marginal probability to observe the data, called the prior , this captures our belief about the data before observing it. $P(\\textbf{D})$ is the marginal distribution (sometimes called marginal likelihood) But what is $\\theta \\;$? $\\theta$ is an unknown yet fixed set of parameters. In Bayesian inference we express our belief about what $\\theta$ might be and instead of trying to guess $\\theta$ exactly, we look for its probability distribution . What that means is that we are looking for the parameters of that distribution. For example, for a Poisson distribution our $\\theta$ is only $\\lambda$. In a normal distribution, our $\\theta$ is often just $\\mu$ and $\\sigma$. Top 2. Introduction to pyMC3 PyMC3 is a Python library for programming Bayesian analysis, and more specifically, data creation, model definition, model fitting, and posterior analysis. It uses the concept of a model which contains assigned parametric statistical distributions to unknown quantities in the model. Within models we define random variables and their distributions. A distribution requires at least a name argument, and other parameters that define it. You may also use the logp() method in the model to build the model log-likelihood function. We define and fit the model. PyMC3 includes a comprehensive set of pre-defined statistical distributions that can be used as model building blocks. Although they are not meant to be used outside of a model , you can invoke them by using the prefix pm , as in pm.Normal . To use them outside of a model one needs to invoke them as pm.Normal.dist() . 3. Probability distributions in scipy and PyMC3 We can invoke probability distributions from scipy or directly from PyMC3 . Distributions in PyMC3 live within the context of models, although the framework provides a way to use the distributions outside of models. For a review of most common discete and continuous distributions see separate notebook. scipy Normal (a.k.a. Gaussian) \\begin{equation} X \\sim \\mathcal{N}(\\mu,\\,\\sigma&#94;{2}) \\end{equation} A Normal distribution can be parameterized either in terms of precision $\\tau$ or variance $\\sigma&#94;{2}$. The link between the two is given by \\begin{equation} \\tau = \\frac{1}{\\sigma&#94;{2}} \\end{equation} Expected value (mean) $\\mu$ Variance $\\frac{1}{\\tau}$ or $\\sigma&#94;{2}$ Parameters: mu: float , sigma: float or tau: float Range of values (-$\\infty$, $\\infty$) In [5]: plt . style . use ( 'seaborn-darkgrid' ) x = np . linspace ( - 5 , 5 , 1000 ) mus = [ 0. , 0. , 0. , - 2. ] sigmas = [ 0.4 , 1. , 2. , 0.4 ] for mu , sigma in zip ( mus , sigmas ): pdf = stats . norm . pdf ( x , mu , sigma ) plt . plot ( x , pdf , label = r '$\\mu$ = ' + f ' { mu } ,' + r '$\\sigma$ = ' + f ' { sigma } ' ) plt . xlabel ( 'random variable' , fontsize = 12 ) plt . ylabel ( 'probability density' , fontsize = 12 ) plt . legend ( loc = 1 ) plt . show () PyMC3 In [6]: az . style . use ( \"arviz-darkgrid\" ) a = pm . Poisson . dist ( mu = 4 ) b = pm . Normal . dist ( mu = 0 , sigma = 10 ) _ , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 4 )) az . plot_dist ( a . random ( size = 1000 ), color = \"C1\" , label = \"Poisson\" , ax = ax [ 0 ]) az . plot_dist ( b . random ( size = 1000 ), color = \"C2\" , label = \"Gaussian\" , ax = ax [ 1 ]) plt . show () Distributions in PyMC3 Information about PyMC3 functions including descriptions of distributions, sampling methods, and other functions, is available via the help command. In [7]: ## uncomment to look at the documentation #help(pm.Poisson) Top 3. Bayesian Linear Regression Define the Problem Our problem is the following: we want to perform multiple linear regression to predict an outcome variable $Y$ which depends on variables $\\bf{x}_1$ and $\\bf{x}_2$. We will model $Y$ as normally distributed observations with an expected value $mu$ that is a linear function of the two predictor variables, $\\bf{x}_1$ and $\\bf{x}_2$. \\begin{equation} Y \\sim \\mathcal{N}(\\mu,\\,\\sigma) \\end{equation} \\begin{equation} \\mu = \\beta_0 + \\beta_1 \\bf{x}_1 + \\beta_2 x_2 \\end{equation} where $\\sigma$ represents the measurement error (in this example, we will use $\\sigma = 10$). Note: In the code we give the value for the standard deviation $\\sigma$. We also choose the parameters to have normal distributions with those parameters set by us. \\begin{eqnarray} \\beta_i \\sim \\mathcal{N}(0,\\,10) \\\\ \\sigma \\sim |\\mathcal{N}(0,\\,10)| \\end{eqnarray} We will artificially create the data to predict on. We will then see if our model predicts them correctly. Artificially create some data to test our model. In [8]: np . random . seed ( 123 ) # True parameter values < --- our model does not see these sigma = 1 beta0 = 1 beta = [ 1 , 2.5 ] # Size of dataset size = 1000 # Predictor variable x1 = np . linspace ( 0 , 1. , size ) x2 = np . linspace ( 0 , 2. , size ) # Simulate outcome variable Y = beta0 + beta [ 0 ] * x1 + beta [ 1 ] * x2 + np . random . randn ( size ) * sigma In [9]: from mpl_toolkits.mplot3d import Axes3D fig = plt . figure ( figsize = ( 5 , 4 )) fontsize = 14 labelsize = 8 title = 'Observed Data ' + r '$Y(x_1,x_2)$' + ' (created artificially)' ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( x1 , x2 , Y ) ax . set_xlabel ( r '$x_1$' , fontsize = fontsize ) ax . set_ylabel ( r '$x_2$' , fontsize = fontsize ) ax . set_zlabel ( r '$Y$' , fontsize = fontsize ) ax . tick_params ( labelsize = labelsize ) fig . suptitle ( title , fontsize = fontsize ) fig . tight_layout ( pad = .1 , w_pad = 10.1 , h_pad = 2. ) #fig.subplots_adjust(); #top=0.5 plt . tight_layout plt . show () Now let's see if our model will correctly predict the values for our unknown parameters, namely $b_0$, $b_1$, $b_2$ and $\\sigma$. Define the Model in PyMC3 In [10]: from pymc3 import Model , Normal , HalfNormal , model_to_graphviz , HalfCauchy from pymc3 import NUTS , sample , find_MAP from scipy import optimize Step1: Formulate the probability model for our data: $Y \\sim \\mathcal{N}(\\mu,\\,\\sigma&#94;{2})$. This the likelihood function and it's defined the same as the other distributions except that there is an observed argument which means its values are determined by the data. Y_obs = pm.Normal('Y_obs', mu=mu, sigma=sigma, observed=Y) Step2: Choose a prior distribution for our unknown parameters. beta0 = Normal('beta0', mu=0, sigma=10) # Note: betas is a vector of two variables, b1 and b2, (denoted by shape=2) # so, in array notation, our beta1 = betas[0], and beta2=betas[1] betas = Normal('betas', mu=0, sigma=10, shape=2) sigma = HalfNormal('sigma', sigma=1) Step3: Determine the posterior distribution, this is our main goal . Step4: Summarize important features of the posterior and/or plot the parameters. In [11]: with Model () as my_linear_model : # Priors for unknown model parameters, specifically created stochastic random variables # with Normal prior distributions for the regression coefficients, # and a half-normal distribution for the standard deviation of the observations. # These are our parameters. beta0 = Normal ( 'beta0' , mu = 0 , sd = 10 ) # Note: betas is a vector of two variables, b1 and b2, (denoted by shape=2) # so, in array notation, our beta1 = betas[0], and beta2=betas[1] betas = Normal ( 'betas' , mu = 0 , sd = 10 , shape = 2 ) sigma = HalfNormal ( 'sigma' , sd = 1 ) # mu is what is called a deterministic random variable, which implies that its # value is completely determined by its parents' values (betas and sigma in our case). # There is no uncertainty in the variable beyond that which is inherent in # the parents' values mu = beta0 + betas [ 0 ] * x1 + betas [ 1 ] * x2 # We could have defined mu as a Deterministic variable # mu = pm.Deterministic('mu', beta0 + betas[0]*x1 + betas[1]*x2) # Likelihood function = how probable is my observed data? # This is a special case of a stochastic variable that we call an observed stochastic. # It is identical to a standard stochastic, except that its observed argument, # which passes the data to the variable, indicates that the values for this variable # were observed, and should not be changed by any fitting algorithm applied to the model. # The data can be passed in the form of either a numpy.ndarray or pandas.DataFrame object. Y_obs = Normal ( 'Y_obs' , mu = mu , sd = sigma , observed = Y ) In [12]: Y_obs . distribution Out[12]: $\\text{None} \\sim \\text{Normal}(\\mathit{mu}=f(f(f(\\text{beta0}),~f(f(f(\\text{betas})),~array)),~f(f(f(\\text{betas})),~array)),~\\mathit{sigma}=f(\\text{sigma}))$ Note: If our problem was a classification for which we would use Logistic regression see below In [13]: model_to_graphviz ( my_linear_model ) Out[13]: %3 cluster2 2 cluster1,000 1,000 sigma sigma ~ HalfNormal Y_obs Y_obs ~ Normal sigma->Y_obs beta0 beta0 ~ Normal beta0->Y_obs betas betas ~ Normal betas->Y_obs Markov Chain Monte Carlo (MCMC) Simulations PyMC3 uses the No-U-Turn Sampler (NUTS) and the Random Walk Metropolis , two Markov chain Monte Carlo (MCMC) algorithms for sampling in posterior space. Monte Carlo gets into the name because when we sample in posterior space, we choose our next move via a pseudo-random process. NUTS is a sophisticated algorithm that can handle a large number of unknown (albeit continuous) variables. Fitting the Model with Sampling - Doing Inference See below for PyMC3's sampling method. As you can see it has quite a few parameters. Most of them are set to default values by the package. For some, it's useful to set your own values. pymc3.sampling.sample(draws=500, step=None, init='auto', n_init=200000, start=None, trace=None, chain_idx=0, chains=None, cores=None, tune=500, progressbar=True, model=None, random_seed=None, discard_tuned_samples=True, compute_convergence_checks=True, **kwargs) Parameters to set: draws (int): number of samples to draw, defaults to 500. step (MCMC method): implementation method for MCMC. Better to let pyMC3 assign the best one. If manually setting we can choose between some implementations such as Metropolis() or NUTS() . tune (int): number of iterations to tune for, a.k.a. the \"burn-in\" period, defaults to 500. target_accept (float in $[0, 1]$). The step size is tuned such that we approximate this acceptance rate. Higher values like 0.9 or 0.95 often work better for problematic posteriors. (optional) cores (int) number of chains to run in parallel, defaults to the number of CPUs in the system, but at most 4. pm.sample returns a pymc3.backends.base.MultiTrace object that contains the samples. We usually name it trace . All the information about the posterior is in trace , which also provides statistics about the sampler. In [14]: ## uncomment this to see more about pm.sample #help(pm.sample) Specify a NUTS() sampler It is the default and we expect good results. In [15]: with my_linear_model : print ( f 'Starting MCMC process' ) # draw 2000 posterior samples and run the default number of chains = 4 trace = sample ( 2000 , tune = 1000 , chains = 2 , target_accept = 0.95 ) print ( f 'DONE' ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Starting MCMC process Multiprocess sampling (2 chains in 4 jobs) NUTS: [sigma, betas, beta0] Sampling 2 chains, 0 divergences: 100%|██████████| 6000/6000 [07:16<00:00, 13.74draws/s] DONE Model Plotting PyMC3 provides a variety of visualizations via plots: https://docs.pymc.io/api/plots.html . ArviZ is a Python library that works with PyMC3 for visualizing posterior distributions. One of them is the traceplot . The left column consists of a smoothed histogram (using kernel density estimation) of the marginal posteriors of each stochastic random variable while the right column contains the samples of the Markov chain plotted in sequential order. The beta variable, being vector-valued, produces two histograms and two sample traces, corresponding to both predictor coefficients. Let's compare with the true hidden parameter values sigma = 1, beta0 = 1 , beta1 = 1, beta2 = 2.5 In [44]: az . plot_posterior ( trace , var_names = [ 'beta0' , 'betas' ]); In [17]: from pymc3 import traceplot , compareplot , plot_posterior , forestplot traceplot ( trace ); The Highest Posterior Density (HPD) is the shortest interval that has the given probability indicated by the HPD. $\\hat{R}$ is a metric for comparing how well a chain has converged to the equilibrium distribution by comparing its behavior to other randomly initialized Markov chains. Multiple chains initialized from different initial conditions should give similar results. If all chains converge to the same equilibrium, $\\hat{R}$ will be 1. If the chains have not converged to a common distribution, $\\hat{R}$ will be > 1.01. $\\hat{R}$ is a necessary but not sufficient condition. For details on the $\\hat{R}$ see Gelman and Rubin (1992) . In [19]: pm . rhat ( trace ) Out[19]: Dimensions: (betas_dim_0: 2) Coordinates: * betas_dim_0 (betas_dim_0) int64 0 1 Data variables: beta0 float64 1.001 betas (betas_dim_0) float64 1.003 1.003 sigma float64 1.001 In [20]: forestplot ( trace , varnames = [ 'beta0' , 'betas' , 'sigma' ], r_hat = True ); Table of summary statistics hdi_3% & hdi_97% — The lower and upper bounds of a ∽95% credible interval. mcse_mean, mcse_sd — Average and stand. deviation of Monte Carlo standard error. ess_mean — Estimated effective sample size from sampling number of draws for each parameter. Effective sample size should be close to actual size, it goes down when parameters become correlated. In [22]: # Then we will generate and display our table az . summary ( trace ) Out[22]: mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat beta0 1.014 0.063 0.887 1.121 0.001 0.001 2291.0 2287.0 2291.0 2065.0 1.0 betas[0] 1.218 8.824 -14.349 18.780 0.230 0.163 1466.0 1466.0 1478.0 1523.0 1.0 betas[1] 2.338 4.412 -6.722 9.858 0.115 0.084 1465.0 1378.0 1477.0 1497.0 1.0 sigma 1.002 0.023 0.958 1.042 0.000 0.000 2421.0 2414.0 2443.0 1893.0 1.0 Specify a Metropolis() sampler We do not expect good results. In [23]: #help(pm.backends.base.MultiTrace) In [24]: with my_linear_model : print ( f 'Starting MCMC process' ) # draw 2000 posterior samples and run the default number of chains = 4 trace_metropolis = sample ( 2000 , step = pm . Metropolis (), tune = 1000 ) #, target_accept=0.9) print ( f 'DONE' ) Starting MCMC process Multiprocess sampling (4 chains in 4 jobs) CompoundStep >Metropolis: [sigma] >Metropolis: [betas] >Metropolis: [beta0] Sampling 4 chains, 0 divergences: 100%|██████████| 12000/12000 [00:05<00:00, 2360.97draws/s] The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge. The estimated number of effective samples is smaller than 200 for some parameters. DONE In [25]: from pymc3 import traceplot , compareplot , plot_posterior , forestplot traceplot ( trace_metropolis ); In [27]: az . plot_posterior ( trace_metropolis , var_names = [ 'beta0' , 'betas' ]); In [28]: #help(az.plot_posterior) In [29]: #help(pm.Normal) In [30]: # Then we will generate and display our table az . summary ( trace_metropolis ) Out[30]: mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat beta0 1.017 0.063 0.904 1.136 0.005 0.003 191.0 191.0 194.0 252.0 1.04 betas[0] 0.472 4.793 -9.535 5.495 2.359 1.803 4.0 4.0 5.0 17.0 2.21 betas[1] 2.709 2.388 0.212 7.667 1.175 0.899 4.0 4.0 5.0 17.0 2.19 sigma 1.003 0.023 0.957 1.045 0.001 0.001 866.0 864.0 870.0 731.0 1.01 In [31]: trace_metropolis . varnames Out[31]: ['beta0', 'betas', 'sigma_log__', 'sigma'] In [32]: #help(pm.backends.base.MultiTrace) This linear regression example is from the original paper on PyMC3: Salvatier J, Wiecki TV, Fonnesbeck C. 2016. Probabilistic programming in Python using PyMC3. PeerJ Computer Science 2:e55 https://doi.org/10.7717/peerj-cs.55 Model Prediction Let's sample directly from the posterior predictive distribution of our PyMC3 model. This way we can create some predicted regression lines. In [33]: with my_linear_model : post = pm . sample_posterior_predictive ( trace , samples = 4 ) post 100%|██████████| 4/4 [00:00<00:00, 44.42it/s] Out[33]: {'Y_obs': array([[ 0.66814578, 1.76554148, 1.41547496, ..., 5.83976609, 5.78308538, 6.09349731], [-0.07221673, 0.50181349, 1.25587781, ..., 8.43380066, 5.86848228, 6.34353836], [ 2.63758695, 0.90434407, 2.84861913, ..., 8.01730308, 7.42665533, 6.61252778], [ 0.12305881, 0.49877369, 1.54025709, ..., 7.20657886, 6.5785189 , 4.9889308 ]])} In [34]: trace [ 'beta0' ] . mean () Out[34]: 1.0142560725210414 In [35]: trace [ 'betas' ][:, 0 ] . mean () Out[35]: 1.2178921328018786 In [36]: np . meshgrid ( np . linspace ( 0 , 2 , 4 ), np . linspace ( 0 , 3 , 4 )) Out[36]: [array([[0. , 0.66666667, 1.33333333, 2. ], [0. , 0.66666667, 1.33333333, 2. ], [0. , 0.66666667, 1.33333333, 2. ], [0. , 0.66666667, 1.33333333, 2. ]]), array([[0., 0., 0., 0.], [1., 1., 1., 1.], [2., 2., 2., 2.], [3., 3., 3., 3.]])] In [37]: fig = plt . figure ( figsize = ( 5 , 4 )) fontsize = 14 labelsize = 8 title = 'Bayesian Regression Line and Observed Data' ax = fig . add_subplot ( 111 , projection = '3d' ) # predicted surface x1_pred , x2_pred = np . meshgrid ( np . linspace ( 0 , 2 , 10 ), np . linspace ( 0 , 3 , 10 )) b0 = trace [ 'beta0' ] . mean () b1 = trace [ 'betas' ][:, 0 ] . mean () b2 = trace [ 'betas' ][:, 1 ] . mean () for b0 , b in zip ( trace [ 'beta0' ][ 30 : 32 ], trace [ 'betas' ][ 30 : 32 ]): ax . plot_surface ( x1_pred , x2_pred , b0 + b [ 0 ] * x1_pred + b [ 1 ] * x2_pred , alpha = 0.1 , color = 'cyan' ) #z = b0 + b1*x1_pred + b2*x2_pred #ax.plot_surface(x1_pred, x2_pred, z, alpha=0.2, color='cyan') # original data ax . scatter ( x1 , x2 , Y , alpha = 1. ) ax . set_xlabel ( r '$x_1$' , fontsize = fontsize ) ax . set_ylabel ( r '$x_2$' , fontsize = fontsize ) ax . set_zlabel ( r '$Y$' , fontsize = fontsize ) ax . tick_params ( labelsize = labelsize ) fig . suptitle ( title , fontsize = fontsize ) fig . tight_layout ( pad = .1 , w_pad = 10.1 , h_pad = 2. ) plt . tight_layout plt . show () What about Logistic Regression? If the problem above was a classification that required a Logistic Regression, we would use the logistic function ( where $\\beta_0$ is the intercept, and $\\beta_i$ (i=1, 2, 3) determines the shape of the logistic function). \\begin{equation} Pr(Y=1|X_1,X_2,X3) = {\\frac{1}{1 + exp&#94;{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3)}}} \\end{equation} Since both $\\beta_0$ and the $\\beta_i$s can be any possitive or negative number, we can model them as gaussian random variables. \\begin{eqnarray} \\beta_0 \\sim \\mathcal{N}(\\mu,\\,\\sigma&#94;2) \\\\ \\beta_i \\sim \\mathcal{N}(\\mu_i,\\,\\sigma_i&#94;2) \\end{eqnarray} In PyMC3 we can model those as: pm.Normal('beta_0', mu=0, sigma=100) (where $\\mu$ and $\\sigma&#94;2$ can have some initial values that we assign them, e.g. 0 and 100) The deterministic variable would be the log-odds: \\begin{equation} p\\_logit = ln{\\frac{p}{1 - p}} = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 \\end{equation} or in code: p_logit = beta0 + beta_1 * X_1 + beta_2 * X_2 + beta_3 * X_3 To connect this variable ($\\ p\\_logit \\ $) with our observed data, we would use a Bernoulli as our likelihood. our_likelihood = pm.Bernoulli('our_likelihood', logit_p=p_logit, observed=our_data) Notice that the main difference with Linear Regression is the use of a Bernoulli distribution instead of a Gaussian distribution, and the use of the logistic function instead of the identity function. We could also use pm.Deterministic as follows: p_i = pm.Deterministic('p_i', pm.math.invlogit(beta0 + beta_1 * X_1 + beta_2 * X_2 + beta_3 * X_3) And then add this to the likelihood function with the parameter p (p: float Probability of success (0 < p < 1), instead of logit_p (logit_p: float Logit of success probability). Note that only one of p or logit_p can be specified. Then you could define the likelihood: likelh = pm.Bernoulli('likelh', p=p_i, observed=data) In [38]: # A reminder of what the logistic function looks like. # Play with parameters a and b to see the shape of the curve change b = 5. x = np . linspace ( - 8 , 8 , 100 ) plt . plot ( x , 1 / ( 1 + np . exp ( - b * x ))) plt . xlabel ( 'y' ) plt . ylabel ( 'y=logistic(x)' ) Out[38]: Text(0, 0.5, 'y=logistic(x)')","tags":"labs","url":"labs/lab02/bayes-nb/"},{"title":"Lab 02:","text":"CS109B Data Science 2: Advanced Topics in Data Science Lab 02 - Review of Probability Distributions Harvard University Spring 2022 Instructors: Mark Glickman and Pavlos Protopapas Lab instructor and content: Eleni Angelaki Kaxiras In [1]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: In [2]: import warnings import arviz as az import matplotlib.pyplot as plt import numpy as np import pymc3 as pm import theano.tensor as tt from pymc3 import summary warnings . simplefilter ( action = \"ignore\" , category = FutureWarning ) warnings . filterwarnings ( 'ignore' ) import numpy as np import matplotlib.pyplot as plt from matplotlib import gridspec import scipy.stats as stats import pandas as pd import seaborn as sns % matplotlib inline import warnings print ( 'Running on PyMC3 v {} ' . format ( pm . __version__ )) Running on PyMC3 v3.8 In [3]: %% javascript IPython . OutputArea . auto_scroll_threshold = 20000 ; In [4]: #pandas trick pd . options . display . max_columns = 50 # None -> No Restrictions pd . options . display . max_rows = 200 # None -> Be careful with this pd . options . display . max_colwidth = 100 pd . options . display . precision = 3 A review of Common Probability Distributions Discrete Distributions The random variable has a probability mass function (pmf) which measures the probability that our random variable will take a specific value $y$, denoted $P(Y=y)$. Bernoulli (binary outcome, success has probability $\\theta$, $one$ trial): $ P(Y=k) = \\theta&#94;k(1-\\theta)&#94;{1-k} $ Binomial (binary outcome, success has probability $\\theta$, $k$ sucesses, $n$ trials): \\begin{equation} P(Y=k) = {{n}\\choose{k}} \\cdot \\theta&#94;k(1-\\theta)&#94;{n-k} \\end{equation} Note : Binomial(1,$p$) = Bernouli($p$) Poisson (counts independent events occurring at a rate $\\lambda$) \\begin{equation} P\\left( Y=y|\\lambda \\right) = \\frac{{e&#94;{ - \\lambda } \\lambda &#94;y }}{{y!}} \\end{equation} y = 0,1,2,... Categorical, or Multinulli (random variables can take any of K possible categories, each having its own probability; this is a generalization of the Bernoulli distribution for a discrete variable with more than two possible outcomes, such as the roll of a die) Continuous Distributions The random variable has a probability density function (pdf) . Uniform (variable equally likely to be near each value in interval $(a,b)$) \\begin{equation} P(X = x) = \\frac{1}{b - a} \\end{equation} anywhere within the interval $(a, b)$, and zero elsewhere. Normal (a.k.a. Gaussian) \\begin{equation} X \\sim \\mathcal{N}(\\mu,\\,\\sigma&#94;{2}) \\end{equation} A Normal distribution can be parameterized either in terms of precision $\\tau$ or variance $\\sigma&#94;{2}$. The link between the two is given by \\begin{equation} \\tau = \\frac{1}{\\sigma&#94;{2}} \\end{equation} Expected mean $\\mu$ Variance $\\frac{1}{\\tau}$ or $\\sigma&#94;{2}$ Parameters: mu: float , sigma: float or tau: float Range of values (-$\\infty$, $\\infty$) Beta (where the variable ($\\theta$) takes on values in the interval $[0,1]$, and is parametrized by two positive parameters, $\\alpha$ and $\\beta$ that control the shape of the distribution. Note that Beta is a good distribution to use for priors (beliefs) because its range is $[0,1]$ which is the natural range for a probability and because we can model a wide range of functions by changing the $\\alpha$ and $\\beta$ parameters. Its density is: \\begin{equation} \\label{eq:beta} P(\\theta|a,b) = \\frac{1}{B(\\alpha, \\beta)} {\\theta}&#94;{\\alpha - 1} (1 - \\theta)&#94;{\\beta - 1} \\propto {\\theta}&#94;{\\alpha - 1} (1 - \\theta)&#94;{\\beta - 1} \\end{equation} where the normalisation constant, $B$, is a beta function of $\\alpha$ and $\\beta$, \\begin{equation} B(\\alpha, \\beta) = \\int_{t=0}&#94;1 t&#94;{\\alpha - 1} (1 - t)&#94;{\\beta - 1} dt. \\end{equation} 'Nice', unimodal distribution Range of values $[0, 1]$ Exponential Range of values [$0$, $\\infty$] Gamma Code Resources: Statistical Distributions in numpy/scipy: scipy.stats Statistical Distributions in pyMC3: distributions in PyMC3 (we will see when we look at PyMC3). Exercises: Discrete Probability Plots Poisson Change the value of $\\lambda$ in the Poisson PMF and see how the plot changes. Remember that the y-axis in a discrete probability distribution shows the probability of the random variable having a specific value in the x-axis. \\begin{equation} P\\left( X=y \\right|\\lambda) = \\frac{{e&#94;{ - \\lambda } \\lambda &#94;y }}{{y!}} \\end{equation} for $y \\ge0$. Routine is stats.poisson.pmf(x, lambda) . $\\lambda$ is our $\\theta$ in this case. $\\lambda$ is also the mean in this distribution. In [5]: plt . style . use ( 'seaborn-darkgrid' ) x = np . arange ( 0 , 60 ) for lam in [ 0.5 , 3 , 8 ]: pmf = stats . poisson . pmf ( x , lam ) plt . plot ( x , pmf , alpha = 0.5 , label = '$\\lambda$ = {} ' . format ( lam )) plt . xlabel ( 'random variable' , fontsize = 12 ) plt . ylabel ( 'probability' , fontsize = 12 ) plt . legend ( loc = 1 ) plt . ylim = ( - 0.1 ) plt . show () Binomial In [6]: plt . style . use ( 'seaborn-darkgrid' ) x = np . arange ( 0 , 50 ) ns = [ 10 , 17 ] ps = [ 0.5 , 0.7 ] for n , p in zip ( ns , ps ): pmf = stats . binom . pmf ( x , n , p ) plt . plot ( x , pmf , alpha = 0.5 , label = 'n = {} , p = {} ' . format ( n , p )) plt . xlabel ( 'x' , fontsize = 14 ) plt . ylabel ( 'f(x)' , fontsize = 14 ) plt . legend ( loc = 1 ) plt . show () Exercise: Continuous Distributions Plot Uniform Change the value of $\\mu$ in the Uniform PDF and see how the plot changes. Remember that the y-axis in a continuous probability distribution does not shows the actual probability of the random variable having a specific value in the x-axis because that probability is zero!. Instead, to see the probability that the variable is within a small margin we look at the integral below the curve of the PDF. The uniform is often used as a noninformative prior. Uniform - numpy.random.uniform(a=0.0, b=1.0, size) $\\alpha$ and $\\beta$ are our parameters. size is how many tries to perform. Our $\\theta$ is basically the combination of the parameters a,b. We can also call it \\begin{equation} \\mu = (a+b)/2 \\end{equation} In [7]: from scipy.stats import uniform r = uniform . rvs ( size = 1000 ) plt . plot ( r , uniform . pdf ( r ), 'r-' , lw = 5 , alpha = 0.6 , label = 'uniform pdf' ) plt . hist ( r , density = True , histtype = 'stepfilled' , alpha = 0.2 ) plt . ylabel ( r 'probability density' ) plt . xlabel ( f 'random variable' ) plt . legend ( loc = 'best' , frameon = False ) plt . show () Beta We get an amazing set of shapes by tweaking the two parameters $a$ and $b$! Notice that for $a=b=1.$ we get a constant. From then on, as the values increase, we get a curve that looks more and more Gaussian. In [8]: from scipy.stats import beta fontsize = 15 alphas = [ 0.5 , 0.5 , 1. , 3. , 6. ] betas = [ 0.5 , 1. , 1. , 3. , 6. ] x = np . linspace ( 0 , 1 , 1000 ) colors = [ 'red' , 'green' , 'blue' , 'black' , 'pink' ] fig , ax = plt . subplots ( figsize = ( 8 , 5 )) for a , b , colors in zip ( alphas , betas , colors ): dist = beta ( a , b ) plt . plot ( x , dist . pdf ( x ), c = colors , label = f 'a= { a } , b= { b } ' ) ax . set_ylim ( 0 , 3 ) ax . set_xlabel ( r '$\\theta$' , fontsize = fontsize ) ax . set_ylabel ( r 'P ($\\theta|\\alpha,\\beta)$' , fontsize = fontsize ) ax . set_title ( 'Beta Distribution' , fontsize = fontsize * 1.2 ) ax . legend ( loc = 'best' ) fig . show (); Gaussian In [9]: y = pm . Exponential . dist ( lam = 2 ), #y = pm.Binomial.dist(n=10, p=0.5) type ( y ) #print(y.logp(4).eval()) #plt.plot(y.random(size=30)) Out[9]: tuple In [10]: plt . style . use ( 'seaborn-darkgrid' ) x = np . linspace ( - 5 , 5 , 1000 ) mus = [ 0. , 0. , 0. , - 2. ] sigmas = [ 0.4 , 1. , 2. , 0.4 ] for mu , sigma in zip ( mus , sigmas ): pdf = stats . norm . pdf ( x , mu , sigma ) plt . plot ( x , pdf , label = r '$\\mu$ = ' + f ' { mu } ,' + r '$\\sigma$ = ' + f ' { sigma } ' ) plt . xlabel ( 'random variable' , fontsize = 12 ) plt . ylabel ( 'probability density' , fontsize = 12 ) plt . legend ( loc = 1 ) plt . show () At home : Prove the formula mentioned in class which gives the probability density for a Beta distribution with parameters $2$ and $5$: $p(\\theta|2,5) = 30 \\cdot \\theta(1 - \\theta)&#94;4$ References : Distributions in PyMC3 Information about PyMC3 functions including descriptions of distributions, sampling methods, and other functions, is available via the help command.","tags":"labs","url":"labs/lab02/pdfs-nb/"},{"title":"Lab 1: Clustering","text":"Notebook Lab 1: Intro Lab 1: Clustering","tags":"labs","url":"labs/lab01/"},{"title":"Lab 1: Clustering","text":"CS109B Data Science 2: Advanced Topics in Data Science Lecture 01 - Clustering with Python Harvard University Spring 2022 Instructors: Mark Glickman and Pavlos Protopapas Lab Instructor: Eleni Kaxiras Content: Eleni Kaxiras, Chris Tanner, and Will Claybaugh In [1]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES # import requests # from IPython.core.display import HTML # styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\").text # HTML(styles) In [2]: import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from sklearn import preprocessing from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans from sklearn.feature_extraction.text import CountVectorizer from scipy.spatial import distance from sklearn.metrics import silhouette_samples , silhouette_score import matplotlib.cm as cm from sklearn.cluster import DBSCAN from sklearn.neighbors import NearestNeighbors from sklearn.preprocessing import StandardScaler from sklearn.datasets import make_blobs import scipy.cluster.hierarchy as hac from scipy.spatial.distance import pdist import scipy.cluster.hierarchy as hac from scipy.spatial.distance import pdist % matplotlib inline In [3]: from gap_statistic import OptimalK In [4]: from sklearn.datasets import make_blobs from sklearn.datasets import make_classification from sklearn.datasets import load_digits import warnings warnings . filterwarnings ( 'ignore' ) Learning Objectives Understand the common distance metrics (e.g., Euclidean, Manhattan, Hamming). Understand the difference between a vector matrix and a distance matrix Understand how different clustering algorithms work (e.g., k-means, Hierarchical, DBSCAN). For home: Review what PCA is and know the differences between PCA and clustering, Table of Contents PCA Refresher Preparing the data Choosing a distance or dissimilarity metric Clustering algorithms and measuring the quality of clusters Extra: Clustering for images Unsupervised Learning, Cluster Analysis, and Classification Review : What is unsupervised learning? What is Cluster Analysis? Is the response variable included in the algorithm? What does it mean to perform classification? 1. PCA Refresher image source: [1] Review What is PCA? How can it be useful? What are its limitations? image source: [1] Sklearn's sklearn.decomposition.PCA uses the LAPACK library written in Fortran 90 (based on the LINPACK library from the 70s) which provides routines for solving systems of simultaneous linear equations, least-squares solutions of linear systems of equations. How to use the sklearn PCA package: a. Instantiate a new PCA object: pca_transformer = PCA() b. Fit some data (learns the transformation based on this data): fitted_pca = pca_transformer.fit(data_frame) c. Transform the data to the reduced dimensions: pca_df = fitted_pca.transform(data_frame) Using two distinct steps (i.e., (b) and (c)) to fit and transform our data allows one the flexibility to transform any dataset according to our learned fit() . Alternatively, if you know you only want to transform a single dataset, you can combine (b) and (c) into one step: Fit and transform: pca_df = pca_transformer.fit_transform(pca_df) Note: We fit on the training set and transform both training and test set. Example: Playing with synthetic data Sklearn has methods for generating synthetic datasets . They can be quite useful for testing clustering for classification purposes. In [5]: n_features = 2 n_classes = 1 plt . title ( f 'Features = { n_features } ' , fontsize = 'medium' ) X1 , Y1 = make_classification ( n_features = n_features , n_redundant = 0 , n_informative = 1 , n_clusters_per_class = 1 , n_classes = n_classes ) colors = [ \"#4EACC5\" , \"#FF9C34\" , \"#4E9A06\" ] plt . scatter ( X1 [:, 0 ], X1 [:, 1 ], marker = 'o' , c = Y1 , s = 25 , edgecolor = 'k' ); class_df = pd . DataFrame ( X1 , Y1 ) . reset_index ( drop = True ) class_df = class_df . rename ( columns = { 0 : 'feature1' , 1 : 'feature2' }) plt . axis ( 'equal' ); Out[5]: (-1.488981045742101, 3.2948130634705572, -2.007589788641174, 2.621466822917984) In [6]: X1 . shape , Y1 . shape Out[6]: ((100, 2), (100,)) In [7]: class_df . head () Out[7]: feature1 feature2 0 1.012265 -0.072951 1 -0.474946 -1.625100 2 0.265944 1.600704 3 1.826242 -1.519311 4 -0.424610 0.802087 In [8]: scaler = StandardScaler () scaler . fit ( class_df ) scaled_df = pd . DataFrame ( scaler . transform ( class_df )) In [9]: pca_transformer = PCA ( n_components = 2 ) fitted_pca = pca_transformer . fit ( class_df ) fitted_pca . explained_variance_ratio_ Out[9]: array([0.591269, 0.408731]) In [10]: pca_df = pd . DataFrame ( fitted_pca . transform ( class_df )) pca_df = pca_df . rename ( columns = { 0 : 'pc1' , 1 : 'pc2' }) pca_df . head () Out[10]: pc1 pc2 0 -0.164327 -0.128186 1 -2.304623 0.072059 2 0.682850 1.496747 3 -0.791398 -1.664837 4 -0.371981 1.541299 In [11]: fitted_pca . explained_variance_ratio_ Out[11]: array([0.591269, 0.408731]) For more read: \" Importance of feature scaling \" Sklearn's StandardScaler 2 - Preparing the data Discussion: To scale or not to scale? Depends For more read: \" Importance of feature scaling \" Sklearn's StandardScaler 3 - Distance and dissimilarity metrics The Euclidean norm (or length) of a vector $\\textbf{v}=[v_1,v_2,..,v_n]&#94;T$ in $\\mathbb{R}&#94;n$ is the nonnegative scalar \\begin{aligned} \\lVert \\textbf{v} \\rVert = \\sqrt{\\textbf{v}\\cdot \\textbf{v}} = \\sqrt{{v_1}&#94;2+{v_2}&#94;2+\\cdots+{v_n}&#94;2} \\end{aligned} The Manhattan norm of the same vector is the nonnegative scalar \\begin{aligned} \\lVert \\textbf{v} \\rVert = \\lvert \\textbf{v} \\rvert = \\lvert v_1 \\rvert + \\lvert v_2 \\rvert + \\cdots + \\lvert v_n \\rvert \\end{aligned} The distance between two vectors $\\textbf{v}$ and $\\textbf{u}$ is defined by $d(\\textbf{v}, \\textbf{u}) = \\lVert \\textbf{v} - \\textbf{u} \\rVert$ Let's practice on the diagram below; we are concerned with measuring the distance between two points, $\\textbf{p}=(p_1,p_2)$ and $\\textbf{q}=(q_1,q_2)$. (edited from Wikipedia.org) Euclidean distance: The Euclidean distance measures the shortest path between the two points, navigating through all dimensions: $d(\\textbf{p}, \\textbf{q}) = \\lVert \\textbf{p} - \\textbf{q} \\rVert = \\sqrt{{(p_1-q_1)}&#94;2+{(p_2-q_2)}&#94;2}$ For vectors in $\\mathbb{R}&#94;n$: $d(\\textbf{p}, \\textbf{q}) = \\lVert \\textbf{p} - \\textbf{q} \\rVert = \\sqrt{{(p_1-q_1)}&#94;2+{(p_2-q_2)}&#94;2+\\cdots +{(p_n-q_n)}&#94;2}$ Manhattan distance: The Manhattan distance measures the cumulative difference between the two points, across all dimensions. $d_1(\\textbf{p}, \\textbf{q}) = \\lVert \\textbf{p} - \\textbf{q} \\rVert_1 = \\sum_{i=1}&#94;{n} \\mid p_i-q_1 \\mid$ Extra: Cosine distance $\\cos{\\theta} = \\frac{\\textbf{q}\\textbf{q}}{\\lVert \\textbf{p}\\rVert \\lVert\\textbf{q} \\rVert} $ In [12]: count_vect = CountVectorizer () sent0 = \"Biden is here\" sent1 = \"President is coming here\" corpus = [ sent0 , sent1 ] sentences = count_vect . fit_transform ( corpus ) v1 = sentences . toarray ()[ 0 ] v2 = sentences . toarray ()[ 1 ] print ( f 'v1 = { v1 } , \\n v2 = { v2 } ' ) # pretty print df = pd . DataFrame ( sentences . toarray (), \\ columns = count_vect . get_feature_names (), index = [ 'Sentence 0' , 'Sentence 1' ]) print ( f 'distance = { distance . cosine ( v1 , v2 ) } ' ) df v1 = [1 0 1 1 0], v2 = [0 1 1 1 1] distance = 0.42264973081037416 Out[12]: biden coming here is president Sentence 0 1 0 1 1 0 Sentence 1 0 1 1 1 1 Note : Normally cosine value=0 means that the two vectors are orthogonal to each other. scipy implements cosine as 1-cosine , so cosine=0 means no connection and cosine=1 means orthogonal. Cosine metric is used in Collaborative Filtering (Recommender systems for movies). Hamming Distance (extra): If our two elements of comparison can be represented a sequence of discrete items, it can be useful to measure how many of their elements differ. For example: Mahmoud and Mahmood differ by just 1 character and thus have a hamming distance of 1. 10101 and 01101 have a hamming distance of 2. Mary and Barry have a hamming distance of 3 (m->b, y->r, null->y). Note : the last example may seem sub-optimal, as we could transform Mary to Barry by just 2 operations (substituting the M with a B, then adding an 'r'). So, their so-called edit distance is smaller than their Hamming distance. The very related Levenshtein distance here can handle this, and thus tends to be more appropriate for Strings. 4 - Clustering Algorithms Question: Why do we care about clustering? How/why is it useful? We will now walk through three clustering algorithms, first discussing them at a high-level, then showing how to implement them with Python libraries. Let's first load and scale our data, so that particular dimensions don't naturally dominate in their contributions in the distant calculations: In [13]: multishapes = pd . read_csv ( \"data/multishapes.csv\" ) ms_df = multishapes [[ 'x' , 'y' ]] ms_df . plot . scatter ( x = 'x' , y = 'y' , c = 'Blue' , \\ title = \"Multishapes data\" , \\ figsize = ( 5.5 , 4.2 )) plt . show () ms_df . head () Out[13]: x y 0 -0.803739 -0.853053 1 0.852851 0.367618 2 0.927180 -0.274902 3 -0.752626 -0.511565 4 0.706846 0.810679 In [14]: # displays our summary statistics of our data ms_df . describe () Out[14]: x y count 1100.000000 1100.000000 mean -0.081222 -0.625431 std 0.644967 1.176170 min -1.489180 -3.353462 25% -0.478839 -1.126752 50% -0.132920 -0.297040 75% 0.366072 0.250817 max 1.492208 1.253874 In [15]: scaler = StandardScaler () scaler = scaler . fit ( ms_df ) print ( scaler . mean_ ) scaled_df = scaler . transform ( ms_df ) ###### if I had a test set I would transform here: # test_scaled_df = scaler.transform(test_df) ################################################## scaled_df = pd . DataFrame ( scaled_df , \\ index = multishapes [ 'shape' ], columns = ms_df . columns ) scaled_df . describe () [-0.08122171 -0.6254313 ] Out[15]: x y count 1.100000e+03 1.100000e+03 mean -3.108624e-17 -2.779595e-16 std 1.000455e+00 1.000455e+00 min -2.183985e+00 -2.320473e+00 25% -6.167723e-01 -4.264248e-01 50% -8.019252e-02 2.793306e-01 75% 6.938298e-01 7.453401e-01 max 2.440659e+00 1.598544e+00 Very important reminder!! If you have a training and a test set, always .fit() your scaler only to the training set, and then .transform() both sets. Let's plot this data with and without scaling In [16]: # plot our data msplot = ms_df . plot . scatter ( x = 'x' , y = 'y' , c = 'Black' , \\ title = \"Multishapes data (no scaling)\" , \\ figsize = ( 5.5 , 4.2 )) msplot . set_xlabel ( \"X\" ) msplot . set_ylabel ( \"Y\" ) plt . show () In [17]: # plots our data msplot = scaled_df . plot . scatter ( x = 'x' , y = 'y' , c = 'Black' , \\ title = \"Multishapes data (w/ scaling)\" , \\ figsize = ( 5.5 , 4.2 )) msplot . set_xlabel ( \"X\" ) msplot . set_ylabel ( \"Y\" ) plt . show () 3a. $k$-Means clustering: Code (via sklearn ): In [18]: ms_kmeans = KMeans ( n_clusters = 2 , init = 'random' , n_init = 3 , random_state = 109 ) ms_kmeans . fit ( scaled_df ) Out[18]: KMeans(init='random', n_clusters=2, n_init=3, random_state=109) Now that we've run k-means, we can look at various attributes of our clusters. Full documenation is here . In [19]: display ( ms_kmeans . cluster_centers_ ) display ( ms_kmeans . labels_ [ 0 : 10 ]) array([[-0.44316951, -1.56366452], [ 0.1434568 , 0.50616818]]) array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32) Plotting Take note of matplotlib's c= argument to color items in the plot, along with our stacking two different plotting functions in the same plot. In [20]: plt . figure ( figsize = ( 10 , 10 )) plt . scatter ( scaled_df [ 'x' ], scaled_df [ 'y' ], c = ms_kmeans . labels_ ); plt . scatter ( ms_kmeans . cluster_centers_ [:, 0 ], ms_kmeans . cluster_centers_ [:, 1 ], c = 'r' , marker = 'h' , s = 100 ); Out[20]: Quality of Clusters A - Inertia Inertia measures the total squared distance from points to their cluster's centroid. We obviously want this distance to be relatively small. If we increase the number of clusters, it will naturally make the average distance smaller. If every point has its own cluster, then our distance would be 0. That's obviously not an ideal way to cluster. One way to determine a reasonable number of clusters to simply try many different clusterings as we vary k , and each time, measure the overall inertia. In [21]: wss = [] for i in range ( 1 , 11 ): fitx = KMeans ( n_clusters = i , init = 'random' , n_init = 5 , random_state = 109 ) . fit ( scaled_df ) wss . append ( fitx . inertia_ ) plt . figure ( figsize = ( 11 , 8.5 )) plt . plot ( range ( 1 , 11 ), wss , 'bx-' ) plt . xlabel ( 'Number of clusters $k$' ) plt . ylabel ( 'Inertia' ) plt . title ( 'The Elbow Method showing the optimal $k$' ) plt . show () Look for the place(s) where distance stops decreasing as much (i.e., the 'elbow' of the curve). It seems that 4 would be a good number of clusters, as a higher k yields diminishing returns. Exercise : Run K-means again with 4 clusters this time. B - Silhouette Let's say we have a data point $i$, and the cluster it belongs to is referred to as $C(i)$. One way to measure the quality of a cluster $C(i)$ is to measure how close its data points are to each other (within-cluster) compared to nearby, other clusters $C(j)$. This is what Silhouette Scores provide for us. The range is [-1,1]; 0 indicates a point on the decision boundary (equal average closeness to points intra-cluster and out-of-cluster), and negative values mean that datum might be better in a different cluster. Specifically, let $a(i)$ denote the average distance data point $i$ is to the other points in the same cluster: Similarly, we can also compute the average distance that data point $i$ is to all other clusters. The cluster that yields the minimum distance is denoted by $b(i)$: Hopefully our data point $i$ is much closer, on average, to points within its own cluster (i.e., $a(i)$ than it is to its closest neighboring cluster $b(i)$). The silhouette score quantifies this as $s(i)$: NOTE: If data point $i$ belongs to its own cluster (no other points), then the silhouette score is set to 0 (otherwise, $a(i)$ would be undefined). The silhouette score plotted below is the overall average across all points in our dataset. The silhouette_score() function is available in sklearn . We can manually loop over values of K (for applying k-Means algorithm), then plot its silhouette score. In [22]: scores = [ 0 ] for i in range ( 2 , 11 ): fitx = KMeans ( n_clusters = i , init = 'random' , n_init = 46 , random_state = 109 ) . fit ( scaled_df ) score = silhouette_score ( scaled_df , fitx . labels_ ) scores . append ( score ) plt . figure ( figsize = ( 11 , 8.5 )) plt . plot ( range ( 1 , 11 ), np . array ( scores ), 'bx-' ) plt . xlabel ( 'Number of clusters $k$' ) plt . ylabel ( 'Average Silhouette' ) plt . title ( 'The Elbow Method showing the optimal $k$' ) plt . show () Visualizing all Silhoutte scores for a particular clustering Below, we borrow from an sklearn example. The second plot may be overkill. The second plot is just the scaled data. It is not a PCA plot If you only need the raw silhouette scores, use the silhouette_samples() function In [23]: # modified code from # http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html def silplot ( X , clusterer , pointlabels = None ): cluster_labels = clusterer . labels_ n_clusters = clusterer . n_clusters # Create a subplot with 1 row and 2 columns fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 ) fig . set_size_inches ( 11 , 8.5 ) # The 1st subplot is the silhouette plot # The silhouette coefficient can range from -1, 1 but in this example all # lie within [-0.1, 1] ax1 . set_xlim ([ - 0.1 , 1 ]) # The (n_clusters+1)*10 is for inserting blank space between silhouette # plots of individual clusters, to demarcate them clearly. ax1 . set_ylim ([ 0 , len ( X ) + ( n_clusters + 1 ) * 10 ]) # The silhouette_score gives the average value for all the samples. # This gives a perspective into the density and separation of the formed # clusters silhouette_avg = silhouette_score ( X , cluster_labels ) print ( \"For n_clusters = \" , n_clusters , \", the average silhouette_score is \" , silhouette_avg , \".\" , sep = \"\" ) # Compute the silhouette scores for each sample sample_silhouette_values = silhouette_samples ( X , cluster_labels ) y_lower = 10 for i in range ( 0 , n_clusters ): # Aggregate the silhouette scores for samples belonging to # cluster i, and sort them ith_cluster_silhouette_values = \\ sample_silhouette_values [ cluster_labels == i ] ith_cluster_silhouette_values . sort () size_cluster_i = ith_cluster_silhouette_values . shape [ 0 ] y_upper = y_lower + size_cluster_i color = cm . nipy_spectral ( float ( i ) / n_clusters ) ax1 . fill_betweenx ( np . arange ( y_lower , y_upper ), 0 , ith_cluster_silhouette_values , facecolor = color , edgecolor = color , alpha = 0.7 ) # Label the silhouette plots with their cluster numbers at the middle ax1 . text ( - 0.05 , y_lower + 0.5 * size_cluster_i , str ( i )) # Compute the new y_lower for next plot y_lower = y_upper + 10 # 10 for the 0 samples ax1 . set_title ( \"The silhouette plot for the various clusters.\" ) ax1 . set_xlabel ( \"The silhouette coefficient values\" ) ax1 . set_ylabel ( \"Cluster label\" ) # The vertical line for average silhouette score of all the values ax1 . axvline ( x = silhouette_avg , color = \"red\" , linestyle = \"--\" ) ax1 . set_yticks ([]) # Clear the yaxis labels / ticks ax1 . set_xticks ([ - 0.1 , 0 , 0.2 , 0.4 , 0.6 , 0.8 , 1 ]) # 2nd Plot showing the actual clusters formed colors = cm . nipy_spectral ( cluster_labels . astype ( float ) / n_clusters ) ax2 . scatter ( X [:, 0 ], X [:, 1 ], marker = '.' , s = 200 , lw = 0 , alpha = 0.7 , c = colors , edgecolor = 'k' ) xs = X [:, 0 ] ys = X [:, 1 ] if pointlabels is not None : for i in range ( len ( xs )): plt . text ( xs [ i ], ys [ i ], pointlabels [ i ]) # Labeling the clusters centers = clusterer . cluster_centers_ # Draw white circles at cluster centers ax2 . scatter ( centers [:, 0 ], centers [:, 1 ], marker = 'o' , c = \"white\" , alpha = 1 , s = 200 , edgecolor = 'k' ) for i , c in enumerate ( centers ): ax2 . scatter ( c [ 0 ], c [ 1 ], marker = '$ %d $' % int ( i ), alpha = 1 , s = 50 , edgecolor = 'k' ) ax2 . set_title ( \"The visualization of the clustered data.\" ) ax2 . set_xlabel ( \"Feature space for the 1st feature\" ) ax2 . set_ylabel ( \"Feature space for the 2nd feature\" ) plt . suptitle (( \"Silhouette analysis for KMeans clustering on sample data \" \"with n_clusters = %d \" % n_clusters ), fontsize = 14 , fontweight = 'bold' ) In [24]: ms_kmeans = KMeans ( n_clusters = 4 , init = 'random' , n_init = 3 , random_state = 109 ) . fit ( scaled_df ) # plot a fancy silhouette plot silplot ( scaled_df . values , ms_kmeans ) For n_clusters = 4, the average silhouette_score is 0.45880539445085916. C - Gap Statistic The gap statistic compares within-cluster distances (such as in silhouette), but instead of comparing against the second-best existing cluster for that point, it compares our clustering's overall average to the average we'd see if the data were generated at random (we'd expect randomly generated data to not necessarily have any inherit patterns that can be easily clustered). In essence, the within-cluster distances (in the elbow plot) will go down just becuse we have more clusters. We additionally calculate how much they'd go down on non-clustered data with the same spread as our data and subtract that trend out to produce the plot below. The original paper is : \" Estimating the number of clusters in a data set via the gap statistic \" (Tibshirani et al.). As suggested in the paper, we would choose the value of $\\hat{k}$ (number of clusters) such that $\\hat{k}$ = smallest $k$ such that Gap($k$) $\\geq$ Gap($k+1$) - $s_{k+1}$. We compare the actual Gap value of the k point to the lower bar of the Gap value of the k+1 point. The following graph should make it clearer. The plot is from the original paper (Fig. 2) (dashed lines are mine) We could argue that we should have chosen the largest value (k =3) instead of the first value that satisfies the Gap statistic equation (k=1 in this case). If you're able to compute for a range of k, then you can choose the maximum. For example, in the graph above, since we're computing over k=1,..,10, we could choose k=3. The original paper although it suggests that we look at the whole range, chooses k=1 in the case above; if you see the raw data plotted (Fig. 2 in the paper) you will also notice that there is really not much structure for cluster subdivision but we should always investigate the whole plot. Also, it's very computationally intensive to compute the Gap statistic. Additionally, you can use domain knowledge or whatever information you have about the data to choose k. The gap statistic is implemented by Miles Granger in the gap_statistic ( https://github.com/milesgranger/gap_statistic ) Python library. The library also implements the Gap$&#94;*$ statistic described in \"A comparison of Gap statistic definitions with and with-out logarithm function( https://core.ac.uk/download/pdf/12172514.pdf )\" (Mohajer, M., Englmeier, K. H., & Schmid, V. J., 2011) which is less conservative but tends to perform suboptimally when clusters overlap. Generating synthetic data again In [25]: plt . title ( \"Four blobs\" , fontsize = 'small' ) X1 , Y1 = make_blobs ( n_features = 2 , centers = 4 ) # centers is number of classes plt . scatter ( X1 [:, 0 ], X1 [:, 1 ], marker = 'o' , c = Y1 , s = 25 , edgecolor = 'k' ); blob_df = pd . DataFrame ( X1 , Y1 ) plt . axis ( 'equal' ); blob_df . head () Out[25]: 0 1 1 -0.780056 6.247700 3 4.942465 -2.969769 2 3.552698 -7.304807 1 -1.574659 6.366932 2 7.879188 -7.814325 In [26]: gs_obj = OptimalK () n_clusters = gs_obj ( X1 , n_refs = 500 , cluster_array = np . arange ( 1 , 15 )) print ( 'Optimal number of clusters: ' , n_clusters ) Optimal number of clusters: 4 In [27]: ms_kmeans = KMeans ( n_clusters = n_clusters , init = 'random' , \\ n_init = 3 , random_state = 109 ) . fit ( X1 ) plt . figure ( figsize = ( 5 , 5 )) plt . scatter ( X1 [:, 0 ], X1 [:, 1 ], marker = 'o' , c = Y1 , s = 25 , edgecolor = 'k' ); plt . scatter ( ms_kmeans . cluster_centers_ [:, 0 ], \\ ms_kmeans . cluster_centers_ [:, 1 ], c = 'r' , marker = 'h' , s = 200 ); plt . axis ( 'equal' ); Out[27]: (-3.3025435783231623, 10.473280979534094, -10.94762035999889, 8.835835742038647) In [28]: def display_gapstat_with_errbars ( gap_df ): gaps = gap_df [ \"gap_value\" ] . values diffs = gap_df [ \"diff\" ] err_bars = np . zeros ( len ( gap_df )) err_bars [ 1 :] = diffs [: - 1 ] - gaps [: - 1 ] + gaps [ 1 :] plt . scatter ( gap_df [ \"n_clusters\" ], gap_df [ \"gap_value\" ]) plt . errorbar ( gap_df [ \"n_clusters\" ], gap_df [ \"gap_value\" ], yerr = err_bars , capsize = 6 ) plt . xlabel ( \"Number of Clusters\" ) plt . ylabel ( \"Gap Statistic\" ) plt . show () display_gapstat_with_errbars ( gs_obj . gap_df ) For more information about the gap_stat package, please see the full documentation here . 3b. Agglomerative Clustering Code (via scipy ): There are many different cluster-merging criteria, one of which is Ward's criteria. Ward's optimizes having the lowest total within-cluster distances, so it merges the two clusters that will harm this objective least. scipy 's agglomerative clustering function implements Ward's method. In [29]: USArrests = pd . read_csv ( 'data/USArrests2.csv' ) In [30]: df = USArrests [[ 'SexualAssault' , 'Assault' , 'UrbanPop' , 'Homicide' ]] arrests_scaled = pd . DataFrame ( preprocessing . scale ( df ), index = USArrests [ 'State' ], columns = df . columns ) Understanding the notion of a vector (data) matrix and a distance or dissimilarity matrix In [31]: # data matrix df . iloc [[ 3 , 15 , 40 ]] Out[31]: SexualAssault Assault UrbanPop Homicide 3 19.5 190 50 8.8 15 18.0 115 66 6.0 40 12.8 86 45 3.8 In [32]: # distance matrix from sklearn.metrics import pairwise_distances pairwise_distances ( df . iloc [[ 3 , 15 , 40 ]], metric = 'euclidean' ) Out[32]: array([[ 0. , 76.75343641, 104.45520571], [ 76.75343641, 0. , 36.24748267], [104.45520571, 36.24748267, 0. ]]) In [33]: # import scipy.cluster.hierarchy as hac plt . figure ( figsize = ( 11 , 8.5 )) dist_mat = pdist ( arrests_scaled , metric = \"euclidean\" ) ward_data = hac . ward ( dist_mat ) hac . dendrogram ( ward_data , labels = USArrests [ \"State\" ] . values ); plt . show () Discussion : How do you read a plot like the above? What are valid options for number of clusters, and how can you tell? Are some more valid than others? Lessons: It's expensive: $O(n&#94;3)$ time complexity and $O(n&#94;2)$ space complexity. Many choices for linkage criteria Every node gets clustered (no child left behind) Example of the use of hierarchical clustering as an exploratory tool Clustering our data in a hierarchical way can reveal associations between them. We can exploit those associations later in the model. 3c. DBSCAN Clustering DBSCAN uses an intuitive notion of denseness to define clusters, rather than defining clusters by a central point as in k-means. Code (via sklearn ): DBscan is implemented in sklearn , but there aren't great automated tools for searching for the optimal epsilon parameter. For full documentation, please visit this page In [34]: plt . figure ( figsize = ( 11 , 8.5 )) fitted_dbscan = DBSCAN ( eps = 0.2 ) . fit ( scaled_df ) plt . scatter ( scaled_df [ 'x' ], scaled_df [ 'y' ], c = fitted_dbscan . labels_ ); Out[34]: Note: the dark purple dots are not clustered with anything else. They are lone singletons. You can validate such by setting epsilon to a very small value, and increase the min_samples to a high value. Under these conditions, nothing would cluster, and yet all dots become dark purple. Exercise : Experiment with the above code by changing its epsilon value and the min_samples (what is the default value for it, since the above code doesn't specify a value?) Lessons: Can cluster non-linear relationships very well; potential for more natural, arbritrarily shaped groupings Does not require specifying the # of clusters (i.e., k ); the algorithm determines such Robust to outliers Doesn't guarantee that every (or ANY) item will be clustered 5. Extra: Clustering for images Breakroom Exercise : Let's load the MNIST digits and try to use $k$-means to cluster them. In [35]: digits = load_digits () print ( digits . data . shape , digits . images . shape , digits . target . shape ) (1797, 64) (1797, 8, 8) (1797,) In [36]: plt . gray () plt . figure ( figsize = ( 8 , 5 )) for i in range ( 12 ): plt . subplot ( 4 , 3 , i + 1 ); plt . imshow ( digits . images [ i ]); In [37]: scaler = StandardScaler () scaler . fit ( digits . data ) digits_scaled = scaler . transform ( digits . data ) In [38]: digits_scaled . shape Out[38]: (1797, 64) In [39]: n_digits = 10 kmeans = KMeans ( init = \"random\" , n_clusters = n_digits , random_state = 22 ) kmeans . fit ( digits_scaled ) Out[39]: KMeans(init='random', n_clusters=10, random_state=22) In [40]: cluster_assignments = zip ( digits_scaled , kmeans . labels_ ) In [41]: # inititalize cluster dict clusters = {} for i in range ( n_digits ): clusters [ i ] = [] clusters Out[41]: {0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: []} In [42]: for idx , j in enumerate ( kmeans . labels_ ): clusters [ j ] . append ( digits . target [ idx ]) Check out the cluster quality manually In [43]: for num in range ( 10 ): df = pd . Series ( clusters [ num ]) print ( f 'Cluster = { num } ' ) print ( df . value_counts ()) Cluster = 0 0 176 6 1 dtype: int64 Cluster = 1 2 113 8 11 3 7 7 5 1 1 dtype: int64 Cluster = 2 1 59 9 19 8 12 4 6 2 5 7 3 6 1 dtype: int64 Cluster = 3 2 43 1 27 3 1 dtype: int64 Cluster = 4 3 158 9 144 8 50 5 40 2 3 dtype: int64 Cluster = 5 7 18 4 8 9 3 dtype: int64 Cluster = 6 4 158 5 2 7 1 0 1 dtype: int64 Cluster = 7 6 178 8 4 5 2 0 1 dtype: int64 Cluster = 8 7 152 1 94 8 94 2 13 3 13 9 11 4 8 5 4 6 1 dtype: int64 Cluster = 9 5 134 3 4 9 3 8 3 4 1 1 1 dtype: int64 References: [1] A Tutorial on Principal Component Analysis","tags":"labs","url":"labs/lab01/notebook-clustering/"},{"title":"Lab 1: Intro to the environment","text":"CS109B Data Science 2: Advanced Topics in Data Science Lab 01 - Coding Environment Setup Harvard University Spring 2022 Instructors: Pavlos Protopapas and Mark Glickman Lab Instructor: Eleni Kaxiras In [2]: import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Learning Goals The purpose of this lab is to help you set up the coding environment for CS109B 1. Getting class material Option 1A: Download directly from Ed Use the >> to download. Option 1B: Cloning the class repo and then copying the contents in a different directory so you can make changes. You may access the code used in class by cloning the class repo: https://github.com/Harvard-IACS/2022-CS109B Open the Terminal in your computer and go to the Directory where you want to clone the repo. Then run git clone https://github.com/Harvard-IACS/2022-CS109B.git If you have already cloned the repo, OR if new material is added (happens every day), go inside the '/2022-CS109B/' directory and run git pull Caution: If you change the notebooks and then run git pull your changes will be overwritten. So create a playground folder and copy the folder with the notebook with which you want to work. 2. Running code: Option 2A: Using your local environment Use Virtual Environments: we cannot stress this enough! Isolating your projects inside specific environments helps you manage dependencies and therefore keep your sanity. You can recover from mess-ups by simply deleting an environment. Sometimes certain installation of libraries conflict with one another. The two most popular tools for setting up environments are: conda (a package and environment manager) pip (a Python package manager) with virtualenv (a tool for creating environments) We recommend using conda package installation and environments. conda installs packages from the Anaconda Repository and Anaconda Cloud, whereas pip installs packages from PyPI. Even if you are using conda as your primary package installer and are inside a conda environment, you can still use pip install for those rare packages that are not included in the conda ecosystem. See here for more details on how to manage Conda Environments . Use the cs109b.yml file to create an environment: $ conda env create -f cs109b.yml $ conda activate cs109b We have included the packages that you will need in the cs109b.yml file. Option 2B: Using Cloud Resources Using FAS OnDemand (supported by CS109b) FAS provides a platform, accessible via the FAS OnDemand menu link in Canvas . Most of the libraries such as keras, tensorflow, pandas, etc., are pre-installed. If a library is missing you may install it via the Terminal. NOTE : The AWS platform is funded by FAS for the purposes of the class. You are not allowed to use it for purposes not related to this course. Make sure you stop your instance as soon as you do not need it. Information on how to use the platform is displayed when you click the link. For more see Fas OnDemand Guide . Using Google Colab (on your own) Google's Colab platform https://colab.research.google.com/ offers a GPU enviromnent to test your ideas, it's fast, free, with the only caveat that your files persist only for 12 hours (last time we checked). The solution is to keep your files in a repository and just clone it each time you use Colab. Using AWS in the Cloud (on your own) For those of you who want to have your own machines in the Cloud to run whatever you want, Amazon Web Services is a (paid) solution. For more see: https://docs.aws.amazon.com/polly/latest/dg/setting-up.html Remember, AWS is a paid service, so if you let your machine run for days you will get charged! 3. Ensuring everything is installed correctly Some of the packages we will need for this class Clustering : Sklearn - https://scikit-learn.org/stable/ scipy - https://www.scipy.org gap_statistic (by Miles Granger) - https://anaconda.org/milesgranger/gap-statistic/notebook Bayes : pymc3 - https://docs.pymc.io Neural Networks : keras - https://www.tensorflow.org/guide/keras Exercise 1: Run the following cells to make sure these packages load correctly in our environment. In [5]: from sklearn import datasets iris = datasets . load_iris () digits = datasets . load_digits () digits . target # you should see [0, 1, 2, ..., 8, 9, 8] Out[5]: array([0, 1, 2, ..., 8, 9, 8]) In [18]: from scipy import misc import matplotlib.pyplot as plt face = misc . face () face . shape , type ( face ) Out[18]: ((768, 1024, 3), numpy.ndarray) In [20]: face [ 1 : 3 , 1 : 3 ] Out[20]: array([[[110, 103, 121], [130, 122, 143]], [[ 94, 87, 105], [115, 108, 126]]], dtype=uint8) In [6]: plt . imshow ( face ) plt . show () # you should see a racoon In [10]: import pymc3 as pm print ( 'Running PyMC3 v {} ' . format ( pm . __version__ )) # you should see 'Running on PyMC3 v3.8' Running PyMC3 v3.8 In [11]: # making sure you have gap_statistic from gap_statistic import OptimalK 4. Plotting matplotlib and seaborn matplotlib seaborn: statistical data visualization . seaborn works great with pandas . It can also be customized easily. Here is the basic seaborn tutorial: Seaborn tutorial . Plotting a function of 2 variables using contours In optimization, our objective function will often be a function of two or more variables. While it's hard to visualize a function of more than 3 variables, it's very informative to plot one of 2 variables. To do this we use contours. First we define the $x$ and $y$ variables and then construct their pairs using meshgrid . Plot the function $f(x,y) = x&#94;2+y&#94;2$ In [12]: import seaborn as sn In [30]: x = np . linspace ( - 0.1 , 0.1 , 50 ) y = np . linspace ( - 0.1 , 0.1 , 100 ) xx , yy = np . meshgrid ( x , y ) z = np . sqrt ( xx ** 2 + yy ** 2 ) plt . contour ( x , y , z ); 5. We will be using keras via tensorflow TensorFlow is a framework for representing complicated ML algorithms and executing them in any platform, from a phone to a distributed system using GPUs. Developed by Google Brain, TensorFlow is used very broadly today. Keras , is a high-level API, created by François Chollet, and used for fast prototyping, advanced research, and production. tf.keras is now maintained by Tensorflow. Exercise 2: Run the following cells to make sure you have the basic libraries to do deep learning In [14]: from __future__ import absolute_import , division , print_function , unicode_literals # TensorFlow and tf.keras import tensorflow as tf from tensorflow.keras import layers from tensorflow.keras import models from tensorflow.keras.layers import Dense from tensorflow.keras.models import Sequential from tensorflow.keras.regularizers import l2 tf . keras . backend . clear_session () # For easy reset of notebook state. # You should see a >=2.3.0 here! # If you do not, upgrade your env to tensorflow==2.3.0 print ( tf . __version__ ) print ( tf . keras . __version__ ) 2.3.0 2.4.0 In [15]: # Check if your machine has NVIDIA GPUs. hasGPU = tf . config . list_physical_devices () print ( f 'My computer has the following devices: { hasGPU } ' ) My computer has the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')]","tags":"labs","url":"labs/lab01/notebook-intro/"},{"title":"CS109b: Advanced Topics in Data Science","text":"Spring 2022 Pavlos Protopapas & Mark Glickman Advanced Topics in Data Science (CS109b) is the second half of a one-year introduction to data science. Building upon the material in Introduction to Data Science, the course introduces advanced methods for data wrangling, data visualization, statistical modeling, and prediction. Topics include big data, multiple deep learning architectures such as CNNs, RNNs, transformers, language models, autoencoders, and generative models as well as basic Bayesian methods, and unsupervised learning. Helpline: cs109b2022@gmail.com Lectures: Mon & Wed 9:45‐11 am Labs: Fri 9:45-11 am Advanced Sections: Weds 2:15-3:30 pm (see schedule for specific dates) Office Hours: See Ed Post Course material can be viewed in the public GitHub repository . Previous Material 2021 2020 2019 2018","tags":"pages","url":"pages/cs109b-advanced-topics-in-data-science/"}]}