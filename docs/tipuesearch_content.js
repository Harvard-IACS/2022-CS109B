var tipuesearch = {"pages":[{"title":"FAQ","text":"General I'm unable to attend lectures in person. Can I take the class asynchronously? Lecture attendance is required. Non-DCE students should only register if they can attend in person. Does the individual HW mean I have to submit on my own but can I still work with a HW partner? An individual CS109B HW means you are supposed to work on your own, without any human intervention, assistance, or input. You are not allowed to work with a partner. You are allowed to use OHs to ask clarification questions, but we may not be able to answer all of your questions or help with your coding. You are allowed to use all of your materials from the course up to the HW, look at problem resolution online, and look at documentation. Where should I send my questions? Use Ed for anying related to the course content or assignments. All other concerns should be sent to the course helpline: cs109b2022@gmail.com Auditing Can I audit this course? What are the rules for auditing? Yes, CS109B does accept auditors, but all auditors must agree to abide by the following rules: Auditors must attend class in person. This is a Harvard policy. Auditors who do not confirm their presence with the head TF during the first week of in-class instruction will lose course access. Auditors are held to the same standard of academic honesty as our registered students. Please do not share homeworks or solutions with anyone. Violations will be reported to the Harvard Administrative Board. Auditors are not permitted to take the course for credit in the future. Audiors should not submit HWs or participate in projects. Auditors should refrain from using any course and TF resources that are designed for our registered students like Ed, FASOnDemand, JupyterHub, and office hours. Auditors must have an active HUID number. If you agree to the above, send an email to cs109a2022@gmail.com with your HUID and state that you've read the auditing policy and that you agree to abide by these rules. Quizzes & Exercises When are quizzes and exercises due? All 'pre-class reading check' quizzes and exercises presented in class are ungraded and so have no due date, though solutions will be released within a few days. All graded quizzes and exercises on Ed will be 'bundled' with a given HW assignment and so will share that HW's due date. Each HW assignment will make clear which Ed quizzes and exercises are to be completed as part of the assignment.","tags":"pages","url":"pages/faq.html"},{"title":"Schedule","text":"Date (Mon) Lecture (Mon) Lecture (Wed) Lab (Fri) Advanced Section (Wed) Assignment (R:Released Wed - D:Due Wed) 24-Jan Lecture 1: Clustering 1 Lecture 2: Clustering 2 Lab 1 R:HW1 31-Jan Lecture 3: Bayes 1 Lecture 4: Bayes 2 Advanced Section 1: Gaussian Mixture Models 7-Feb Lecture 5: Bayes 3 Lecture 6: Bayes 4 Lab 2 (+Lab 3 recording) R:HW2 - D:HW1 14-Feb Lecture 7: Bayes 5 Lecture 8: Neural Networks 1 Lab 4 Advanced Section 2: Particle Filters/Sequential Monte Carlo 21-Feb No Lectture (Holiday) Lecture 9: Neural Networks 2 Lab 5 Advanced Section 3: Universal Approximator R:HW3 - D:HW2 28-Feb Lecture 10: Neural Networks 3) Lecture 11: CNNs 1 Lab 6 Advanced Section 4: Solvers 7-Mar Lecture 12:CNNs 2 Lecture 13: CNNs 3 Lab 7 Advanced Section 5: Segmentation R:HW4 - D:HW3 14-Mar No Lecture (Spring Break) No Lecture No Lab 21-Mar Lecture 14: CNNs 4 Lecture 15: AUtoencoders Lab 8 Review Session 28-Mar Lecture 16: RNNs 1 Lecture 17: RNNs 2 Lab 9 Advanced Section 6: Variational Autoencoders R:HW5(Individual) - D:HW4 4-Apr Lecture 18: NLP 1 Lecture 19: NLP 2 Lab 10 Advanced Section 7: Word2Vec R:HW6(Individual) - D:HW5(Individual) 11-Apr Lecture 20: NLP 3 Lecture 21: NLP 4 Lab 11 Advanced Section 8: BERT 18-Apr Lecture 22: GANs 1 Lecture 23: GANs 2 Lab 12 Advanced Section 9: More GANs! D:HW6(Individual) - R:HW7 25-Apr Module: Lecture Domain Module: Problem Background Project Work D:HW7 2-May Project Work Project Work Project Submission Due 9-May Peer Evaluations Due Final Project Showcase","tags":"pages","url":"pages/schedule.html"},{"title":"Syllabus","text":"Draft Syllabus Subject to Change Advanced Topics in Data Science (Spring 2022) CS 109b, AC 209b, Stat 121b, or CSCI E-109b Instructors Pavlos Protopapas (SEAS) & Mark Glickman (Statistics) Lectures: Mon & Wed 9:45‐11am SEC 1.321 Labs: Fri 9:45-11am SEC 1.321 Advanced Sections: Wed 2:15-3:30pm SEC 1.321 (starting 2/2; see schedule for specific dates) Office Hours: TBA Prerequisites: CS 109a, AC 209a, Stat 121a, or CSCI E-109a or the equivalent. Course description Tentative Course Topics Course Objectives Course Components Lectures Labs Advanced Sections Midterm Projects Homework Assignments Course Resources Online Materials Recommended Textbooks & Articles Getting Help Course Policies and Expectations Grading Collaboration Policy Late or Incorrectly Submitted Assignments Re-grade Requests Auditing the Class Academic Integrity Accommodations for students with disabilities Diversity and Inclusion Statement Course Description Advanced Topics in Data Science (CS109b) is the second half of a one-year introduction to data science. Building upon the material in Introduction to Data Science, the course introduces advanced methods for data wrangling, data visualization, statistical modeling, and prediction. Topics include big data, multiple deep learning architectures such as CNNs, RNNs, language models, transformers, autoencoders, and generative models as well as Bayesian modeling, sampling methods, and unsupervised learning. The programming language will be Python. Tentative Course Topics Unsupervised Learning, Clustering Bayesian Inference Hierarchical Bayesian Modeling Fully Connected Neural Networks Convolutional Neural Networks Autoencoders Recurrent Neural Networks NLP / Text Analysis Transformers Variational AutoEncoders & Generative Models Generative Adversarial Networks Course Objectives Upon successful completion of this course, you should feel comfortable with the material mentioned above, and you will have gained experience working with others on real-world problems. The content knowledge, the project, and teamwork will prepare you for the professional world or further studies. Course Components Lectures, labs, and advanced sections will be live-streamed for Extension School students and can be accessed through the Zoom section on Canvas. Video recordings of the live stream will be made available to all students within 24 hours after the event, and will be accessible from the Lecture Video section on Canvas. Lectures The class meets for lectures twice a week (M & W). Attending and participating in lectures is a crucial component of learning the material presented in this course. Students may be asked to complete short readings before certain lectures. Some lectures will also include real-time coding exercises which we will complete as a class. Labs Lab will be held every Friday. Labs will present guided, hands-on coding challenges to prepare students for successfully completing the homework assignments. Advanced Sections The course will include advanced sections for 209b students and will cover a different topic per week. These 75 min sessions will cover advanced topics like the mathematical underpinnings of the methods seen in the main course lectures and lab as well as extensions of those methods. The material covered in the advanced sections is required for all AC209b students. Tentative topics are: Gaussian Mixture Models Particle Filters/Sequential Monte Carlo NN as Universal Approximator Solvers Segmentation Techniques, YOLO, Unet and M-RCNN Variational Autoencoders Word2Vec BERT GANS, Cycle GANS, etc. Note: Advanced Section are not held every week. Consult the course calendar for exact dates. Midterm There will be a midterm exam on Friday, March 25th from 9:45-11am (regular lab time). The exam will likely consist of multiple choice questions with a take-home coding component. More information to follow. Projects Beginning the last week of classes (4/25), students will join groups of 3-4 and be divided into break-out, thematic sections to study an open problem in one of various domains. The domains are tentative at the moment but may include medicine, law, astronomy, e-commerce, government, and areas in the humanities. Each section will include lectures by Harvard faculty who are experts in the field. Project work will continue on through reading period and conclude with final submissions on 5/6. The final submission will consist of a written report, a Jupyter notebook with all relevant code, and a 6-minute, pre-recorded presentation video. Homework Assignments There will be 7 graded homework assignments. Some of them will be due one week after being assigned and some will be due two weeks after being assigned. For 5 assignments, you have the option to work and submit in pairs, the 2 remaining are to be completed individually. Standard assignments are graded out of 5 points. AC209b students will have additional homework content for most assignments worth 1 point. Course Resources Online Materials All course materials, including lecture notes, lab notes, and section notes will be published on Ed, the course GitHub repo, and the public site's 'Materials' section. Note: Lecture content for lectures 1-7 will only be accessible to registered students. Assignments will only be posted on Canvas. Working Environment You will be working in Jupyter Notebooks which you can run in your own machine or in the SEAS JupyterHub. Recommended Textbooks ISLR: An Introduction to Statistical Learning by James, Witten, Hastie, Tibshirani (Springer: New York, 2013) BDA3: Bayesian Data Analysis by Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, Donald B. Rubin (CRC Press: New York, 2013) DL: Deep Learning by Goodfellow, Bengio and Courville. (The MIT Press: Cambridge, 2016) Glassner: Deep Learning, Vol. 1 & 2 by Andrew Glassner SLP Speech and Language Processing by Jurafsky and Martin (3rd Edition Draft) INLP Introduction to Natural Language Processing by Jacob Eisenstein (The MIT Press: Cambridge, 2019) Free electronic versions are available ( ISLR , DL , SLP , INLP ) or hard copy through Amazon ( ISLR , DL , Glassner , SLP , INLP ). Articles & Excerpts Unsupervised learning: Basics: James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An introduction to statistical learning (2nd ed.). New York: Springer. Chapter 12 https://hastie.su.domains/ISLR2/ISLRv2_website.pdf Silhouette plots: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html Gap statistic: Tibshirani, R., Walther, G., & Hastie, T. (2001). Estimating the number of clusters in a data set via the gap statistic. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), 411-423. https://hastie.su.domains/Papers/gap.pdf DBSCAN: Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017). DBSCAN revisited, revisited: why and how you should (still) use DBSCAN. ACM Transactions on Database Systems (TODS), 42(3), 1-21. https://dl.acm.org/doi/pdf/10.1145/3068335?casa_token=_P479lYnlpsAAAAA:PckzU6ZiTt3yMNzFrXyzESZ3N_pp904kN0N2QEwIoq6CxtfPCxnL9bNTGtjhuiNtzSfKyXoM-QI Bayesian material Basics: Glickman, Mark E. and Van Dyk, David A. (2007) \"Basic Bayesian Methods\" In Topics in Biostatistics (Methods in Molecular Biology). Edited by Walter Ambrosius. The Humana Press Inc., Totowa, NJ. ISBN 1-58829-531-1. pp 319-338. Chapter accessible from http://www.glicko.net/research/glickman-vandyk.pdf Importance sampling, rejection sampling, MCMC, Metropolis, Gibbs sampler: Andrieu, C., De Freitas, N., Doucet, A., & Jordan, M. I. (2003). An introduction to MCMC for machine learning. Machine learning, 50(1), 5-43. Article accessible from: https://www.cs.ubc.ca/~arnaud/andrieu_defreitas_doucet_jordan_intromontecarlomachinelearning.pdf Bayesian examples - regression, hierarchical modeling: Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A., & Rubin, D.B. (2013). Bayesian Data Analysis (3rd ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b16018 http://www.stat.columbia.edu/~gelman/book/BDA3.pdf Chapter 14: Introduction to regression models Chapter 15: Hierarchical linear models Chapter 16: Generalized linear models (includes logistic regression) Getting Help For questions about homework, course content, package installation, the process is: try to troubleshoot yourself by reading the lecture, lab, and section notes, and looking up online resources. go to office hours this is the best way to get help. post on the class Ed forum; we want you and your peers to engage in helping each other. TFs also monitor Ed and will respond within 24 hours. Note that Ed questions are visible to everyone. If you are citing homework solution code you must post privately so that only the staff sees your message. watch for official announcements via Ed. These announcements will also be sent to the email address associated with your Canvas account so make sure you have it set appropriately. send an email to the Helpline cs109b2022@gmail.com for administrative issues, regrade requests, and non-content specific questions. for personal matters that you do not feel comfortable sharing with the TFs, you may send an email to either or both of the instructors. Course Policies and Expectations Grading for CS109b, STAT121b, and CS209b: Your final score for the course will be computed using the following weights: Assignment Final Grade Weight Paired Homework (5) 45% Individual Homework (2) 23% Midterm 12% Project 20% Total 100% Note: Regular homework (for everyone) counts as 5 points. 209b extra homework counts as 1 point. Collaboration Policy We expect you to adhere to the Harvard Honor Code at all times. Failure to adhere to the honor code and our policies may result in serious penalties, up to and including automatic failure in the course and reference to the ad board. If you work with a partner on an assignment make sure both parties solve all the problems. Do not divide and conquer. You are expected to be intellectually honest and give credit where credit is due. In particular: if you work with a fellow student but decide to submit different papers, include the name of each other in the designated area of the submission paper. if you work with a fellow student and want to submit the same paper you need to form a group prior to the submission. Details in the assignment. Not all assignments will permit group submissions. you need to write your solutions entirely on your own or with your collaborator you are welcome to take ideas from code presented in labs, lecture, or sections but you need to change it, adapt it to your style, and ultimately write your own. We do not want to see code copied verbatim from the above sources. if you use code found on the internet, books, or other sources you need to cite those sources. you should not view any written materials or code created by other students for the same assignment; you may not provide or make available solutions to individuals who take or may take this course in the future. if the assignment allows it you may use third-party libraries and example code, so long as the material is available to all students in the class and you give proper attribution. Do not remove any original copyright notices and headers. Late or Incorrectly Submitted Assignments Each student is allowed up to 3 late days over the semester with at most 1 day applied to any single homework . Outside of these allotted late days, late homework will not be accepted unless there is a medical (if accompanied by a doctor's note) or other official University-excused reasons. There is no need to ask before using one of your late days. If you forgot to join a Group with your peer and are asking for the same grade we will accept this with no penalty up to HW3. For homeworks beyond that we feel that you should be familiar with the process of joining groups. After that there will be a penalty of -1 point for both members of the group provided the submission was on time. Grading Guidelines Homework will be graded based on: How correct your code is (the Notebook cells should run, we are not troubleshooting code) How you have interpreted the results — we want text not just code. It should be a report. How well you present the results. The scale is 0 to 5 for each assignment and 0 to 1 for the additional 209 assignments. Re-grade Requests Our graders and instructors make every effort in grading accurately and in giving you a lot of feedback. If you discover that your answer to a homework problem was correct but it was marked as incorrect, send an email to the Helpline with a description of the error. Please do not submit regrade requests based on what you perceive is overly harsh grading . The points we take off are based on a grading rubric that is being applied uniformly to all assignments. If you decide to send a regrade request , send an email to the Helpline with subject line \"Regrade HW1: Grader=johnsmith\" replacing 'HW1' with the current assignment and 'johnsmith' with the name of the grader within 48 hours of the grade release. Auditing the Class You are welcome to audit this course. To request access, send an email to cs109b2022@gmail.com with you HUID (required) and a statement of agreement to the terms below. All auditors must agree to abide by the following rules: Auditors must attend class in person. This is a Harvard policy. Auditors who do not confirm their presence with the head TF during the first week of in-class instruction will lose course access. Auditors are held to the same standard of academic honesty as our registered students. Please do not share homeworks or solutions with anyone. Violations will be reported to the Harvard Administrative Board. Auditors are not permitted to take the course for credit in the future. Audiors should not submit HWs or participate in projects. Auditors should refrain from using any course and TF resources that are designed for our registered students like Ed, FASOnDemand, JupyterHub, and office hours. Academic Integrity Ethical behavior is an important trait of a Data Scientist, from ethically handling data to attribution of code and work of others. Thus, in CS109b we give a strong emphasis to Academic Honesty. As a student your best guidelines are to be reasonable and fair. We encourage teamwork for problem sets, but you should not split the homework and you should work on all the problems together. For more detailed expectations, please refer to the Collaborations section above. You are responsible for understanding Harvard Extension School policies on academic integrity https://www.extension.harvard.edu/resources-policies/student-conduct/academic-integrity and how to use sources responsibly. Stated most broadly, academic integrity means that all course work submitted, whether a draft or a final version of a paper, project, take-home exam, online exam, computer program, oral presentation, or lab report, must be your own words and ideas, or the sources must be clearly acknowledged. The potential outcomes for violations of academic integrity are serious and ordinarily include all of the following: required withdrawal (RQ), which means a failing grade in the course (with no refund), the suspension of registration privileges, and a notation on your transcript. Using sources responsibly https://www.extension.harvard.edu/resources-policies/resources/avoiding-plagiarism is an essential part of your Harvard education. We provide additional information about our expectations regarding academic integrity on our website. We invite you to review that information and to check your understanding of academic citation rules by completing two free online 15-minute tutorials that are also available on our site. (The tutorials are anonymous open-learning tools.) Accommodations for students with disabilities Harvard students needing academic adjustments or accommodations because of a documented disability must present their Faculty Letter from the Accessible Education Office (AEO) and speak with the professor by the end of the second week of the term, (fill in specific date). Failure to do so may result in the Course Head's inability to respond in a timely manner. All discussions will remain confidential, although Faculty are invited to contact AEO to discuss appropriate implementation. Harvard Extension School is committed to providing an inclusive, accessible academic community for students with disabilities and chronic health conditions. The Accessibility Services Office (ASO) https://www.extension.harvard.edu/resources-policies/accessibility-services-office-aso offers accommodations and supports to students with documented disabilities. If you have a need for accommodations or adjustments in your course, please contact the Accessibility Services Office by email at accessibility@extension.harvard.edu or by phone at 617-998-9640. Diversity and Inclusion Statement Data Science and Computer Science have historically been representative of only a small sliver of the population. This is despite the contributions of a diverse group of early pioneers - see Ada Lovelace, Dorothy Vaughan, and Grace Hopper for just a few examples. As educators, we aim to build a diverse, inclusive, and representative community offering opportunities in data science to those who have been historically marginalized. We will encourage learning that advances ethical data science, exposes bias in the way data science is used, and advances research into fair and responsible data science. We need your help to create a learning environment that supports a diversity of thoughts, perspectives, and experiences, and honors your identities (including but not limited to race, gender, class, sexuality, religion, ability, etc.) To help accomplish this: If you have a name and/or set of pronouns that differ from those in your official Harvard records, please let us know! If you feel like your performance in the class is being impacted by your experiences outside of class, please do not hesitate to come and talk with us. We want to be a resource for you. Remember that you can also submit anonymous feedback (which will lead to us making a general announcement to the class, if necessary, to address your concerns). If you prefer to speak with someone outside of the course, you may find helpful resources at the Harvard Office of Diversity and Inclusion. We (like many people) are still learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to us about it. As a participant in course discussions, you are expected to respect your classmates' diverse backgrounds and perspectives. Our course will discuss diversity, inclusion, and ethics in data science. Please contact us (in person or electronically) or submit anonymous feedback if you have any suggestions for how we can improve.","tags":"pages","url":"pages/syllabus.html"},{"title":"Lab 1: Intro to the environment","text":"CS109B Data Science 2: Advanced Topics in Data Science Lab 01 - Coding Environment Setup Harvard University Spring 2022 Instructors: Pavlos Protopapas and Mark Glickman Lab Instructor: Eleni Kaxiras In [2]: import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Learning Goals The purpose of this lab is to help you set up the coding environment for CS109B 1. Getting class material Option 1A: Download directly from Ed Use the >> to download. Option 1B: Cloning the class repo and then copying the contents in a different directory so you can make changes. You may access the code used in class by cloning the class repo: https://github.com/Harvard-IACS/2022-CS109B Open the Terminal in your computer and go to the Directory where you want to clone the repo. Then run git clone https://github.com/Harvard-IACS/2022-CS109B.git If you have already cloned the repo, OR if new material is added (happens every day), go inside the '/2022-CS109B/' directory and run git pull Caution: If you change the notebooks and then run git pull your changes will be overwritten. So create a playground folder and copy the folder with the notebook with which you want to work. 2. Running code: Option 2A: Using your local environment Use Virtual Environments: we cannot stress this enough! Isolating your projects inside specific environments helps you manage dependencies and therefore keep your sanity. You can recover from mess-ups by simply deleting an environment. Sometimes certain installation of libraries conflict with one another. The two most popular tools for setting up environments are: conda (a package and environment manager) pip (a Python package manager) with virtualenv (a tool for creating environments) We recommend using conda package installation and environments. conda installs packages from the Anaconda Repository and Anaconda Cloud, whereas pip installs packages from PyPI. Even if you are using conda as your primary package installer and are inside a conda environment, you can still use pip install for those rare packages that are not included in the conda ecosystem. See here for more details on how to manage Conda Environments . Use the cs109b.yml file to create an environment: $ conda env create -f cs109b.yml $ conda activate cs109b We have included the packages that you will need in the cs109b.yml file. Option 2B: Using Cloud Resources Using FAS OnDemand (supported by CS109b) FAS provides a platform, accessible via the FAS OnDemand menu link in Canvas . Most of the libraries such as keras, tensorflow, pandas, etc., are pre-installed. If a library is missing you may install it via the Terminal. NOTE : The AWS platform is funded by FAS for the purposes of the class. You are not allowed to use it for purposes not related to this course. Make sure you stop your instance as soon as you do not need it. Information on how to use the platform is displayed when you click the link. For more see Fas OnDemand Guide . Using Google Colab (on your own) Google's Colab platform https://colab.research.google.com/ offers a GPU enviromnent to test your ideas, it's fast, free, with the only caveat that your files persist only for 12 hours (last time we checked). The solution is to keep your files in a repository and just clone it each time you use Colab. Using AWS in the Cloud (on your own) For those of you who want to have your own machines in the Cloud to run whatever you want, Amazon Web Services is a (paid) solution. For more see: https://docs.aws.amazon.com/polly/latest/dg/setting-up.html Remember, AWS is a paid service, so if you let your machine run for days you will get charged! 3. Ensuring everything is installed correctly Some of the packages we will need for this class Clustering : Sklearn - https://scikit-learn.org/stable/ scipy - https://www.scipy.org gap_statistic (by Miles Granger) - https://anaconda.org/milesgranger/gap-statistic/notebook Bayes : pymc3 - https://docs.pymc.io Neural Networks : keras - https://www.tensorflow.org/guide/keras Exercise 1: Run the following cells to make sure these packages load correctly in our environment. In [5]: from sklearn import datasets iris = datasets . load_iris () digits = datasets . load_digits () digits . target # you should see [0, 1, 2, ..., 8, 9, 8] Out[5]: array([0, 1, 2, ..., 8, 9, 8]) In [18]: from scipy import misc import matplotlib.pyplot as plt face = misc . face () face . shape , type ( face ) Out[18]: ((768, 1024, 3), numpy.ndarray) In [20]: face [ 1 : 3 , 1 : 3 ] Out[20]: array([[[110, 103, 121], [130, 122, 143]], [[ 94, 87, 105], [115, 108, 126]]], dtype=uint8) In [6]: plt . imshow ( face ) plt . show () # you should see a racoon In [10]: import pymc3 as pm print ( 'Running PyMC3 v {} ' . format ( pm . __version__ )) # you should see 'Running on PyMC3 v3.8' Running PyMC3 v3.8 In [11]: # making sure you have gap_statistic from gap_statistic import OptimalK 4. Plotting matplotlib and seaborn matplotlib seaborn: statistical data visualization . seaborn works great with pandas . It can also be customized easily. Here is the basic seaborn tutorial: Seaborn tutorial . Plotting a function of 2 variables using contours In optimization, our objective function will often be a function of two or more variables. While it's hard to visualize a function of more than 3 variables, it's very informative to plot one of 2 variables. To do this we use contours. First we define the $x$ and $y$ variables and then construct their pairs using meshgrid . Plot the function $f(x,y) = x&#94;2+y&#94;2$ In [12]: import seaborn as sn In [30]: x = np . linspace ( - 0.1 , 0.1 , 50 ) y = np . linspace ( - 0.1 , 0.1 , 100 ) xx , yy = np . meshgrid ( x , y ) z = np . sqrt ( xx ** 2 + yy ** 2 ) plt . contour ( x , y , z ); 5. We will be using keras via tensorflow TensorFlow is a framework for representing complicated ML algorithms and executing them in any platform, from a phone to a distributed system using GPUs. Developed by Google Brain, TensorFlow is used very broadly today. Keras , is a high-level API, created by François Chollet, and used for fast prototyping, advanced research, and production. tf.keras is now maintained by Tensorflow. Exercise 2: Run the following cells to make sure you have the basic libraries to do deep learning In [14]: from __future__ import absolute_import , division , print_function , unicode_literals # TensorFlow and tf.keras import tensorflow as tf from tensorflow.keras import layers from tensorflow.keras import models from tensorflow.keras.layers import Dense from tensorflow.keras.models import Sequential from tensorflow.keras.regularizers import l2 tf . keras . backend . clear_session () # For easy reset of notebook state. # You should see a >=2.3.0 here! # If you do not, upgrade your env to tensorflow==2.3.0 print ( tf . __version__ ) print ( tf . keras . __version__ ) 2.3.0 2.4.0 In [15]: # Check if your machine has NVIDIA GPUs. hasGPU = tf . config . list_physical_devices () print ( f 'My computer has the following devices: { hasGPU } ' ) My computer has the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')]","tags":"labs","url":"labs/lab-1-intro-to-the-environment/"},{"title":"Lab 1: Clustering","text":"CS109B Data Science 2: Advanced Topics in Data Science Lecture 01 - Clustering with Python Harvard University Spring 2022 Instructors: Mark Glickman and Pavlos Protopapas Lab Instructor: Eleni Kaxiras Content: Eleni Kaxiras, Chris Tanner, and Will Claybaugh In [1]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES # import requests # from IPython.core.display import HTML # styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\").text # HTML(styles) In [2]: import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from sklearn import preprocessing from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans from sklearn.feature_extraction.text import CountVectorizer from scipy.spatial import distance from sklearn.metrics import silhouette_samples , silhouette_score import matplotlib.cm as cm from sklearn.cluster import DBSCAN from sklearn.neighbors import NearestNeighbors from sklearn.preprocessing import StandardScaler from sklearn.datasets import make_blobs import scipy.cluster.hierarchy as hac from scipy.spatial.distance import pdist import scipy.cluster.hierarchy as hac from scipy.spatial.distance import pdist % matplotlib inline In [3]: from gap_statistic import OptimalK In [4]: from sklearn.datasets import make_blobs from sklearn.datasets import make_classification from sklearn.datasets import load_digits import warnings warnings . filterwarnings ( 'ignore' ) Learning Objectives Understand the common distance metrics (e.g., Euclidean, Manhattan, Hamming). Understand the difference between a vector matrix and a distance matrix Understand how different clustering algorithms work (e.g., k-means, Hierarchical, DBSCAN). For home: Review what PCA is and know the differences between PCA and clustering, Table of Contents PCA Refresher Preparing the data Choosing a distance or dissimilarity metric Clustering algorithms and measuring the quality of clusters Extra: Clustering for images Unsupervised Learning, Cluster Analysis, and Classification Review : What is unsupervised learning? What is Cluster Analysis? Is the response variable included in the algorithm? What does it mean to perform classification? 1. PCA Refresher image source: [1] Review What is PCA? How can it be useful? What are its limitations? image source: [1] Sklearn's sklearn.decomposition.PCA uses the LAPACK library written in Fortran 90 (based on the LINPACK library from the 70s) which provides routines for solving systems of simultaneous linear equations, least-squares solutions of linear systems of equations. How to use the sklearn PCA package: a. Instantiate a new PCA object: pca_transformer = PCA() b. Fit some data (learns the transformation based on this data): fitted_pca = pca_transformer.fit(data_frame) c. Transform the data to the reduced dimensions: pca_df = fitted_pca.transform(data_frame) Using two distinct steps (i.e., (b) and (c)) to fit and transform our data allows one the flexibility to transform any dataset according to our learned fit() . Alternatively, if you know you only want to transform a single dataset, you can combine (b) and (c) into one step: Fit and transform: pca_df = pca_transformer.fit_transform(pca_df) Note: We fit on the training set and transform both training and test set. Example: Playing with synthetic data Sklearn has methods for generating synthetic datasets . They can be quite useful for testing clustering for classification purposes. In [5]: n_features = 2 n_classes = 1 plt . title ( f 'Features = { n_features } ' , fontsize = 'medium' ) X1 , Y1 = make_classification ( n_features = n_features , n_redundant = 0 , n_informative = 1 , n_clusters_per_class = 1 , n_classes = n_classes ) colors = [ \"#4EACC5\" , \"#FF9C34\" , \"#4E9A06\" ] plt . scatter ( X1 [:, 0 ], X1 [:, 1 ], marker = 'o' , c = Y1 , s = 25 , edgecolor = 'k' ); class_df = pd . DataFrame ( X1 , Y1 ) . reset_index ( drop = True ) class_df = class_df . rename ( columns = { 0 : 'feature1' , 1 : 'feature2' }) plt . axis ( 'equal' ); Out[5]: (-1.488981045742101, 3.2948130634705572, -2.007589788641174, 2.621466822917984) In [6]: X1 . shape , Y1 . shape Out[6]: ((100, 2), (100,)) In [7]: class_df . head () Out[7]: feature1 feature2 0 1.012265 -0.072951 1 -0.474946 -1.625100 2 0.265944 1.600704 3 1.826242 -1.519311 4 -0.424610 0.802087 In [8]: scaler = StandardScaler () scaler . fit ( class_df ) scaled_df = pd . DataFrame ( scaler . transform ( class_df )) In [9]: pca_transformer = PCA ( n_components = 2 ) fitted_pca = pca_transformer . fit ( class_df ) fitted_pca . explained_variance_ratio_ Out[9]: array([0.591269, 0.408731]) In [10]: pca_df = pd . DataFrame ( fitted_pca . transform ( class_df )) pca_df = pca_df . rename ( columns = { 0 : 'pc1' , 1 : 'pc2' }) pca_df . head () Out[10]: pc1 pc2 0 -0.164327 -0.128186 1 -2.304623 0.072059 2 0.682850 1.496747 3 -0.791398 -1.664837 4 -0.371981 1.541299 In [11]: fitted_pca . explained_variance_ratio_ Out[11]: array([0.591269, 0.408731]) For more read: \" Importance of feature scaling \" Sklearn's StandardScaler 2 - Preparing the data Discussion: To scale or not to scale? Depends For more read: \" Importance of feature scaling \" Sklearn's StandardScaler 3 - Distance and dissimilarity metrics The Euclidean norm (or length) of a vector $\\textbf{v}=[v_1,v_2,..,v_n]&#94;T$ in $\\mathbb{R}&#94;n$ is the nonnegative scalar \\begin{aligned} \\lVert \\textbf{v} \\rVert = \\sqrt{\\textbf{v}\\cdot \\textbf{v}} = \\sqrt{{v_1}&#94;2+{v_2}&#94;2+\\cdots+{v_n}&#94;2} \\end{aligned} The Manhattan norm of the same vector is the nonnegative scalar \\begin{aligned} \\lVert \\textbf{v} \\rVert = \\lvert \\textbf{v} \\rvert = \\lvert v_1 \\rvert + \\lvert v_2 \\rvert + \\cdots + \\lvert v_n \\rvert \\end{aligned} The distance between two vectors $\\textbf{v}$ and $\\textbf{u}$ is defined by $d(\\textbf{v}, \\textbf{u}) = \\lVert \\textbf{v} - \\textbf{u} \\rVert$ Let's practice on the diagram below; we are concerned with measuring the distance between two points, $\\textbf{p}=(p_1,p_2)$ and $\\textbf{q}=(q_1,q_2)$. (edited from Wikipedia.org) Euclidean distance: The Euclidean distance measures the shortest path between the two points, navigating through all dimensions: $d(\\textbf{p}, \\textbf{q}) = \\lVert \\textbf{p} - \\textbf{q} \\rVert = \\sqrt{{(p_1-q_1)}&#94;2+{(p_2-q_2)}&#94;2}$ For vectors in $\\mathbb{R}&#94;n$: $d(\\textbf{p}, \\textbf{q}) = \\lVert \\textbf{p} - \\textbf{q} \\rVert = \\sqrt{{(p_1-q_1)}&#94;2+{(p_2-q_2)}&#94;2+\\cdots +{(p_n-q_n)}&#94;2}$ Manhattan distance: The Manhattan distance measures the cumulative difference between the two points, across all dimensions. $d_1(\\textbf{p}, \\textbf{q}) = \\lVert \\textbf{p} - \\textbf{q} \\rVert_1 = \\sum_{i=1}&#94;{n} \\mid p_i-q_1 \\mid$ Extra: Cosine distance $\\cos{\\theta} = \\frac{\\textbf{q}\\textbf{q}}{\\lVert \\textbf{p}\\rVert \\lVert\\textbf{q} \\rVert} $ In [12]: count_vect = CountVectorizer () sent0 = \"Biden is here\" sent1 = \"President is coming here\" corpus = [ sent0 , sent1 ] sentences = count_vect . fit_transform ( corpus ) v1 = sentences . toarray ()[ 0 ] v2 = sentences . toarray ()[ 1 ] print ( f 'v1 = { v1 } , \\n v2 = { v2 } ' ) # pretty print df = pd . DataFrame ( sentences . toarray (), \\ columns = count_vect . get_feature_names (), index = [ 'Sentence 0' , 'Sentence 1' ]) print ( f 'distance = { distance . cosine ( v1 , v2 ) } ' ) df v1 = [1 0 1 1 0], v2 = [0 1 1 1 1] distance = 0.42264973081037416 Out[12]: biden coming here is president Sentence 0 1 0 1 1 0 Sentence 1 0 1 1 1 1 Note : Normally cosine value=0 means that the two vectors are orthogonal to each other. scipy implements cosine as 1-cosine , so cosine=0 means no connection and cosine=1 means orthogonal. Cosine metric is used in Collaborative Filtering (Recommender systems for movies). Hamming Distance (extra): If our two elements of comparison can be represented a sequence of discrete items, it can be useful to measure how many of their elements differ. For example: Mahmoud and Mahmood differ by just 1 character and thus have a hamming distance of 1. 10101 and 01101 have a hamming distance of 2. Mary and Barry have a hamming distance of 3 (m->b, y->r, null->y). Note : the last example may seem sub-optimal, as we could transform Mary to Barry by just 2 operations (substituting the M with a B, then adding an 'r'). So, their so-called edit distance is smaller than their Hamming distance. The very related Levenshtein distance here can handle this, and thus tends to be more appropriate for Strings. 4 - Clustering Algorithms Question: Why do we care about clustering? How/why is it useful? We will now walk through three clustering algorithms, first discussing them at a high-level, then showing how to implement them with Python libraries. Let's first load and scale our data, so that particular dimensions don't naturally dominate in their contributions in the distant calculations: In [13]: multishapes = pd . read_csv ( \"data/multishapes.csv\" ) ms_df = multishapes [[ 'x' , 'y' ]] ms_df . plot . scatter ( x = 'x' , y = 'y' , c = 'Blue' , \\ title = \"Multishapes data\" , \\ figsize = ( 5.5 , 4.2 )) plt . show () ms_df . head () Out[13]: x y 0 -0.803739 -0.853053 1 0.852851 0.367618 2 0.927180 -0.274902 3 -0.752626 -0.511565 4 0.706846 0.810679 In [14]: # displays our summary statistics of our data ms_df . describe () Out[14]: x y count 1100.000000 1100.000000 mean -0.081222 -0.625431 std 0.644967 1.176170 min -1.489180 -3.353462 25% -0.478839 -1.126752 50% -0.132920 -0.297040 75% 0.366072 0.250817 max 1.492208 1.253874 In [15]: scaler = StandardScaler () scaler = scaler . fit ( ms_df ) print ( scaler . mean_ ) scaled_df = scaler . transform ( ms_df ) ###### if I had a test set I would transform here: # test_scaled_df = scaler.transform(test_df) ################################################## scaled_df = pd . DataFrame ( scaled_df , \\ index = multishapes [ 'shape' ], columns = ms_df . columns ) scaled_df . describe () [-0.08122171 -0.6254313 ] Out[15]: x y count 1.100000e+03 1.100000e+03 mean -3.108624e-17 -2.779595e-16 std 1.000455e+00 1.000455e+00 min -2.183985e+00 -2.320473e+00 25% -6.167723e-01 -4.264248e-01 50% -8.019252e-02 2.793306e-01 75% 6.938298e-01 7.453401e-01 max 2.440659e+00 1.598544e+00 Very important reminder!! If you have a training and a test set, always .fit() your scaler only to the training set, and then .transform() both sets. Let's plot this data with and without scaling In [16]: # plot our data msplot = ms_df . plot . scatter ( x = 'x' , y = 'y' , c = 'Black' , \\ title = \"Multishapes data (no scaling)\" , \\ figsize = ( 5.5 , 4.2 )) msplot . set_xlabel ( \"X\" ) msplot . set_ylabel ( \"Y\" ) plt . show () In [17]: # plots our data msplot = scaled_df . plot . scatter ( x = 'x' , y = 'y' , c = 'Black' , \\ title = \"Multishapes data (w/ scaling)\" , \\ figsize = ( 5.5 , 4.2 )) msplot . set_xlabel ( \"X\" ) msplot . set_ylabel ( \"Y\" ) plt . show () 3a. $k$-Means clustering: Code (via sklearn ): In [18]: ms_kmeans = KMeans ( n_clusters = 2 , init = 'random' , n_init = 3 , random_state = 109 ) ms_kmeans . fit ( scaled_df ) Out[18]: KMeans(init='random', n_clusters=2, n_init=3, random_state=109) Now that we've run k-means, we can look at various attributes of our clusters. Full documenation is here . In [19]: display ( ms_kmeans . cluster_centers_ ) display ( ms_kmeans . labels_ [ 0 : 10 ]) array([[-0.44316951, -1.56366452], [ 0.1434568 , 0.50616818]]) array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32) Plotting Take note of matplotlib's c= argument to color items in the plot, along with our stacking two different plotting functions in the same plot. In [20]: plt . figure ( figsize = ( 10 , 10 )) plt . scatter ( scaled_df [ 'x' ], scaled_df [ 'y' ], c = ms_kmeans . labels_ ); plt . scatter ( ms_kmeans . cluster_centers_ [:, 0 ], ms_kmeans . cluster_centers_ [:, 1 ], c = 'r' , marker = 'h' , s = 100 ); Out[20]: Quality of Clusters A - Inertia Inertia measures the total squared distance from points to their cluster's centroid. We obviously want this distance to be relatively small. If we increase the number of clusters, it will naturally make the average distance smaller. If every point has its own cluster, then our distance would be 0. That's obviously not an ideal way to cluster. One way to determine a reasonable number of clusters to simply try many different clusterings as we vary k , and each time, measure the overall inertia. In [21]: wss = [] for i in range ( 1 , 11 ): fitx = KMeans ( n_clusters = i , init = 'random' , n_init = 5 , random_state = 109 ) . fit ( scaled_df ) wss . append ( fitx . inertia_ ) plt . figure ( figsize = ( 11 , 8.5 )) plt . plot ( range ( 1 , 11 ), wss , 'bx-' ) plt . xlabel ( 'Number of clusters $k$' ) plt . ylabel ( 'Inertia' ) plt . title ( 'The Elbow Method showing the optimal $k$' ) plt . show () Look for the place(s) where distance stops decreasing as much (i.e., the 'elbow' of the curve). It seems that 4 would be a good number of clusters, as a higher k yields diminishing returns. Exercise : Run K-means again with 4 clusters this time. B - Silhouette Let's say we have a data point $i$, and the cluster it belongs to is referred to as $C(i)$. One way to measure the quality of a cluster $C(i)$ is to measure how close its data points are to each other (within-cluster) compared to nearby, other clusters $C(j)$. This is what Silhouette Scores provide for us. The range is [-1,1]; 0 indicates a point on the decision boundary (equal average closeness to points intra-cluster and out-of-cluster), and negative values mean that datum might be better in a different cluster. Specifically, let $a(i)$ denote the average distance data point $i$ is to the other points in the same cluster: Similarly, we can also compute the average distance that data point $i$ is to all other clusters. The cluster that yields the minimum distance is denoted by $b(i)$: Hopefully our data point $i$ is much closer, on average, to points within its own cluster (i.e., $a(i)$ than it is to its closest neighboring cluster $b(i)$). The silhouette score quantifies this as $s(i)$: NOTE: If data point $i$ belongs to its own cluster (no other points), then the silhouette score is set to 0 (otherwise, $a(i)$ would be undefined). The silhouette score plotted below is the overall average across all points in our dataset. The silhouette_score() function is available in sklearn . We can manually loop over values of K (for applying k-Means algorithm), then plot its silhouette score. In [22]: scores = [ 0 ] for i in range ( 2 , 11 ): fitx = KMeans ( n_clusters = i , init = 'random' , n_init = 46 , random_state = 109 ) . fit ( scaled_df ) score = silhouette_score ( scaled_df , fitx . labels_ ) scores . append ( score ) plt . figure ( figsize = ( 11 , 8.5 )) plt . plot ( range ( 1 , 11 ), np . array ( scores ), 'bx-' ) plt . xlabel ( 'Number of clusters $k$' ) plt . ylabel ( 'Average Silhouette' ) plt . title ( 'The Elbow Method showing the optimal $k$' ) plt . show () Visualizing all Silhoutte scores for a particular clustering Below, we borrow from an sklearn example. The second plot may be overkill. The second plot is just the scaled data. It is not a PCA plot If you only need the raw silhouette scores, use the silhouette_samples() function In [23]: # modified code from # http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html def silplot ( X , clusterer , pointlabels = None ): cluster_labels = clusterer . labels_ n_clusters = clusterer . n_clusters # Create a subplot with 1 row and 2 columns fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 ) fig . set_size_inches ( 11 , 8.5 ) # The 1st subplot is the silhouette plot # The silhouette coefficient can range from -1, 1 but in this example all # lie within [-0.1, 1] ax1 . set_xlim ([ - 0.1 , 1 ]) # The (n_clusters+1)*10 is for inserting blank space between silhouette # plots of individual clusters, to demarcate them clearly. ax1 . set_ylim ([ 0 , len ( X ) + ( n_clusters + 1 ) * 10 ]) # The silhouette_score gives the average value for all the samples. # This gives a perspective into the density and separation of the formed # clusters silhouette_avg = silhouette_score ( X , cluster_labels ) print ( \"For n_clusters = \" , n_clusters , \", the average silhouette_score is \" , silhouette_avg , \".\" , sep = \"\" ) # Compute the silhouette scores for each sample sample_silhouette_values = silhouette_samples ( X , cluster_labels ) y_lower = 10 for i in range ( 0 , n_clusters ): # Aggregate the silhouette scores for samples belonging to # cluster i, and sort them ith_cluster_silhouette_values = \\ sample_silhouette_values [ cluster_labels == i ] ith_cluster_silhouette_values . sort () size_cluster_i = ith_cluster_silhouette_values . shape [ 0 ] y_upper = y_lower + size_cluster_i color = cm . nipy_spectral ( float ( i ) / n_clusters ) ax1 . fill_betweenx ( np . arange ( y_lower , y_upper ), 0 , ith_cluster_silhouette_values , facecolor = color , edgecolor = color , alpha = 0.7 ) # Label the silhouette plots with their cluster numbers at the middle ax1 . text ( - 0.05 , y_lower + 0.5 * size_cluster_i , str ( i )) # Compute the new y_lower for next plot y_lower = y_upper + 10 # 10 for the 0 samples ax1 . set_title ( \"The silhouette plot for the various clusters.\" ) ax1 . set_xlabel ( \"The silhouette coefficient values\" ) ax1 . set_ylabel ( \"Cluster label\" ) # The vertical line for average silhouette score of all the values ax1 . axvline ( x = silhouette_avg , color = \"red\" , linestyle = \"--\" ) ax1 . set_yticks ([]) # Clear the yaxis labels / ticks ax1 . set_xticks ([ - 0.1 , 0 , 0.2 , 0.4 , 0.6 , 0.8 , 1 ]) # 2nd Plot showing the actual clusters formed colors = cm . nipy_spectral ( cluster_labels . astype ( float ) / n_clusters ) ax2 . scatter ( X [:, 0 ], X [:, 1 ], marker = '.' , s = 200 , lw = 0 , alpha = 0.7 , c = colors , edgecolor = 'k' ) xs = X [:, 0 ] ys = X [:, 1 ] if pointlabels is not None : for i in range ( len ( xs )): plt . text ( xs [ i ], ys [ i ], pointlabels [ i ]) # Labeling the clusters centers = clusterer . cluster_centers_ # Draw white circles at cluster centers ax2 . scatter ( centers [:, 0 ], centers [:, 1 ], marker = 'o' , c = \"white\" , alpha = 1 , s = 200 , edgecolor = 'k' ) for i , c in enumerate ( centers ): ax2 . scatter ( c [ 0 ], c [ 1 ], marker = '$ %d $' % int ( i ), alpha = 1 , s = 50 , edgecolor = 'k' ) ax2 . set_title ( \"The visualization of the clustered data.\" ) ax2 . set_xlabel ( \"Feature space for the 1st feature\" ) ax2 . set_ylabel ( \"Feature space for the 2nd feature\" ) plt . suptitle (( \"Silhouette analysis for KMeans clustering on sample data \" \"with n_clusters = %d \" % n_clusters ), fontsize = 14 , fontweight = 'bold' ) In [24]: ms_kmeans = KMeans ( n_clusters = 4 , init = 'random' , n_init = 3 , random_state = 109 ) . fit ( scaled_df ) # plot a fancy silhouette plot silplot ( scaled_df . values , ms_kmeans ) For n_clusters = 4, the average silhouette_score is 0.45880539445085916. C - Gap Statistic The gap statistic compares within-cluster distances (such as in silhouette), but instead of comparing against the second-best existing cluster for that point, it compares our clustering's overall average to the average we'd see if the data were generated at random (we'd expect randomly generated data to not necessarily have any inherit patterns that can be easily clustered). In essence, the within-cluster distances (in the elbow plot) will go down just becuse we have more clusters. We additionally calculate how much they'd go down on non-clustered data with the same spread as our data and subtract that trend out to produce the plot below. The original paper is : \" Estimating the number of clusters in a data set via the gap statistic \" (Tibshirani et al.). As suggested in the paper, we would choose the value of $\\hat{k}$ (number of clusters) such that $\\hat{k}$ = smallest $k$ such that Gap($k$) $\\geq$ Gap($k+1$) - $s_{k+1}$. We compare the actual Gap value of the k point to the lower bar of the Gap value of the k+1 point. The following graph should make it clearer. The plot is from the original paper (Fig. 2) (dashed lines are mine) We could argue that we should have chosen the largest value (k =3) instead of the first value that satisfies the Gap statistic equation (k=1 in this case). If you're able to compute for a range of k, then you can choose the maximum. For example, in the graph above, since we're computing over k=1,..,10, we could choose k=3. The original paper although it suggests that we look at the whole range, chooses k=1 in the case above; if you see the raw data plotted (Fig. 2 in the paper) you will also notice that there is really not much structure for cluster subdivision but we should always investigate the whole plot. Also, it's very computationally intensive to compute the Gap statistic. Additionally, you can use domain knowledge or whatever information you have about the data to choose k. The gap statistic is implemented by Miles Granger in the gap_statistic ( https://github.com/milesgranger/gap_statistic ) Python library. The library also implements the Gap$&#94;*$ statistic described in \"A comparison of Gap statistic definitions with and with-out logarithm function( https://core.ac.uk/download/pdf/12172514.pdf )\" (Mohajer, M., Englmeier, K. H., & Schmid, V. J., 2011) which is less conservative but tends to perform suboptimally when clusters overlap. Generating synthetic data again In [25]: plt . title ( \"Four blobs\" , fontsize = 'small' ) X1 , Y1 = make_blobs ( n_features = 2 , centers = 4 ) # centers is number of classes plt . scatter ( X1 [:, 0 ], X1 [:, 1 ], marker = 'o' , c = Y1 , s = 25 , edgecolor = 'k' ); blob_df = pd . DataFrame ( X1 , Y1 ) plt . axis ( 'equal' ); blob_df . head () Out[25]: 0 1 1 -0.780056 6.247700 3 4.942465 -2.969769 2 3.552698 -7.304807 1 -1.574659 6.366932 2 7.879188 -7.814325 In [26]: gs_obj = OptimalK () n_clusters = gs_obj ( X1 , n_refs = 500 , cluster_array = np . arange ( 1 , 15 )) print ( 'Optimal number of clusters: ' , n_clusters ) Optimal number of clusters: 4 In [27]: ms_kmeans = KMeans ( n_clusters = n_clusters , init = 'random' , \\ n_init = 3 , random_state = 109 ) . fit ( X1 ) plt . figure ( figsize = ( 5 , 5 )) plt . scatter ( X1 [:, 0 ], X1 [:, 1 ], marker = 'o' , c = Y1 , s = 25 , edgecolor = 'k' ); plt . scatter ( ms_kmeans . cluster_centers_ [:, 0 ], \\ ms_kmeans . cluster_centers_ [:, 1 ], c = 'r' , marker = 'h' , s = 200 ); plt . axis ( 'equal' ); Out[27]: (-3.3025435783231623, 10.473280979534094, -10.94762035999889, 8.835835742038647) In [28]: def display_gapstat_with_errbars ( gap_df ): gaps = gap_df [ \"gap_value\" ] . values diffs = gap_df [ \"diff\" ] err_bars = np . zeros ( len ( gap_df )) err_bars [ 1 :] = diffs [: - 1 ] - gaps [: - 1 ] + gaps [ 1 :] plt . scatter ( gap_df [ \"n_clusters\" ], gap_df [ \"gap_value\" ]) plt . errorbar ( gap_df [ \"n_clusters\" ], gap_df [ \"gap_value\" ], yerr = err_bars , capsize = 6 ) plt . xlabel ( \"Number of Clusters\" ) plt . ylabel ( \"Gap Statistic\" ) plt . show () display_gapstat_with_errbars ( gs_obj . gap_df ) For more information about the gap_stat package, please see the full documentation here . 3b. Agglomerative Clustering Code (via scipy ): There are many different cluster-merging criteria, one of which is Ward's criteria. Ward's optimizes having the lowest total within-cluster distances, so it merges the two clusters that will harm this objective least. scipy 's agglomerative clustering function implements Ward's method. In [29]: USArrests = pd . read_csv ( 'data/USArrests2.csv' ) In [30]: df = USArrests [[ 'SexualAssault' , 'Assault' , 'UrbanPop' , 'Homicide' ]] arrests_scaled = pd . DataFrame ( preprocessing . scale ( df ), index = USArrests [ 'State' ], columns = df . columns ) Understanding the notion of a vector (data) matrix and a distance or dissimilarity matrix In [31]: # data matrix df . iloc [[ 3 , 15 , 40 ]] Out[31]: SexualAssault Assault UrbanPop Homicide 3 19.5 190 50 8.8 15 18.0 115 66 6.0 40 12.8 86 45 3.8 In [32]: # distance matrix from sklearn.metrics import pairwise_distances pairwise_distances ( df . iloc [[ 3 , 15 , 40 ]], metric = 'euclidean' ) Out[32]: array([[ 0. , 76.75343641, 104.45520571], [ 76.75343641, 0. , 36.24748267], [104.45520571, 36.24748267, 0. ]]) In [33]: # import scipy.cluster.hierarchy as hac plt . figure ( figsize = ( 11 , 8.5 )) dist_mat = pdist ( arrests_scaled , metric = \"euclidean\" ) ward_data = hac . ward ( dist_mat ) hac . dendrogram ( ward_data , labels = USArrests [ \"State\" ] . values ); plt . show () Discussion : How do you read a plot like the above? What are valid options for number of clusters, and how can you tell? Are some more valid than others? Lessons: It's expensive: $O(n&#94;3)$ time complexity and $O(n&#94;2)$ space complexity. Many choices for linkage criteria Every node gets clustered (no child left behind) Example of the use of hierarchical clustering as an exploratory tool Clustering our data in a hierarchical way can reveal associations between them. We can exploit those associations later in the model. 3c. DBSCAN Clustering DBSCAN uses an intuitive notion of denseness to define clusters, rather than defining clusters by a central point as in k-means. Code (via sklearn ): DBscan is implemented in sklearn , but there aren't great automated tools for searching for the optimal epsilon parameter. For full documentation, please visit this page In [34]: plt . figure ( figsize = ( 11 , 8.5 )) fitted_dbscan = DBSCAN ( eps = 0.2 ) . fit ( scaled_df ) plt . scatter ( scaled_df [ 'x' ], scaled_df [ 'y' ], c = fitted_dbscan . labels_ ); Out[34]: Note: the dark purple dots are not clustered with anything else. They are lone singletons. You can validate such by setting epsilon to a very small value, and increase the min_samples to a high value. Under these conditions, nothing would cluster, and yet all dots become dark purple. Exercise : Experiment with the above code by changing its epsilon value and the min_samples (what is the default value for it, since the above code doesn't specify a value?) Lessons: Can cluster non-linear relationships very well; potential for more natural, arbritrarily shaped groupings Does not require specifying the # of clusters (i.e., k ); the algorithm determines such Robust to outliers Doesn't guarantee that every (or ANY) item will be clustered 5. Extra: Clustering for images Breakroom Exercise : Let's load the MNIST digits and try to use $k$-means to cluster them. In [35]: digits = load_digits () print ( digits . data . shape , digits . images . shape , digits . target . shape ) (1797, 64) (1797, 8, 8) (1797,) In [36]: plt . gray () plt . figure ( figsize = ( 8 , 5 )) for i in range ( 12 ): plt . subplot ( 4 , 3 , i + 1 ); plt . imshow ( digits . images [ i ]); In [37]: scaler = StandardScaler () scaler . fit ( digits . data ) digits_scaled = scaler . transform ( digits . data ) In [38]: digits_scaled . shape Out[38]: (1797, 64) In [39]: n_digits = 10 kmeans = KMeans ( init = \"random\" , n_clusters = n_digits , random_state = 22 ) kmeans . fit ( digits_scaled ) Out[39]: KMeans(init='random', n_clusters=10, random_state=22) In [40]: cluster_assignments = zip ( digits_scaled , kmeans . labels_ ) In [41]: # inititalize cluster dict clusters = {} for i in range ( n_digits ): clusters [ i ] = [] clusters Out[41]: {0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: []} In [42]: for idx , j in enumerate ( kmeans . labels_ ): clusters [ j ] . append ( digits . target [ idx ]) Check out the cluster quality manually In [43]: for num in range ( 10 ): df = pd . Series ( clusters [ num ]) print ( f 'Cluster = { num } ' ) print ( df . value_counts ()) Cluster = 0 0 176 6 1 dtype: int64 Cluster = 1 2 113 8 11 3 7 7 5 1 1 dtype: int64 Cluster = 2 1 59 9 19 8 12 4 6 2 5 7 3 6 1 dtype: int64 Cluster = 3 2 43 1 27 3 1 dtype: int64 Cluster = 4 3 158 9 144 8 50 5 40 2 3 dtype: int64 Cluster = 5 7 18 4 8 9 3 dtype: int64 Cluster = 6 4 158 5 2 7 1 0 1 dtype: int64 Cluster = 7 6 178 8 4 5 2 0 1 dtype: int64 Cluster = 8 7 152 1 94 8 94 2 13 3 13 9 11 4 8 5 4 6 1 dtype: int64 Cluster = 9 5 134 3 4 9 3 8 3 4 1 1 1 dtype: int64 References: [1] A Tutorial on Principal Component Analysis","tags":"labs","url":"labs/lab01/"},{"title":"Lab 1: Clustering","text":"Notebook Lab 1: Intro Lab 1: Clustering","tags":"labs","url":"labs/labs01/"},{"title":"CS109b: Advanced Topics in Data Science","text":"Spring 2022 Pavlos Protopapas & Mark Glickman Advanced Topics in Data Science (CS109b) is the second half of a one-year introduction to data science. Building upon the material in Introduction to Data Science, the course introduces advanced methods for data wrangling, data visualization, statistical modeling, and prediction. Topics include big data, multiple deep learning architectures such as CNNs, RNNs, transformers, language models, autoencoders, and generative models as well as basic Bayesian methods, and unsupervised learning. Helpline: cs109b2022@gmail.com Lectures: Mon & Wed 9:45‐11 am Sections: Fri 9:45-11 am Advanced Sections: Weds 2:15-3:30 pm (see schedule for specific dates) Office Hours: TBD Course material can be viewed in the public GitHub repository . Previous Material 2021 2020 2019 2018","tags":"pages","url":"pages/cs109b-advanced-topics-in-data-science/"}]}