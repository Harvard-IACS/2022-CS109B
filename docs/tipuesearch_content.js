var tipuesearch = {"pages":[{"title":"FAQ","text":"General I'm unable to attend lectures in person. Can I take the class asynchronously? Lecture attendance is required. Non-DCE students should only register if they can attend in person. Does the individual HW mean I have to submit on my own but can I still work with a HW partner? An individual CS109B HW means you are supposed to work on your own, without any human intervention, assistance, or input. You are not allowed to work with a partner. You are allowed to use OHs to ask clarification questions, but we may not be able to answer all of your questions or help with your coding. You are allowed to use all of your materials from the course up to the HW, look at problem resolution online, and look at documentation. Where should I send my questions? Use Ed for anying related to the course content or assignments. All other concerns should be sent to the course helpline: cs109b2022@gmail.com Auditing Can I audit this course? What are the rules for auditing? Yes, CS109B does accept auditors, but all auditors must agree to abide by the following rules: Auditors must attend class in person. This is a Harvard policy. Auditors who do not confirm their presence with the head TF during the first week of in-class instruction will lose course access. Auditors are held to the same standard of academic honesty as our registered students. Please do not share homeworks or solutions with anyone. Violations will be reported to the Harvard Administrative Board. Auditors are not permitted to take the course for credit in the future. Audiors should not submit HWs or participate in projects. Auditors should refrain from using any course and TF resources that are designed for our registered students like Ed, FASOnDemand, JupyterHub, and office hours. Auditors must have an active HUID number. If you agree to the above, send an email to cs109a2022@gmail.com with your HUID and state that you've read the auditing policy and that you agree to abide by these rules. Quizzes & Exercises When are quizzes and exercises due? All 'pre-class reading check' quizzes and exercises presented in class are ungraded and so have no due date, though solutions will be released within a few days. All graded quizzes and exercises on Ed will be 'bundled' with a given HW assignment and so will share that HW's due date. Each HW assignment will make clear which Ed quizzes and exercises are to be completed as part of the assignment.","tags":"pages","url":"pages/faq.html"},{"title":"Schedule","text":"Date (Mon) Lecture (Mon) Lecture (Wed) Lab (Fri) Advanced Section (Wed) Assignment (R:Released Wed - D:Due Wed) 24-Jan Lecture 1: Clustering 1 Lecture 2: Clustering 2 Lab 1 R:HW1 31-Jan Lecture 3: Bayes 1 Lecture 4: Bayes 2 Lab 2 Advanced Section 1: Gaussian Mixture Models 7-Feb Lecture 5: Bayes 3 Lecture 6: Bayes 4 Lab 3 R:HW2 - D:HW1 14-Feb Lecture 7: Bayes 5 Lecture 8: Neural Networks 1 (MLP) Lab 4 Advanced Section 2: Particle Filters/Sequential Monte Carlo 21-Feb No Lecture (Holiday) Lecture 9: NN 2 Gradient Descent SGD & BackProp) Lab 5 28-Feb Lecture 10: NN 3 (Optimizers) Lecture 11: NN 4 (Regularization) Lab 6 Advanced Section 3: Solvers 7-Mar Lecture 12: Convolutional Neural Networks 1 (Basics) Lecture 13: CNNs 2 (Regularization) Lab 7 Advanced Section 4: Segmentation R:HW4 - D:HW3 14-Mar No Lecture (Spring Break) No Lecture No Lab 21-Mar Lecture 14: CNNs 3 (Receptive Field) Lecture 15: CNNs 4 (Saliency Map) No Lab (Midterm) Advanced Section 5: SOTA & Transfer Learning 28-Mar Lecture 16: Intro to Language Models Lecture 17: Recurrent Neural Networks Lab 8 Advanced Section 6: Autoencoders R:HW5(Individual) - D:HW4 4-Apr Lecture 18: NLP 1 (GRUs/LSTMs) Lecture 19: NLP 2 (ELMO) Lab 9 Advanced Section 7: Word2Vec R:HW6(Individual) - D:HW5(Individual) 11-Apr Lecture 20: NLP 3 (Seq2Seq & Attention) Lecture 21: NLP 4 (Transformers) Lab 10 Advanced Section 8: BERT 18-Apr Lecture 22: GANs 1 Lecture 23: GANs 2 Lab 11 Advanced Section 9: More GANs! D:HW6(Individual) - R:HW7 25-Apr Module: Lecture Domain Module: Problem Background Project Work D:HW7 2-May Project Work Project Work Project Submission Due 9-May Peer Evaluations Due Final Project Showcase","tags":"pages","url":"pages/schedule.html"},{"title":"Syllabus","text":"Draft Syllabus Subject to Change Advanced Topics in Data Science (Spring 2022) CS 109b, AC 209b, Stat 121b, or CSCI E-109b Instructors Pavlos Protopapas (SEAS) & Mark Glickman (Statistics) Lectures: Mon & Wed 9:45‚Äê11am SEC 1.321 Labs: Fri 9:45-11am SEC 1.321 Advanced Sections: Wed 2:15-3:30pm SEC 1.321 (starting 2/2; see schedule for specific dates) Office Hours: See Ed Post Prerequisites: CS 109a, AC 209a, Stat 121a, or CSCI E-109a or the equivalent. Course description Tentative Course Topics Course Objectives Course Components Lectures Labs Advanced Sections Midterm Projects Homework Assignments Course Resources Online Materials Recommended Textbooks & Articles Getting Help Course Policies and Expectations Grading Collaboration Policy Late or Incorrectly Submitted Assignments Re-grade Requests Auditing the Class Academic Integrity Accommodations for students with disabilities Diversity and Inclusion Statement Course Description Advanced Topics in Data Science (CS109b) is the second half of a one-year introduction to data science. Building upon the material in Introduction to Data Science, the course introduces advanced methods for data wrangling, data visualization, statistical modeling, and prediction. Topics include big data, multiple deep learning architectures such as CNNs, RNNs, language models, transformers, autoencoders, and generative models as well as Bayesian modeling, sampling methods, and unsupervised learning. The programming language will be Python. Tentative Course Topics Unsupervised Learning, Clustering Bayesian Inference Hierarchical Bayesian Modeling Fully Connected Neural Networks Convolutional Neural Networks Autoencoders Recurrent Neural Networks NLP / Text Analysis Transformers Variational AutoEncoders & Generative Models Generative Adversarial Networks Course Objectives Upon successful completion of this course, you should feel comfortable with the material mentioned above, and you will have gained experience working with others on real-world problems. The content knowledge, the project, and teamwork will prepare you for the professional world or further studies. Course Components Lectures, labs, and advanced sections will be live-streamed for Extension School students and can be accessed through the Zoom section on Canvas. Video recordings of the live stream will be made available to all students within 24 hours after the event, and will be accessible from the Lecture Video section on Canvas. Lectures The class meets for lectures twice a week (M & W). Attending and participating in lectures is a crucial component of learning the material presented in this course. Students may be asked to complete short readings before certain lectures. Some lectures will also include real-time coding exercises which we will complete as a class. Labs Lab will be held every Friday. Labs will present guided, hands-on coding challenges to prepare students for successfully completing the homework assignments. Advanced Sections The course will include advanced sections for 209b students and will cover a different topic per week. These 75 min sessions will cover advanced topics like the mathematical underpinnings of the methods seen in the main course lectures and lab as well as extensions of those methods. The material covered in the advanced sections is required for all AC209b students. Tentative topics are: Gaussian Mixture Models Particle Filters/Sequential Monte Carlo NN as Universal Approximator Solvers Segmentation Techniques, YOLO, Unet and M-RCNN Variational Autoencoders Word2Vec BERT GANS, Cycle GANS, etc. Note: Advanced Section are not held every week. Consult the course calendar for exact dates. Midterm There will be a midterm exam on Friday, March 25th from 9:45-11am (regular lab time). The exam will likely consist of multiple choice questions with a take-home coding component. More information to follow. Projects Beginning the last week of classes (4/25), students will join groups of 3-4 and be divided into break-out, thematic sections to study an open problem in one of various domains. The domains are tentative at the moment but may include medicine, law, astronomy, e-commerce, government, and areas in the humanities. Each section will include lectures by Harvard faculty who are experts in the field. Project work will continue on through reading period and conclude with final submissions on 5/6. The final submission will consist of a written report, a Jupyter notebook with all relevant code, and a 6-minute, pre-recorded presentation video. Homework Assignments There will be 7 graded homework assignments. Some of them will be due one week after being assigned and some will be due two weeks after being assigned. For 5 assignments, you have the option to work and submit in pairs, the 2 remaining are to be completed individually. Standard assignments are graded out of 5 points. AC209b students will have additional homework content for most assignments worth 1 point. Course Resources Online Materials All course materials, including lecture notes, lab notes, and section notes will be published on Ed, the course GitHub repo, and the public site's 'Materials' section. Note: Lecture content for lectures 1-7 will only be accessible to registered students. Assignments will only be posted on Canvas. Working Environment You will be working in Jupyter Notebooks which you can run in your own machine or in the SEAS JupyterHub. Recommended Textbooks ISLR: An Introduction to Statistical Learning by James, Witten, Hastie, Tibshirani (Springer: New York, 2013) BDA3: Bayesian Data Analysis by Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, Donald B. Rubin (CRC Press: New York, 2013) DL: Deep Learning by Goodfellow, Bengio and Courville. (The MIT Press: Cambridge, 2016) Glassner: Deep Learning, Vol. 1 & 2 by Andrew Glassner SLP Speech and Language Processing by Jurafsky and Martin (3rd Edition Draft) INLP Introduction to Natural Language Processing by Jacob Eisenstein (The MIT Press: Cambridge, 2019) Free electronic versions are available ( ISLR , DL , SLP , INLP ) or hard copy through Amazon ( ISLR , DL , Glassner , SLP , INLP ). Articles & Excerpts Unsupervised learning: Basics: James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An introduction to statistical learning (2nd ed.). New York: Springer. Chapter 12 https://hastie.su.domains/ISLR2/ISLRv2_website.pdf Silhouette plots: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html Gap statistic: Tibshirani, R., Walther, G., & Hastie, T. (2001). Estimating the number of clusters in a data set via the gap statistic. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), 411-423. https://hastie.su.domains/Papers/gap.pdf DBSCAN: Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017). DBSCAN revisited, revisited: why and how you should (still) use DBSCAN. ACM Transactions on Database Systems (TODS), 42(3), 1-21. https://dl.acm.org/doi/pdf/10.1145/3068335?casa_token=_P479lYnlpsAAAAA:PckzU6ZiTt3yMNzFrXyzESZ3N_pp904kN0N2QEwIoq6CxtfPCxnL9bNTGtjhuiNtzSfKyXoM-QI Bayesian material Basics: Glickman, Mark E. and Van Dyk, David A. (2007) \"Basic Bayesian Methods\" In Topics in Biostatistics (Methods in Molecular Biology). Edited by Walter Ambrosius. The Humana Press Inc., Totowa, NJ. ISBN 1-58829-531-1. pp 319-338. Chapter accessible from http://www.glicko.net/research/glickman-vandyk.pdf Importance sampling, rejection sampling, MCMC, Metropolis, Gibbs sampler: Andrieu, C., De Freitas, N., Doucet, A., & Jordan, M. I. (2003). An introduction to MCMC for machine learning. Machine learning, 50(1), 5-43. Article accessible from: https://www.cs.ubc.ca/~arnaud/andrieu_defreitas_doucet_jordan_intromontecarlomachinelearning.pdf Bayesian examples - regression, hierarchical modeling: Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A., & Rubin, D.B. (2013). Bayesian Data Analysis (3rd ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b16018 http://www.stat.columbia.edu/~gelman/book/BDA3.pdf Chapter 14: Introduction to regression models Chapter 15: Hierarchical linear models Chapter 16: Generalized linear models (includes logistic regression) Getting Help For questions about homework, course content, package installation, the process is: try to troubleshoot yourself by reading the lecture, lab, and section notes, and looking up online resources. go to office hours this is the best way to get help. post on the class Ed forum; we want you and your peers to engage in helping each other. TFs also monitor Ed and will respond within 24 hours. Note that Ed questions are visible to everyone. If you are citing homework solution code you must post privately so that only the staff sees your message. watch for official announcements via Ed. These announcements will also be sent to the email address associated with your Canvas account so make sure you have it set appropriately. send an email to the Helpline cs109b2022@gmail.com for administrative issues, regrade requests, and non-content specific questions. for personal matters that you do not feel comfortable sharing with the TFs, you may send an email to either or both of the instructors. Course Policies and Expectations Grading for CS109b, STAT121b, and CS209b: Your final score for the course will be computed using the following weights: Assignment Final Grade Weight Paired Homework (5) 45% Individual Homework (2) 23% Midterm 12% Project 20% Total 100% Note: Regular homework (for everyone) counts as 5 points. 209b extra homework counts as 1 point. Collaboration Policy We expect you to adhere to the Harvard Honor Code at all times. Failure to adhere to the honor code and our policies may result in serious penalties, up to and including automatic failure in the course and reference to the ad board. If you work with a partner on an assignment make sure both parties solve all the problems. Do not divide and conquer. You are expected to be intellectually honest and give credit where credit is due. In particular: if you work with a fellow student but decide to submit different papers, include the name of each other in the designated area of the submission paper. if you work with a fellow student and want to submit the same paper you need to form a group prior to the submission. Details in the assignment. Not all assignments will permit group submissions. you need to write your solutions entirely on your own or with your collaborator you are welcome to take ideas from code presented in labs, lecture, or sections but you need to change it, adapt it to your style, and ultimately write your own. We do not want to see code copied verbatim from the above sources. if you use code found on the internet, books, or other sources you need to cite those sources. you should not view any written materials or code created by other students for the same assignment; you may not provide or make available solutions to individuals who take or may take this course in the future. if the assignment allows it you may use third-party libraries and example code, so long as the material is available to all students in the class and you give proper attribution. Do not remove any original copyright notices and headers. Late or Incorrectly Submitted Assignments Each student is allowed up to 3 late days over the semester with at most 1 day applied to any single homework . Outside of these allotted late days, late homework will not be accepted unless there is a medical (if accompanied by a doctor's note) or other official University-excused reasons. There is no need to ask before using one of your late days. If you forgot to join a Group with your peer and are asking for the same grade we will accept this with no penalty up to HW3. For homeworks beyond that we feel that you should be familiar with the process of joining groups. After that there will be a penalty of -1 point for both members of the group provided the submission was on time. Grading Guidelines Homework will be graded based on: How correct your code is (the Notebook cells should run, we are not troubleshooting code) How you have interpreted the results ‚Äî we want text not just code. It should be a report. How well you present the results. The scale is 0 to 5 for each assignment and 0 to 1 for the additional 209 assignments. Re-grade Requests Our graders and instructors make every effort in grading accurately and in giving you a lot of feedback. If you discover that your answer to a homework problem was correct but it was marked as incorrect, send an email to the Helpline with a description of the error. Please do not submit regrade requests based on what you perceive is overly harsh grading . The points we take off are based on a grading rubric that is being applied uniformly to all assignments. If you decide to send a regrade request , send an email to the Helpline with subject line \"Regrade HW1: Grader=johnsmith\" replacing 'HW1' with the current assignment and 'johnsmith' with the name of the grader within 48 hours of the grade release. Auditing the Class You are welcome to audit this course. To request access, send an email to cs109b2022@gmail.com with you HUID (required) and a statement of agreement to the terms below. All auditors must agree to abide by the following rules: Auditors must attend class in person. This is a Harvard policy. Auditors who do not confirm their presence with the head TF during the first week of in-class instruction will lose course access. Auditors are held to the same standard of academic honesty as our registered students. Please do not share homeworks or solutions with anyone. Violations will be reported to the Harvard Administrative Board. Auditors are not permitted to take the course for credit in the future. Audiors should not submit HWs or participate in projects. Auditors should refrain from using any course and TF resources that are designed for our registered students like Ed, FASOnDemand, JupyterHub, and office hours. Academic Integrity Ethical behavior is an important trait of a Data Scientist, from ethically handling data to attribution of code and work of others. Thus, in CS109b we give a strong emphasis to Academic Honesty. As a student your best guidelines are to be reasonable and fair. We encourage teamwork for problem sets, but you should not split the homework and you should work on all the problems together. For more detailed expectations, please refer to the Collaborations section above. You are responsible for understanding Harvard Extension School policies on academic integrity https://www.extension.harvard.edu/resources-policies/student-conduct/academic-integrity and how to use sources responsibly. Stated most broadly, academic integrity means that all course work submitted, whether a draft or a final version of a paper, project, take-home exam, online exam, computer program, oral presentation, or lab report, must be your own words and ideas, or the sources must be clearly acknowledged. The potential outcomes for violations of academic integrity are serious and ordinarily include all of the following: required withdrawal (RQ), which means a failing grade in the course (with no refund), the suspension of registration privileges, and a notation on your transcript. Using sources responsibly https://www.extension.harvard.edu/resources-policies/resources/avoiding-plagiarism is an essential part of your Harvard education. We provide additional information about our expectations regarding academic integrity on our website. We invite you to review that information and to check your understanding of academic citation rules by completing two free online 15-minute tutorials that are also available on our site. (The tutorials are anonymous open-learning tools.) Accommodations for students with disabilities Harvard students needing academic adjustments or accommodations because of a documented disability must present their Faculty Letter from the Accessible Education Office (AEO) and speak with the professor by the end of the second week of the term, (fill in specific date). Failure to do so may result in the Course Head's inability to respond in a timely manner. All discussions will remain confidential, although Faculty are invited to contact AEO to discuss appropriate implementation. Harvard Extension School is committed to providing an inclusive, accessible academic community for students with disabilities and chronic health conditions. The Accessibility Services Office (ASO) https://www.extension.harvard.edu/resources-policies/accessibility-services-office-aso offers accommodations and supports to students with documented disabilities. If you have a need for accommodations or adjustments in your course, please contact the Accessibility Services Office by email at accessibility@extension.harvard.edu or by phone at 617-998-9640. Diversity and Inclusion Statement Data Science and Computer Science have historically been representative of only a small sliver of the population. This is despite the contributions of a diverse group of early pioneers - see Ada Lovelace, Dorothy Vaughan, and Grace Hopper for just a few examples. As educators, we aim to build a diverse, inclusive, and representative community offering opportunities in data science to those who have been historically marginalized. We will encourage learning that advances ethical data science, exposes bias in the way data science is used, and advances research into fair and responsible data science. We need your help to create a learning environment that supports a diversity of thoughts, perspectives, and experiences, and honors your identities (including but not limited to race, gender, class, sexuality, religion, ability, etc.) To help accomplish this: If you have a name and/or set of pronouns that differ from those in your official Harvard records, please let us know! If you feel like your performance in the class is being impacted by your experiences outside of class, please do not hesitate to come and talk with us. We want to be a resource for you. Remember that you can also submit anonymous feedback (which will lead to us making a general announcement to the class, if necessary, to address your concerns). If you prefer to speak with someone outside of the course, you may find helpful resources at the Harvard Office of Diversity and Inclusion. We (like many people) are still learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to us about it. As a participant in course discussions, you are expected to respect your classmates' diverse backgrounds and perspectives. Our course will discuss diversity, inclusion, and ethics in data science. Please contact us (in person or electronically) or submit anonymous feedback if you have any suggestions for how we can improve.","tags":"pages","url":"pages/syllabus.html"},{"title":"Advanced Section 1: Gaussian Mixture Models","text":"Slides GMM (PDF) Notebooks GMM (IPYNB)","tags":"a-sections","url":"a-sections/a-sec01/"},{"title":"Advanced Section 1: Gaussian Mixture Models","text":"Advanced Section: Gaussian Mixture Models CS 109B Spring, 2021 In [1]: ### Import basic libraries import numpy as np import scipy as sp import pandas as pd import sklearn as sk from sklearn.linear_model import LinearRegression import matplotlib.pyplot as plt % matplotlib inline Motivation for Latent Variable Models A Model for Birth Weights Recall our model for birth weigths, $Y_1,\\ldots, Y_N$. We posited that the birth weights are iid normally distributed with known $\\sigma&#94;2$, $Y_n \\sim \\mathcal{N}(\\mu, 1)$. Compare the maximum likelihood model and the Bayesian model for bith weight. Which model would you use to make clinical decisions? What's hard about this comparison? A Similarity Measure for Distributions: Kullback‚ÄìLeibler Divergence Visually comparing models to the empirical distribution of the data is impractical. Fortunately, there are a large number of quantitative measures for comparing two distributions, these are called divergence measures . For example, the Kullback‚ÄìLeibler (KL) Divergence is defined for two distributions $p(\\theta)$ and $q(\\theta)$ supported on $\\Theta$ as: $$ D_{\\text{KL}}[q \\,\\|\\, p] = \\int_{\\Theta} \\log\\left[\\frac{q(\\theta)}{p(\\theta)} \\right] q(\\theta)d\\theta $$ The KL-divergence $D_{\\text{KL}}[q \\,\\|\\, p]$ is bounded below by 0, which happens if and only if $q=p$. The KL-divergence has information theoretic interpretations that we will explore later in the course. Note: The KL-divergence is defined in terms of the pdf's of $p$ and $q$. If $p$ is a distribution from which we only have samples and not the pdf (like the empirical distribution), we can nontheless estimate $D_{\\text{KL}}[q \\,\\|\\, p]$. Techniques that estimate the KL-divergence from samples are called non-parametric . We will use them later in the course. Why is the KL bounded below by 0? First let's see why the answer isn't obvious. Recall that the KL divergence is the expected log ratio between two distribution : $$ D_{\\text{KL}} [q\\| p] = \\mathbb{E}_{q}\\left[ \\log \\frac{q}{p}\\right] $$ Now, we know that when $q$ is less than $p$ (i.e. $q/p < 1$) then the log can be an arbitrarily negative number. So it's not immediately obvious that the expected value of this fraction should always be non-negative! An intuitive explanation: Let the blue curve be q and the red be p. We have $q < p$ from $(-\\infty, 55)$, on this part of the domain $\\log(q/p)$ is negative. On $[55, \\infty)$, $\\log(q/p)$ is nonnegative. However, since we are sampling from $q$, and $q$'s mass is largely over $[55, \\infty)$, the log fraction $\\log(q/p$) will tend to be nonnegative. A formal argument: There are many proofs of the non-negativity of the KL. Ranging from the very complex to the very simple. Here is one that just involves a bit of algebra: We want to show that $D_{\\text{KL}}[q\\|p] \\geq 0$. Instead we'll show, equivalently, that $-D_{\\text{KL}}[q\\|p] \\leq 0$ (we're choosing show the statement about the negative KL, just so we can flip the fraction on the inside of the log and cancel terms): Class Membership as a Latent Variable We observe that there are three clusters in the data. We posit that there are three classes of infants in the study: infants with low birth weights, infants with normal birth weights and those with high birth weights. The numbers of infants in the classes are not equal. For each observation $Y_n$, we model its class membership $Z_n$ as a categorical variable, $$Z_n\\sim Cat(\\pi),$$ where $\\pi_i$ in $\\pi = [\\pi_1, \\pi_2, \\pi_3]$ is the class proportion. Note that we don't have the class membership $Z_n$ in the data! So $Z_n$ is called a latent variable . Depending on the class, the $n$-th birth weight $Y_n$ will have a different normal distribution, $$ Y_n | Z_n \\sim \\mathcal{N}\\left(\\mu_{Z_n}, \\sigma&#94;2_{Z_n}\\right) $$ where $\\mu_{Z_n}$ is one of the three class means $[\\mu_1, \\mu_2, \\mu_3]$ and $\\sigma&#94;2_{Z_n}$ is one of the three class variances $[\\sigma&#94;2_1, \\sigma&#94;2_2, \\sigma&#94;2_3]$. Common Latent Variable Models Latent Variable Models Models that include an observed variable $Y$ and at least one unobserved variable $Z$ are called latent variable models . In general, our model can allow $Y$ and $Z$ to interact in many different ways. Today, we will study models with one type of interaction: Gaussian Mixture Models (GMMs) In a Gaussian Mixture Model (GMM) , we posit that the observed data $Y$ is generated by a mixture, $\\pi=[\\pi_1, \\ldots, \\pi_K]$, of $K$ number of Gaussians with means $\\mu = [\\mu_1, \\ldots, \\mu_K]$ and covariances $\\Sigma = [\\Sigma_1, \\ldots, \\Sigma_K]$. For each observation $Y_n$ the class of the observation $Z_n$ is a latent variable that indicates which of the $K$ Gaussian is responsible for generating $Y_n$: \\begin{aligned} Z_n &\\sim Cat(\\pi),\\\\ Y_n | Z_n&\\sim \\mathcal{N}(\\mu_{Z_n}, \\Sigma_{Z_n}), \\end{aligned} where $n=1, \\ldots, N$ and $\\sum_{k=1}&#94;K \\pi_k = 1$. GMMs are examples of model based clustering - breaking up a data set into natural clusters based on a statistical model fitted to the data. Item-Response Models In item-response models , we measure an real-valued unobserved trait $Z$ of a subject by performing a series of experiments with binary observable outcomes, $Y$: \\begin{aligned} Z_n &\\sim \\mathcal{N}(\\mu, \\sigma&#94;2),\\\\ \\theta_n &= g(Z_n)\\\\ Y_n|Z_n &\\sim Ber(\\theta_n), \\end{aligned} where $n=1, \\ldots, N$ and $g$ is some fixed function of $Z_n$. Applications Item response models are used to model the way \"underlying intelligence\" $Z$ relates to scores $Y$ on IQ tests. Item response models can also be used to model the way \"suicidality\" $Z$ relates to answers on mental health surveys. Building a good model may help to infer when a patient is at psychiatric risk based on in-take surveys at points of care through out the health-care system. Factor Analysis Models In factor analysis models , we posit that the observed data $Y$ with many measurements is generated by a small set of unobserved factors $Z$: \\begin{aligned} Z_n &\\sim \\mathcal{N}(0, I),\\\\ Y_n|Z_n &\\sim \\mathcal{N}(\\mu + \\Lambda Z_n, \\Phi), \\end{aligned} where $n=1, \\ldots, N$, $Z_n\\in \\mathbb{R}&#94;{D'}$ and $Y_n\\in \\mathbb{R}&#94;{D}$. We typically assume that $D'$ is much smaller than $D$. Applications Factor analysis models are useful for biomedical data, where we typically measure a large number of characteristics of a patient (e.g. blood pressure, heart rate, etc), but these characteristics are all generated by a small list of health factors (e.g. diabetes, cancer, hypertension etc). Building a good model means we may be able to infer the list of health factors of a patient from their observed measurements. Maximum Likelihood Estimation for Latent Variable Models: Expectation Maximization Expectation Maximization: Estimating the MLE for Latent Variable Models Given a latent variable model $p(Y, Z| \\phi, \\theta) = p(Y | Z, \\phi) p(Z|\\theta)$, we are interested computing the MLE of parameters $\\phi$ and $\\theta$: \\begin{aligned} \\theta_{\\text{MLE}}, \\phi_{\\text{MLE}} &= \\underset{\\theta, \\phi}{\\mathrm{argmax}}\\; \\ell(\\theta, \\phi)\\\\ &= \\underset{\\theta, \\phi}{\\mathrm{argmax}}\\; \\log \\prod_{n=1}&#94;N \\int_{\\Omega_Z} p(y_n, z_n | \\theta, \\phi) dz\\\\ &= \\underset{\\theta, \\phi}{\\mathrm{argmax}}\\; \\log \\prod_{n=1}&#94;N \\int_{\\Omega_Z} p(y_n| z_n, \\phi)p(z_n| \\theta) dz \\end{aligned} where $\\Omega_Z$ is the domain of $Z$. Why is this an hard optimization problem? There are two major problems: the product in the integrand gradients cannot be past the integral (i.e. we cannot easily compute the gradient to solve the optimization problem). We solve these two problems by: pushing the log past the integral so that it can be applied to the integrand (Jensen's Inequality) introducing an auxiliary variables $q(Z_n)$ to allow the gradient to be pushed past the integral. \\begin{aligned} \\underset{\\theta, \\phi}{\\mathrm{max}}\\; \\ell(\\theta, \\phi) &= \\underset{\\theta, \\phi, q}{\\mathrm{max}}\\; \\log \\prod_{n=1}&#94;N\\int_{\\Omega_Z} \\left(\\frac{p(y_n, z_n|\\theta, \\phi)}{q(z_n)}q(z_n)\\right) dz\\\\ &= \\underset{\\theta, \\phi, q}{\\mathrm{max}}\\; \\log\\,\\prod_{n=1}&#94;N\\mathbb{E}_{Z\\sim q(Z)} \\left[ \\frac{p(y_n, Z|\\theta, \\phi)}{q(Z)}\\right]\\\\ &= \\underset{\\theta, \\phi, q}{\\mathrm{max}}\\; \\sum_{n=1}&#94;N \\log \\mathbb{E}_{Z\\sim q(Z)} \\left[\\,\\left( \\frac{p(y_n, Z|\\theta, \\phi)}{q(Z)}\\right)\\right]\\\\ &\\geq \\underset{\\theta, \\phi, q}{\\mathrm{max}}\\; \\underbrace{\\sum_{n=1}&#94;N\\mathbb{E}_{Z_n\\sim q(Z)} \\left[ \\log\\,\\left(\\frac{p(y_n, Z_n|\\theta, \\phi)}{q(Z_n)}\\right)\\right]}_{ELBO(\\theta, \\phi)}, \\quad (\\text{Jensen's Inequality})\\\\ \\end{aligned} We call $\\sum_{n=1}&#94;N\\mathbb{E}_{Z_n\\sim q(Z)} \\left[ \\log\\,\\left(\\frac{p(y_n, Z_n|\\theta, \\phi)}{q(Z)}\\right)\\right]$ the Evidence Lower Bound (ELBO). Note that maximizing the ELBO will yield a lower bound of the maximum value of the log likelihood. Although the optimal point of the ELBO may not be the optimal point of the log likelihood , we nontheless prefer to optimize the ELBO because the gradients, with respect to $\\theta, \\phi$, of the ELBO are easier to compute: $$ \\nabla_{\\theta, \\phi} ELBO(\\theta, \\phi) = \\nabla_{\\theta, \\phi}\\left[ \\sum_{n=1}&#94;N\\mathbb{E}_{Z_n\\sim q(Z)} \\left[ \\log\\,\\left(\\frac{p(y_n, Z_n|\\theta, \\phi)}{q(Z_n)}\\right)\\right]\\right] = \\sum_{n=1}&#94;N\\mathbb{E}_{Z_n\\sim q(Z)} \\left[ \\nabla_{\\theta, \\phi} \\left( \\log\\,\\left(\\frac{p(y_n, Z_n|\\theta, \\phi)}{q(Z_n)}\\right)\\right)\\right] $$ Note that we can push the gradient $\\nabla_{\\theta, \\phi}$ past the expectation $\\mathbb{E}_{Z_n\\sim q(Z)}$ since the expectation is not computed with respect to our optimization variables! Rather than optimizing the ELBO over all variables $\\theta, \\phi, q$ (this would be hard), we optimize one set of variables at a time: Step I: the M-step Optimize the ELBO with respect to $\\theta, \\phi$: \\begin{aligned} \\theta&#94;*, \\phi&#94;* = \\underset{\\theta, \\phi}{\\mathrm{max}}\\; ELBO(\\theta, \\phi, q) &= \\underset{\\theta, \\phi}{\\mathrm{max}}\\; \\sum_{n=1}&#94;N\\mathbb{E}_{Z_n\\sim q(Z)} \\left[ \\log\\,\\left(\\frac{p(y_n, Z_n|\\theta, \\phi)}{q(Z_n)}\\right)\\right]\\\\ &= \\underset{\\theta, \\phi}{\\mathrm{max}}\\; \\sum_{n=1}&#94;N \\int_{\\Omega_Z} \\log\\,\\left(\\frac{p(y_n, z_n|\\theta, \\phi)}{q(z_n)}\\right)q(z_n) dz_n\\\\ &= \\underset{\\theta, \\phi}{\\mathrm{max}}\\; \\sum_{n=1}&#94;N \\int_{\\Omega_Z} \\log\\,\\left(p(y_n, z_n|\\theta, \\phi)\\right) q(z_n)dz_n - \\underbrace{\\int_{\\Omega_Z} \\log \\left(q(z_n)\\right)q(z_n) dz_n}_{\\text{constant with respect to }\\theta, \\phi}\\\\ &\\equiv \\underset{\\theta, \\phi}{\\mathrm{max}}\\;\\sum_{n=1}&#94;N \\int_{\\Omega_Z} \\log\\,\\left(p(y_n, z_n|\\theta, \\phi)\\right) q(z_n)dz_n\\\\ &= \\underset{\\theta, \\phi}{\\mathrm{max}}\\;\\sum_{n=1}&#94;N \\mathbb{E}_{Z_n\\sim q(Z)} \\left[ \\log\\left(p(y_n, z_n|\\theta, \\phi)\\right)\\right] \\end{aligned} Step II: the E-step Optimize the ELBO with respect to $q$: \\begin{aligned} q&#94;*(Z_n) = \\underset{q}{\\mathrm{argmax}}\\;\\left(\\underset{\\theta, \\phi}{\\mathrm{argmax}}\\; ELBO(\\theta, \\phi, q) \\right) = \\underset{q}{\\mathrm{argmax}}\\; ELBO(\\theta&#94;*, \\phi&#94;*, q) \\end{aligned} Rather than optimizing the ELBO with respect to $q$, which seems hard, we will argue that optimizing the ELBO is equivalent to optimizing another function of $q$, one whose optimum is easy for us to compute. Note: We can recognize the difference between the log likelihood and the ELBO as a function we've seen: \\begin{aligned} \\ell(\\theta, \\phi) - ELBO(\\theta, \\phi, q) &= \\sum_{n=1}&#94;N \\log p(y_n| \\theta, \\phi) - \\sum_{n=1}&#94;N \\int_{\\Omega_Z} \\log\\left(\\frac{p(y_n, z_n|\\theta, \\phi)}{q(z_n)}\\right)q(z_n) dz_n\\\\ &= \\sum_{n=1}&#94;N \\int_{\\Omega_Z} \\log\\left(p(y_n| \\theta, \\phi)\\right) q(z_n) dz_n - \\sum_{n=1}&#94;N \\int_{\\Omega_Z} \\log\\left(\\frac{p(y_n, z_n|\\theta, \\phi)}{q(z_n)}\\right)q(z_n) dz_n\\\\ &= \\sum_{n=1}&#94;N \\int_{\\Omega_Z} \\left(\\log\\left(p(y_n| \\theta, \\phi)\\right) - \\log\\left(\\frac{p(y_n, z_n|\\theta, \\phi)}{q(z_n)}\\right) \\right)q(z_n) dz_n\\\\ &= \\sum_{n=1}&#94;N \\int_{\\Omega_Z} \\log\\left(\\frac{p(y_n| \\theta, \\phi)q(z_n)}{p(y_n, z_n|\\theta, \\phi)} \\right)q(z_n) dz_n\\\\ &= \\sum_{n=1}&#94;N \\int_{\\Omega_Z} \\log\\left(\\frac{q(z_n)}{p(z_n| y_n, \\theta, \\phi)} \\right)q(z_n) dz_n, \\quad\\left(\\text{Baye's Rule: } \\frac{p(y_n, z_n|\\theta, \\phi)}{p(y_n| \\theta, \\phi)} = p(z_n| y_n, \\theta, \\phi)\\right)\\\\ &= \\sum_{n=1}&#94;N D_{\\text{KL}} \\left[ q(Z_n) \\| p(Z_n| Y_n, \\theta, \\phi)\\right]. \\end{aligned} Since $\\ell(\\theta, \\phi)$ is a constant, the difference $\\sum_{n=1}&#94;N D_{\\text{KL}} \\left[ q(Z_n) \\| p(Z_n| Y_n, \\theta, \\phi)\\right] = \\ell(\\theta, \\phi) - ELBO(\\theta, \\phi, q)$ descreases when $ELBO(\\theta, \\phi, q)$ increases (and vice versa). Thus, maximizing the ELBO is equivalent to minimizing $D_{\\text{KL}} \\left[ q(Z_n) \\| p(Y_n| Z_n, \\theta, \\phi)\\right]$: $$ \\underset{q}{\\mathrm{argmax}}\\, ELBO(\\theta, \\phi, q) = \\underset{q}{\\mathrm{argmin}}\\sum_{n=1}&#94;N D_{\\text{KL}} \\left[ q(Z_n) \\| p(Z_n| Y_n, \\theta, \\phi)\\right]. $$ Thus, we see that \\begin{aligned} q&#94;*(Z_n) = \\underset{q}{\\mathrm{argmax}}\\; ELBO(\\theta&#94;*, \\phi&#94;*, q) = \\underset{q}{\\mathrm{argmin}}\\sum_{n=1}&#94;N D_{\\text{KL}} \\left[ q(Z_n) \\| p(Z_n| Y_n, \\theta, \\phi)\\right] = p(Z_n| Y_n, \\theta, \\phi) \\end{aligned} That is, we should set the optimal distribution $q$ to be the posterior $p(Z_n| Y_n, \\theta, \\phi)$. Iteration Of course, we know that optimizing a function with respect to each variable is not sufficient for finding the global optimum over all the variables, considered together! Thus, performing one E-step and one M-step is not enough to maximize the ELBO. We need to repeat the two steps over and over. Question: Why don't gradients commute with expectation? We have the following property of expectations: $$ \\nabla_z \\mathbb{E}_{x\\sim p(x)}[f(x, z)] = \\mathbb{E}_{x\\sim p(x)}[ \\nabla_z f(x, z)] $$ That is, when the gradient is with respect to a variable that does not appear in the distribution with respect to which you are taking the expectation, then you can push the gradient past the expectation. The intuition: the gradient with respect to $z$ is computing the changes in a function by making infinitesimally small changes to $z$, the expectation is computing the average value of a function by sampling $x$ from a distribution that does not depend on $z$. Each operation is making an independent change to two different variables and hence can be done in any order. Why can't you do this in general? I.e. why is it that, $$ \\nabla_z\\mathbb{E}_{x\\sim p(x|z)}[f(x, z)] \\neq \\mathbb{E}_{x\\sim p(x|z)}[ \\nabla_z f(x, z)]?$$ The intuition: the gradient with respect to z is computing the changes in a function by making infinitesimally small changes to z, which in turn affects the samples produced by p(x|z), these samples finally affect the output of f. This is a chain of effects and the order matters. The formal proof: Consider the following case, $$ p(x\\vert z) = (z+1)x&#94;z,\\; x\\in [0, 1] $$ and $$ f(x, z) = xzf ( x , z ) = x z. $$ Then, we have $$\\nabla_z \\mathbb{E}_{x\\sim p(x|z)} [f(x, z)] = \\nabla_z \\int_0&#94;1 f(x, z) p(x|z) dx = \\nabla_z\\int_0&#94;1 xz \\cdot (z+1)x&#94;z dx = \\nabla_z z (z+1)\\int_0&#94;1x&#94;{z+1} dx = \\nabla_z \\frac{z (z+1)}{z+2} [x&#94;{z+2} ]_0&#94;1 = \\nabla_z \\frac{z (z+1)}{z+2} = \\frac{z&#94;2 + 4z + 2}{(z+2)&#94;2} $$ On the other hand, we have $$ \\mathbb{E}_{x\\sim p(x|z)}\\left[ \\nabla_z f(x, z) \\right] = \\int_0&#94;1 \\nabla_z[ xz] (z+1)x&#94;zdx = \\int_0&#94;1(z+1)x&#94;{z+1}dx = \\frac{z+1}{z+2} [x&#94;{z+2}]_0&#94;1 = \\frac{z+1}{z+2}. $$ Note that: $$ \\nabla_z \\mathbb{E}_{x\\sim p(x|z)} [f(x, z)] = \\frac{z&#94;2 + 4z+ 2}{(z+2)&#94;2} \\neq \\frac{z+1}{z+2} = \\mathbb{E}_{x\\sim p(x|z)}\\left[ \\nabla_z f(x, z) \\right]. $$ Question: Why do we need to maximize the ELBO with respect to q? Recall that in the derivation of the ELBO, we first introduced an auxiliary variable q to rewrite the observed log-likelihood: $$ \\log p(y|\\theta, \\phi) = \\log \\int_\\Omega p(y, z| \\theta, \\phi) dz = \\log \\int_\\Omega \\frac{p(y, z| \\theta, \\phi}{q(z)}q(z) dz = \\log \\mathbb{E}_{q(z)} \\left[ \\frac{p(y, z|\\theta, \\phi)}{q(z)} \\right] $$ Again, the reason why we do this is because: when we eventually take the gradient wrt to $\\theta, \\phi$ during optimization we can use the identity $$ \\nabla_{\\theta, \\phi} \\mathbb{E}_{q(z)}\\left[\\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right] = \\mathbb{E}_{q(z)}\\left[\\nabla_{\\theta, \\phi} \\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right] $$ At this point, there is no need to maximize over q , that is: $$ \\max_{\\theta, \\phi, q}\\log \\mathbb{E}_{q(z)}\\left[\\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right] = \\max_{\\theta, \\phi}\\log \\mathbb{E}_{q(z)}\\left[\\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right] $$ The $q$ cancels and has no effect on the outcome or process of the optimization (but you can't just choose any $q$ you want - can you see what are the constraints on $q$?). Now, the problem is that the log is on the outside of the expectation. This isn't a problem in the sense that we don't know how to take the derivative of a logarithm of a complex function (this is just the chain rule ), the problem is that $$ \\nabla_{\\phi, \\theta} \\frac{p(y, z|\\theta, \\phi)}{q(z)} $$ can be very complex (since p and q are pdf's) and so over all the gradient of the log expectation is not something you can compute roots for. Here is where we push the log inside the expectation using Jensen's inequality: $$ \\log \\mathbb{E}_{q(z)}\\left[\\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right] \\geq \\mathbb{E}_{q(z)}\\left[\\log \\left(\\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right)\\right] \\overset{\\text{def}}{=} ELBO(\\phi, \\theta, q) $$ When we push the log inside the expectation, we obtain the E vidence L ower Bo und (ELBO). Now, for any choice of $q$, we always have: $$ \\max_{\\theta, \\phi}\\log \\mathbb{E}_{q(z)}\\left[\\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right] \\geq \\max_{\\theta, \\phi}ELBO(\\phi, \\theta, q) $$ But the ELBO is not necessarily a tight bound (i.e. maximizing the ELBO can be very far from maximizing the log-likelihood!)! In particular, some choices of $q$ might give you a tighter bound on the log-likelihood than others. Thus, we want to select the $q$ that give us the tightest bound: $$ \\max_{\\theta, \\phi}\\log \\mathbb{E}_{q(z)}\\left[\\frac{p(y, z|\\theta, \\phi)}{q(z)}\\right] \\geq \\max_{\\theta, \\phi, q}ELBO(\\phi, \\theta, q). $$ The Expectation Maximization Algorithm The exepectation maximization (EM) algorithm maximize the ELBO of the model, Initialization: Pick $\\theta_0$, $\\phi_0$. Repeat $i=1, \\ldots, I$ times: E-Step: $$q_{\\text{new}}(Z_n) = \\underset{q}{\\mathrm{argmax}}\\; ELBO(\\theta_{\\text{old}}, \\phi_{\\text{old}}, q) = p(Z_n|Y_n, \\theta_{\\text{old}}, \\phi_{\\text{old}})$$ M-Step: \\begin{aligned} \\theta_{\\text{new}}, \\phi_{\\text{new}} &= \\underset{\\theta, \\phi}{\\mathrm{argmax}}\\; ELBO(\\theta, \\phi, q_{\\text{new}})\\\\ &= \\underset{\\theta, \\phi}{\\mathrm{argmax}}\\; \\sum_{n=1}&#94;N\\mathbb{E}_{Z_n\\sim p(Z_n|Y_n, \\theta_{\\text{old}}, \\phi_{\\text{old}})}\\left[\\log \\left( p(y_n, Z_n | \\phi, \\theta\\right) \\right]. \\end{aligned} The Auxiliary Function We often denote the expectation in the M-step by $Q\\left(\\theta, \\phi| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right)$ $$ Q\\left(\\theta, \\phi| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right) = \\sum_{n=1}&#94;N\\mathbb{E}_{Z_n\\sim p(Z_n|Y_n, \\theta_{\\text{old}}, \\phi_{\\text{old}})}\\left[\\log \\left( p(y_n, Z_n | \\phi, \\theta\\right) \\right] $$ and call $Q$ the auxiliary function. Frequently, the EM algorithm is equivalently presented as E-step: compute the auxiliary function: $Q\\left(\\theta, \\phi| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right)$ M-step: maximize the auxiliary function: $\\theta&#94;{\\text{new}}, \\phi&#94;{\\text{new}} = \\underset{\\theta, \\phi}{\\mathrm{argmax}}\\,Q\\left(\\theta, \\phi| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right)$. The log of the joint distribution $\\prod_{n=1}&#94;N p(Z_n, Y_n, \\theta, \\phi)$ is called the complete data log-likelihood (since it is the likelihood of both observed and latent variables), whereas $\\log \\prod_{n=1}&#94;N p(Y_n| \\theta, \\phi)$ is called the observed data log-likelihood (since it is the likelihood of only the observed variable). The auxiliary function presentation of EM is easy to interpret: In the E-step, you fill in the latent variables in the complete data log-likelihood using \"average\" values, this leaves just an estimate of the observed log-likelihood. In the M-step, you find parameters $\\phi$ and $\\theta$ that maximizes your estimate of the observed log-likelihood. We chose to derive EM via the ELBO in this lecture because it makes an explicit connection between the EM algorithm for estimating MLE and variational inference method for approximating the posterior of Bayesian models. It is, however, worthwhile to derive EM using the auxiliary function $Q$, as $Q$ makes it convient for us to prove properties of the EM algorithm. Monotonicity and Convergence of EM Before we run off estimating MLE parameters of latent variable models with EM, we need to sanity check two points: (Monotonicity) we need to know that repeating the E, M-steps will never decrease the ELBO! (Convergence) we need to know that at some point the EM algorithm will naturally terminate (the algorithm will cease to update the parameters). We first prove the monotonicity of EM. Consider the difference between $\\ell(\\theta, \\phi) - \\ell(\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})$, i.e. the amount by which the log-likelihood can increase or decrease by going from $\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}$ to $\\theta, \\phi$: \\begin{aligned} \\ell(\\theta, \\phi) - \\ell(\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}) &= \\sum_{n=1}&#94;N\\log \\left[ \\frac{p(y_n|\\theta, \\phi)}{p(y_n| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})}\\right]\\\\ &= \\sum_{n=1}&#94;N \\log\\int \\frac{p(y_n, z_n|\\theta, \\phi)}{p(y_n| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})} dz_n\\\\ &= \\sum_{n=1}&#94;N \\log\\int \\frac{p(y_n, z_n|\\theta, \\phi)}{p(y_n| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}) p(z_n|y_n, \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})}p(z_n|y_n, \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}) dz_n\\\\ &= \\sum_{n=1}&#94;N \\log\\int \\frac{p(y_n, z_n|\\theta, \\phi)}{p(y_n, z_n| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})}p(z_n|y_n, \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}) dz_n\\\\ &= \\sum_{n=1}&#94;N \\log \\mathbb{E}_{p(z_n|y_n, \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})} \\left[\\frac{p(y_n, z_n|\\theta, \\phi)}{p(y_n, z_n| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})}\\right]\\\\ &\\geq \\sum_{n=1}&#94;N \\mathbb{E}_{p(z_n|y_n, \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})} \\log\\left[\\frac{p(y_n, z_n|\\theta, \\phi)}{p(y_n, z_n| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})}\\right]\\\\ &= \\sum_{n=1}&#94;N \\mathbb{E}_{p(z_n|y_n, \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})} \\left[\\log p(y_n, z_n|\\theta, \\phi) - \\log p(y_n, z_n| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})\\right]\\\\ &= \\sum_{n=1}&#94;N \\mathbb{E}_{p(z_n|y_n, \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})} \\left[\\log p(y_n, z_n|\\theta, \\phi)\\right] - \\sum_{n=1}&#94;N \\mathbb{E}_{p(z_n|y_n, \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})}\\left[ \\log p(y_n, z_n| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})\\right]\\\\ &= Q\\left(\\theta, \\phi| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right) - Q\\left(\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right) \\end{aligned} Thus, when we maximize the gain in log-likelihood going from $\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}$ to $\\theta, \\phi$, we get: \\begin{aligned} \\underset{\\theta, \\phi}{\\max} \\left[\\ell(\\theta, \\phi) - \\ell(\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})\\right] \\geq \\underset{\\theta, \\phi}{\\max} \\left[Q\\left(\\theta, \\phi| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right) - Q\\left(\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right)\\right] \\end{aligned} or equivalently, \\begin{aligned} \\underset{\\theta, \\phi}{\\max} \\left[\\ell(\\theta, \\phi)\\right] - \\ell(\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}) \\geq \\underset{\\theta, \\phi}{\\max} \\left[Q\\left(\\theta, \\phi| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right)\\right] - Q\\left(\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right). \\end{aligned} Note that the above max is always greater than or equal to zero: $$\\underset{\\theta, \\phi}{\\max} \\left[Q\\left(\\theta, \\phi| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right)\\right] - Q\\left(\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right) \\geq 0$$ since we can always maintain the status quo by choosing $theta = \\theta&#94;{\\text{old}}$ $\\phi = \\phi&#94;{\\text{old}}$: $$ Q\\left(\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right) - Q\\left(\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right) = 0.$$ Thus, we have that by maximizing $Q\\left(\\theta, \\phi| \\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}}\\right)$, we ensure that $\\ell(\\theta, \\phi) - \\ell(\\theta&#94;{\\text{old}}, \\phi&#94;{\\text{old}})\\geq 0$ in each iteration of EM. If the likelihood of the model is bounded above (i.e. $\\ell(\\theta, \\phi) \\leq M$ for some constant $M$), then EM is guaranteed to convergence. This is because we've proved that EM increases (or maintains) log-likelihood in each iteration, therefore, if $\\ell(\\theta, \\phi)$ is bounded, the process must converge. Disclaimer: Although EM converges for bounded likelihoods, it is not guaranteed to converge to the global max of the log-likelihood! Maximizing a lower bound of a function does not necessarily maximize the function itself! Often time, EM converges to local optima of the likelihood function and the point to which it converges may be very sensitive to initialization. We will study this kind of behaviour in more detail when we cover non-convex optimization later in the course. Example: EM for the Gaussian Mixture Model of Birth Weight The Gaussian mixture model for the birth weight data has 3 Gaussians with meand $\\mu = [\\mu_1, \\mu_2, \\mu_3]$ and variances $\\sigma&#94;2 = [\\sigma_1&#94;2, \\sigma_2&#94;2, \\sigma_3&#94;2]$, and the model is defined as: \\begin{aligned} Z_n &\\sim Cat(\\pi),\\\\ Y_n | Z_n &\\sim \\mathcal{N}(\\mu_{Z_n}, \\sigma&#94;2_{Z_n}), \\end{aligned} where $n=1, \\ldots, N$ and $\\sum_{k=1}&#94;3 \\pi_k = 1$. The E-Step The E-step in EM computes the distribution: $$q_{\\text{new}}(Z_n) = \\underset{q}{\\mathrm{argmax}}\\; ELBO(\\mu_{i-1}, \\sigma&#94;2_{i-1}, \\pi_{i_1}, q) = p(Z_n|Y_n, \\mu_{\\text{old}}, \\sigma&#94;2_{\\text{old}}, \\pi_{\\text{old}}).$$ Since $Z_n$ is a label, $p(Z_n|Y_n, \\ldots)$ is a categorical distribution, with the probability of $Z_n=k$ given by: $$ p(Z_n = k|Y_n, \\mu_{\\text{old}}, \\sigma&#94;2_{\\text{old}}, \\pi_{\\text{old}}) = \\frac{p(y_n|Z_n = k, \\mu_{\\text{old}}, \\sigma&#94;2_{\\text{old}})p(Z_n=k | \\pi_{\\text{old}})}{\\sum_{k=1}&#94;K p(y|Z_n = k, \\mu_{\\text{old}}, \\sigma&#94;2_{\\text{old}})p(Z_n=k | \\pi_{\\text{old}})} = \\underbrace{\\frac{\\pi_{k, \\text{old}}\\,\\mathcal{N}(y_n; \\mu_{k, \\text{old}}, \\sigma&#94;2_{k, \\text{old}})}{\\mathcal{Z}}}_{r_{n, k}}, $$ where $\\mathcal{Z} = \\sum_{k=1}&#94;K \\pi_{k, \\text{old}}\\,\\mathcal{N}(y_n; \\mu_{k, \\text{old}}, \\sigma&#94;2_{k, \\text{old}})$. Example: EM for the Gaussian Mixture Model of Birth Weight Setting Up the M-Step The M-step in EM maximize the following: $$\\underset{\\mu, \\sigma&#94;2, \\pi}{\\mathrm{argmax}}\\; ELBO(\\mu, \\sigma&#94;2, \\pi, q_{\\text{new}}) = \\underset{\\mu, \\sigma&#94;2, \\pi}{\\mathrm{argmax}}\\; \\sum_{n=1}&#94;N\\mathbb{E}_{Z_n\\sim p(Z_n|Y_n, \\mu_{k, \\text{old}}, \\sigma&#94;2_{k, \\text{old}})}\\left[\\log \\left( p(y_n, Z_n | \\mu, \\sigma&#94;2, \\pi\\right) \\right].$$ If we expand the expectation a little, we get: \\begin{aligned} \\sum_{n=1}&#94;N\\mathbb{E}_{Z_n\\sim p(Z_n|Y_n, \\mu_{\\text{old}}, \\sigma&#94;2_{\\text{old}}, \\pi_{\\text{old}})}\\left[\\log \\left(p(y_n, Z_n | \\mu, \\sigma&#94;2, \\pi) \\right) \\right] &= \\sum_{n=1}&#94;N \\underbrace{\\sum_{n=1}&#94;K \\log \\left(\\underbrace{ p(y_n| Z_n=k, \\mu, \\sigma&#94;2) p(Z_n=k| \\pi)}_{\\text{factoring the joint }p(y_n, Z_n| \\ldots) } \\right) p(Z_n=k|y_n, \\theta_{\\text{old}}, \\phi_{\\text{old}})}_{\\text{expanding the expectation}}\\\\ &=\\sum_{n=1}&#94;N \\sum_{k=1}&#94;K \\underbrace{r_{n, k}}_{p(Z_n=k|y_n, \\theta_{\\text{old}}, \\phi_{\\text{old}})} \\left[\\log \\underbrace{\\mathcal{N}(y_n; \\mu_k, \\sigma&#94;2_k)}_{p(y_n| Z_n=k, \\mu, \\sigma&#94;2)} + \\log \\underbrace{\\pi_k}_{p(Z_n=k| \\pi)}\\right]\\\\ &= \\underbrace{\\sum_{n=1}&#94;N \\sum_{k=1}&#94;K r_{n, k} \\log \\mathcal{N}(y_n; \\mu_k, \\sigma&#94;2_k)}_{\\text{Term #1}} + \\underbrace{\\sum_{n=1}&#94;N \\sum_{k=1}&#94;K r_{n, k}\\pi_k}_{\\text{Term #2}} \\end{aligned} We can maximize each Term #1 and Term #2 individually. Example: EM for the Gaussian Mixture Model of Birth Weight Solving the M-Step We see that the optimization problem in the M-step: $\\mu_{\\text{new}}, \\sigma&#94;2_{\\text{new}}, \\pi_{\\text{new}} = \\underset{\\mu, \\sigma&#94;2, \\pi}{\\mathrm{argmax}}\\; ELBO(\\mu, \\sigma&#94;2, \\pi, q_{\\text{new}})$ is equivalent to two problems \\begin{aligned} &1.\\quad \\underset{\\mu, \\sigma&#94;2}{\\mathrm{argmax}}\\; \\sum_{n=1}&#94;N \\sum_{k=1}&#94;K r_{n, k} \\log \\mathcal{N}(y_n; \\mu_k, \\sigma&#94;2_k)\\\\ &2.\\quad \\underset{\\pi}{\\mathrm{argmax}}\\; \\sum_{n=1}&#94;N \\sum_{k=1}&#94;K r_{n, k}\\pi_k \\end{aligned} We can solve each optimization problem analytically by finding stationary points of the gradient (or the Lagrangian): $\\mu_{\\text{new}} = \\frac{1}{ \\sum_{n=1}&#94;N r_{n, k}} \\sum_{n=1}&#94;N r_{n, k} y_n$ $\\sigma&#94;2_{\\text{new}} = \\frac{1}{ \\sum_{n=1}&#94;N r_{n, k}} \\sum_{n=1}&#94;N r_{n, k} (y_n - \\mu_{\\text{new}})&#94;2$ $\\pi_{\\text{new}} = \\frac{\\sum_{n=1}&#94;N r_{n, k}}{N}$ Example: EM for the Gaussian Mixture Model of Birth Weight All Together Initialization: Pick any $\\pi$, $\\mu$, $\\sigma&#94;2$ E-Step: Compute $r_{n, k} = \\displaystyle\\frac{\\pi_{k, \\text{old}}\\,\\mathcal{N}(y_n; \\mu_{k, \\text{old}}, \\sigma&#94;2_{k, \\text{old}})}{\\mathcal{Z}}$, where $\\mathcal{Z} = \\sum_{k=1}&#94;K \\pi_{k, \\text{old}}\\,\\mathcal{N}(y_n; \\mu_{k, \\text{old}}, \\sigma&#94;2_{k, \\text{old}})$. M-Step: Compute model parameters: $\\mu_{\\text{new}} = \\frac{1}{ \\sum_{n=1}&#94;N r_{n, k}} \\sum_{n=1}&#94;N r_{n, k} y_n$ $\\sigma&#94;2_{\\text{new}} = \\frac{1}{ \\sum_{n=1}&#94;N r_{n, k}} \\sum_{n=1}&#94;N r_{n, k} (y_n - \\mu_{\\text{new}})&#94;2$ $\\pi_{\\text{new}} = \\frac{\\sum_{n=1}&#94;N r_{n, k}}{N}$ Implementing EM for the Gaussian Mixture Model of Birth Weight In [2]: #Generate data N = 2000 pis = [ 0.2 , 0.6 , 0.2 ] mus = [ 4.3 , 6 , 7.8 ] sigmas = [ 0.5 ** 2 , 0.7 ** 2 , 0.5 ** 2 ] K = 3 zs = np . random . choice ( np . arange ( K ), size = N , p = pis ) y = np . array ([ np . random . normal ( mus [ z ], sigmas [ z ] ** 0.5 , 1 )[ 0 ] for z in zs ]) #initialization mu_init = [ 2 , 4 , 5 ] sigma_init = [ 2. , 2. , 2. ] pi_init = [ 0.33 , 0.33 , 0.33 ] #implement EM mu_current = mu_init sigma_current = sigma_init pi_current = pi_init log_lkhd = [] total_iter = 1500 threshold = 1e-10 mu_diff = 1. pi_diff = 1. sigma_diff = 1. i = 0 while i < total_iter and mu_diff > threshold and pi_diff > threshold and sigma_diff > threshold : #E-step r_unnormalized = np . array ([( pi_current [ k ] * sp . stats . norm ( mu_current [ k ], sigma_current [ k ] ** 0.5 ) . pdf ( y )) for k in range ( K )]) . T r = r_unnormalized / r_unnormalized . sum ( axis = 1 ) . reshape (( - 1 , 1 )) #M-step mu_next = np . array ([ 1. / r [:, k ] . sum () * ( r [:, k ] * y ) . sum () for k in range ( K )]) sigma_next = np . array ([ 1. / r [:, k ] . sum () * ( r [:, k ] * ( y - mu_next [ k ]) ** 2 ) . sum () for k in range ( K )]) pi_next = r . sum ( axis = 0 ) / r . shape [ 0 ] #compute log observed likelihood if i % 100 == 0 : print ( 'iteration ' , i ) ll = 0 for n in range ( len ( y )): ll += np . log ( np . sum ([ sp . stats . norm ( mu_next [ k ], sigma_next [ k ] ** 0.5 ) . pdf ( y [ n ]) * pi_next [ k ] for k in range ( K )])) log_lkhd . append ( ll ) #convergence check mu_diff = np . linalg . norm ( mu_next - mu_current ) pi_diff = np . linalg . norm ( pi_next - pi_current ) sigma_diff = np . linalg . norm ( sigma_next - sigma_current ) #update parameters mu_current = mu_next sigma_current = sigma_next pi_current = pi_next i += 1 x = np . linspace ( y . min (), y . max (), 100 ) iteration 0 iteration 100 iteration 200 iteration 300 iteration 400 iteration 500 iteration 600 iteration 700 iteration 800 iteration 900 iteration 1000 iteration 1100 iteration 1200 iteration 1300 iteration 1400 In [3]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) ax . hist ( y , bins = 60 , density = True , color = 'gray' , alpha = 0.5 , label = 'histogram of birth weights' ) ax . plot ( x , pi_current [ 0 ] * sp . stats . norm ( mu_current [ 0 ], sigma_current [ 0 ] ** 0.5 ) . pdf ( x ), color = 'red' , label = 'First Gaussian' ) ax . plot ( x , pi_current [ 1 ] * sp . stats . norm ( mu_current [ 1 ], sigma_current [ 1 ] ** 0.5 ) . pdf ( x ), color = 'blue' , label = 'Second Gaussian' ) ax . plot ( x , pi_current [ 2 ] * sp . stats . norm ( mu_current [ 2 ], sigma_current [ 2 ] ** 0.5 ) . pdf ( x ), color = 'green' , label = 'Third Gaussian' ) ax . set_title ( 'GMM for Birth Weights' ) ax . legend ( loc = 'best' ) plt . show () Example: EM for Gaussian Mixture Models (Multivariate) Recall that our Gaussian mixture model, of $K$ number of Gaussians with means $\\mu = [\\mu_1, \\ldots, \\mu_K]$ and covariances $\\Sigma = [\\Sigma_1, \\ldots, \\Sigma_K]$, is defined as: \\begin{aligned} Z_n &\\sim Cat(\\pi),\\\\ Y_n &\\sim \\mathcal{N}(\\mu_{Z_n}, \\Sigma_{Z_n}), \\end{aligned} where $n=1, \\ldots, N$ and $\\sum_{k=1}&#94;K \\pi_k = 1$. We derive the updates for $\\pi$, $\\mu$ and $\\Sigma$ for the EM algorithm E-step: $$ q_{\\text{new}} = p(Z_n|y_n, \\pi_{\\text{old}}, \\mu_{\\text{old}}, \\Sigma_{\\text{old}}) = \\frac{p(y_n|Z_n, \\mu_{\\text{old}}, \\Sigma_{\\text{old}})p(Z_n|\\pi_{\\text{old}})}{\\int p(y_n|z_n, \\mu_{\\text{old}}, \\Sigma_{\\text{old}})p(z_n|\\pi_{\\text{old}}) dz_n} $$ Since $Z_n$ is a categorical variable, we compute the probability of $Z_n = k$ separately: $$ p(Z_n = k|y_n, \\pi_{\\text{old}}, \\mu_{\\text{old}}, \\Sigma_{\\text{old}}) = \\frac{p(y_n|Z_n = k, \\mu_{\\text{old}}, \\Sigma_{\\text{old}})p(Z_n=k | \\pi_{\\text{old}})}{\\sum_{k=1}&#94;K p(y|Z_n = k, \\mu_{\\text{old}}, \\Sigma_{\\text{old}})p(Z_n=k | \\pi_{\\text{old}})} = \\underbrace{\\frac{\\pi_{k, \\text{old}}\\,\\mathcal{N}(y_n; \\mu_{k, \\text{old}}, \\Sigma_{k, \\text{old}})}{\\mathcal{Z}}}_{r_{n, k}} $$ where $\\mathcal{Z} = \\sum_{k=1}&#94;K \\pi_{k, \\text{old}}\\,\\mathcal{N}(y_n; \\mu_{k, \\text{old}}, \\Sigma_{k, \\text{old}})$. Thus, $q_{\\text{new}}(Z_n)$ is a categorical distribution $Cat([r_{n, 1}, \\ldots, r_{n, K}])$. M-Step: \\begin{aligned} \\mu_{\\text{new}}, \\Sigma_{\\text{new}}, \\pi_{\\text{new}} &= \\underset{\\mu, \\Sigma, \\pi}{\\mathrm{argmax}}\\, \\sum_{n=1}&#94;N\\mathbb{E}_{Z_n\\sim p(Z_n|Y_n, \\mu_{\\text{old}}, \\Sigma_{\\text{old}}, \\pi_{\\text{old}})}\\left[\\log \\left( p(y_n, Z_n | \\mu, \\sigma \\right) \\right]\\\\ &= \\underset{\\mu, \\Sigma, \\pi}{\\mathrm{argmax}}\\,\\sum_{n=1}&#94;N \\sum_{k=1}&#94;K r_{n, k} \\left[\\log p(y_n | Z_n=k, \\mu, \\Sigma) + \\log p(Z_n=k | \\pi)\\right]\\\\ &= \\underset{\\mu, \\Sigma}{\\mathrm{argmax}}\\,\\sum_{n=1}&#94;N \\sum_{k=1}&#94;K r_{n, k} \\log p(y_n | Z_n=k, \\mu, \\Sigma) + \\underset{\\pi}{\\mathrm{argmax}}\\,\\sum_{n=1}&#94;N \\sum_{k=1}&#94;K r_{n, k} \\log p(Z_n=k | \\pi)\\\\ &=\\underset{\\mu, \\Sigma}{\\mathrm{argmax}}\\,\\sum_{n=1}&#94;N \\sum_{k=1}&#94;K r_{n, k} \\log \\mathcal{N}(y_n; \\mu_{k}, \\Sigma_{k}) + \\underset{\\pi}{\\mathrm{argmax}}\\,\\sum_{n=1}&#94;N \\sum_{k=1}&#94;K r_{n, k} \\log \\pi_k \\end{aligned} where $n=1, \\ldots, N$ and $\\sum_{k=1}&#94;K \\pi_k = 1$. We solve the two optimization problems separately. The optimization problem $$ \\underset{\\pi}{\\mathrm{argmax}}\\,\\sum_{n=1}&#94;N \\sum_{k=1}&#94;K r_{n, k} \\log \\pi_k,\\quad \\sum_{k=1}&#94;K \\pi_k = 1 $$ can be solved using Lagrangian multipliers yielding the solution: $$ \\pi_{\\text{new}, k} = \\frac{\\sum_{n=1}&#94;N r_{n, k}}{N} $$ The optimization problem $$ \\underset{\\mu, \\Sigma}{\\mathrm{argmax}}\\,\\sum_{n=1}&#94;N \\sum_{k=1}&#94;K r_{n, k} \\log \\mathcal{N}(y_n; \\mu_{k}, \\Sigma_{k}) $$ can be solved by taking the gradient with respect to $\\mu_k$, $\\Sigma_k$ for each $k$ and computing the stationary points of the gradient (remember to check for the global concavity to ensure you've found a global max). Doing so gives us the optimal points \\begin{aligned} \\mu_{\\text{new},k} &= \\frac{1}{\\sum_{n=1}&#94;N r_{n, k}} \\sum_{n=1}&#94;N r_{n,k}y_n, &\\quad (\\text{weighted sample mean})\\\\ \\Sigma_{\\text{new},k} &= \\frac{1}{\\sum_{n=1}&#94;N r_{n, k}} \\sum_{n=1}&#94;N r_{n,k} (y_n - \\mu_{\\text{new},k})(y_n - \\mu_{\\text{new},k})&#94;\\top, &\\quad (\\text{weighted sample covariance}) \\end{aligned} Exercise: Verify that the updates for $\\pi_{\\text{new},k}, \\mu_{\\text{new},k}, \\Sigma_{\\text{new},k}$ maximizes $\\mathbb{E}_{Z_n\\sim p(Z_n|Y_n, \\mu_{\\text{old}}, \\Sigma_{\\text{old}}, \\pi_{\\text{old}})}\\left[\\log \\left( p(y_n, Z_n | \\mu, \\sigma \\right) \\right]$. Sanity Check: Log-Likelihood During Training Remember that ploting the MLE model against actual data is not always an option (e.g. high-dimensional data). A sanity check for that your EM algorithm has been implemented correctly is to plot the observed data log-likelihood over the iterations of the algorithm: $$ \\ell_y(\\mu, \\sigma&#94;2, \\pi) = \\sum_{n=1}&#94;N \\log \\sum_{k=1}&#94;K \\mathcal{N}(y_n; \\mu_k, \\sigma_k&#94;2) \\pi_k $$ In [4]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 3 )) ax . plot ( range ( len ( log_lkhd )), log_lkhd , color = 'red' , alpha = 0.5 ) ax . set_title ( 'observed data log-likelihood over iterations of EM' ) plt . show () Expectation Maximization versus Gradient-based Optimization Pros of EM: No learning rates to adjust Don't need to worry about incorporating constraints (i.e. $p(Z_n|Y_n)$ is between 0 and 1) Each iteration is guaranteed to increase or maintain observed data log-likelihood Is guaranteed to converge to local optimum Can be very fast to converge (when parameters are fewer) Cons of EM: Can get stuck in local optima May not maximize observed data log-likelihood (the ELBO is just a lower bound) Requires you to do math - you need analytic solutions for E-step and M-step May be much slower than fancier gradient-based optimization Review of EM for Latent Variable Models Review: Latent Variable Models Models that include an observed variable $Y$ and at least one unobserved variable $Z$ are called latent variable models . In general, our model can allow $Y$ and $Z$ to interact in many different ways. We have studied models with one type of interaction: We treat the parameters $\\theta$ and $\\phi$ as unknown constants , and we estimate them from the observed data $y_1, \\ldots, y_N$. Example: Gaussian Mixture Models (GMMs) In a Gaussian Mixture Model (GMM) , we posit that the observed data $Y$ is generated by a mixture, $\\pi=[\\pi_1, \\ldots, \\pi_K]$, of $K$ number of Gaussians with means $\\mu = [\\mu_1, \\ldots, \\mu_K]$ and covariances $\\Sigma = [\\Sigma_1, \\ldots, \\Sigma_K]$. For each observation $Y_n$ the class of the observation $Z_n$ is a latent variable that indicates which of the $K$ Gaussian is responsible for generating $Y_n$: \\begin{aligned} Z_n &\\sim Cat(\\pi),\\\\ Y_n | Z_n&\\sim \\mathcal{N}(\\mu_{Z_n}, \\Sigma_{Z_n}), \\end{aligned} where $n=1, \\ldots, N$ and $\\sum_{k=1}&#94;K \\pi_k = 1$. GMMs are examples of model based clustering - breaking up a data set into natural clusters based on a statistical model fitted to the data. Inference for this model may mean that we want to learn the mean and covariance for each class in the mixture. Or we may want to infer the class membership $z_n$ for each observation $y_n$. Maximum Likelihood Estimate Inference for Latent Variable Models If we are interested in computing the maximum likelihood estimators of the parameters $\\theta$ and $\\phi$, we need to compute them with respect to the observed likelihood $p(y| \\theta, \\phi)$ - this is simply because we don't have access to the latent variable values, so we can't evaluate $p(y, z| \\theta, \\phi)$ given values for $\\theta$ and $\\phi$. Just like from before, we maximize the log-likelihood rather than the likelihood due to the simplifying properties of the log function: $$ \\theta&#94;*, \\phi&#94;* = \\underset{\\theta, \\phi}{\\text{argmax}}\\; \\ell_y(\\theta, \\phi) = \\underset{\\theta, \\phi}{\\text{argmax}}\\; \\log p(y| \\theta, \\phi) = \\underset{\\theta, \\phi}{\\text{argmax}}\\;\\log \\int p(y, z| \\theta, \\phi)\\, dz $$ Maximizing the the above requires taking a gradient, $$ \\nabla_{\\theta, \\phi} \\log \\int p(y, z| \\theta, \\phi)\\, dz $$ but it's not clear how to evaluate this expression. Rewriting the integral as an expectation, it turns out, illuminates the source of the problem: $$ \\nabla_{\\theta, \\phi} \\log \\int p(y, z| \\theta, \\phi)\\, dz = \\nabla_{\\theta, \\phi} \\log \\int p(y| z, \\phi)p(z|\\theta)\\, dz = \\nabla_{\\theta, \\phi} \\log \\mathbb{E}_{z\\sim p(z|\\theta)}[p(y| z, \\phi)] = \\frac{\\nabla_{\\theta, \\phi} \\mathbb{E}_{z\\sim p(z|\\theta)}[p(y| z, \\phi)]}{\\mathbb{E}_{z\\sim p(z|\\theta)}[p(y| z, \\phi)]},\\quad \\text{(chain rule)} $$ The above makes it clear that the gradient is not trivial to compute -- the gradient cannot be pushed into the expectation, since the distribution with respect to which we are taking the expectation depends on the optimization variable $\\theta$. To make the gradient computation easier, we make two changes: we introduce an auxiliary variable $q(z)$ so that we can replace $\\mathbb{E}_{z\\sim p(z|\\theta)}$ with $\\mathbb{E}_{z\\sim q(z)}$. Note then the latter expectation no longer depends on $\\theta$. we push the log inside the expectation using Jensen's inequality. That is, \\begin{aligned} \\ell_y(\\theta, \\phi) &= \\log \\int p(y, z| \\theta, \\phi)\\, dz\\\\ &= \\log \\int \\frac{p(y, z| \\theta, \\phi)}{q(z)}q(z)\\, dz\\\\ &= \\log \\mathbb{E}_{z\\sim q(z)}\\left[\\frac{p(y, z| \\theta, \\phi)}{q(z)}\\right]\\\\ &\\geq \\underbrace{\\mathbb{E}_{z\\sim q(z)} \\left[\\log\\left(\\frac{p(y, z| \\theta, \\phi)}{q(z)}\\right)\\right]}_{ELBO(\\theta, \\phi, q)} \\end{aligned} We have dervied that $ELBO(\\theta, \\phi, q)$ is a lower bound of the log-likelihood $\\ell_y(\\theta, \\phi)$, for any choice of $q$. So rather than maximizing the log-likelihood, we maximize the $ELBO(\\theta, \\phi, q)$, thus ensuring that $\\ell_y(\\theta, \\phi)$ is at least as big: $$ \\underset{\\theta, \\phi}{\\max}\\ell_y(\\theta, \\phi)\\geq \\underset{\\theta, \\phi, q}{\\max}ELBO(\\theta, \\phi, q) $$ In order to maximize the ELBO, we use coordinate ascent. That is, we take turns maximizing the ELBO with respect to $q$ and then with repect to $\\theta, \\phi$. This algorithm is called expectation maximization (EM) .","tags":"a-sections","url":"a-sections/a-sec01/notebook/"},{"title":"Advanced Section 2: Particle Filters / Sequential Monte Carlo","text":"Slides Particle Filters (PDF)","tags":"a-sections","url":"a-sections/a-sec02/"},{"title":"Lecture 8: Neural Networks 1","text":"Slides Perceptron and Multilayer Perceptron (PDF) Anatomy of NN (PDF)","tags":"lectures","url":"lectures/lecture08/"},{"title":"Lab 4: FFNN","text":"Notebooks FFNN","tags":"labs","url":"labs/lab04/"},{"title":"Lab 04:","text":"CS109B Introduction to Data Science Lab 4: Feed Forward Neural Networks I Harvard University Spring 2022 Instructors : Mark Glickman & Pavlos Protopapas Lab Leaders : Marios Mattheakis & Chris Gumb The goal of this section is to become familiar with a basic Artificial Neural Network architecture, the Feed-Forward Neural Network (FFNN). Specifically, we will: Quickly review the FFNN anatomy . Design a simple FFNN from scratch (using numpy) and fit toy datasets. Quantitatively evaluate the prediction (fit) by using the mean square error (MSE) metric. Develop intuition for the FFNN as a universal approximator and understand this property by inspecting the functions generated by an FFNN. Use forward propagation with TensorFlow and Keras with our previous designs as well as more complex network architectures. PyTorch implementation (extra material) Import packages In [1]: import numpy as np import pandas as pd import matplotlib import matplotlib.pyplot as plt #### import tensorflow as tf # from sklearn.metrics import mean_squared_error 2022-02-18 01:23:37.420114: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /.singularity.d/libs 2022-02-18 01:23:37.420179: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. 1. Review of the ANN anatomy Input, Hidden Layers, and Output Layers The forward pass through an FFNN is a sequence of linear (affine) and nonlinear operations (activation). The Activation function is a nonlinear function. A list of activation functions can be found here . 2. Design a Feed Forward Neural Network Let's create a simple FFNN with one input neuron, one hidden layer of arbitrary number of neurons, and one linear neuron for the output layer. The purpose here is to become familiar with the forward propagation . Define the ReLU and Sigmoid nonlinear functions. These are two commonly used activation functions. Create an FFNN with one hidden neuron and become familiar with the activation function. Activity : Load the toyDataSet_1.csv and fit (manually tuning the weights). This is a simple regression problem with one input and one output. Write a function for the forward pass of a single input/output FFNN with a single hidden layer of arbitrary number of neurons. Tune the weights randomly and inspect the generated functions. Is this network a universal approximator ? Define activation functions Rectified Linear Unit (ReLU) function is defined as $$g(x)=\\max(0,x)$$ Sigmoid function is defined as $$\\sigma(x)=\\frac{1}{1+e&#94;{-z}}$$ In [2]: def g ( z : float ) -> float : return np . maximum ( 0 , z ) # or # g = lambda z: np.maximum(0, z) def sig ( z : float ) -> float : return 1 / ( 1 + np . exp ( - z )) Construct a FFNN with hard-coded parameters. No training is performed here. ReLU activation In [4]: # create an input vector x_train = np . linspace ( - 1 , 1 , 100 ) # set the network parameters w1 , b1 = 1 , 0. w2 , b2 = 1 , 0 #### HIDDEN LAYER #### # affine operation l1 = w1 * x_train + b1 # RELU activation h = g ( l1 ) # output linear layer y_train = w2 * h + b2 plt . plot ( x_train , y_train , '-b' ) plt . title ( 'ReLU Activation' ) plt . show () Sigmoid activation In [5]: # input vector x_train = np . linspace ( - 1 , 1 , 100 ) # set the network parameters w1 , b1 = 10 , 0. w2 , b2 = 1 , 0 #### HIDDEN LAYER #### # affine operation l1 = w1 * x_train + b1 # Sigmoid activation h = sig ( l1 ) # output linear layer y_train = w2 * h + b2 plt . plot ( x_train , y_train , '-b' ) plt . title ( 'Sigmoid Activation' ) plt . show () Plot a few cases to become familiar with the activation In [6]: #weights and biases that we want to explore. weight1, bias1, weight2, bias2 weights1 = 1 , 0 , 1 , 0 weights2 = 1 , 0.5 , 1 , 0 weights3 = 1 , 0.5 , 1 , - 0.5 weights4 = 1 , 0.5 , 4 , - .5 weights_list = [ weights1 , weights2 , weights3 , weights4 ] def simple_FFN ( w1 , b1 , w2 , b2 , activation ): \"\"\" Takes weights, biases, and an activation function and returns a simple prediction. Arguments: w1, w2: weights 1 and 2 b1, b2: biases 1 and 2 \"\"\" # linear transformation l1 = w1 * x_train + b1 #activation function + output linear layer y_pred = w2 * activation ( l1 ) + b2 return y_pred #make our plot plt . figure ( figsize = [ 12 , 8 ]) for i , w_list in enumerate ( weights_list ): #make our weight dictionary then feed the dictionary as arguments to the FFN to get a prediction. w_dict = dict ( zip ([ \"w1\" , \"b1\" , \"w2\" , \"b2\" ], w_list )) # print(w_dict) y_train_pred = simple_FFN ( ** w_dict , activation = g ) #make the plot plt . subplot ( 2 , 2 , i + 1 ) plt . plot ( x_train , y_train_pred , 'b' ) plt . ylim ([ - 1 , 1 ]) plt . xlim ([ - 1 , 1 ]) plt . title ( 'w1, b1, w2, b2 = {} ' . format ( w_list )) plt . ylabel ( \"y(x)\" ) plt . xlabel ( \"x\" ) plt . grid ( 'on' ) plt . tight_layout () Explore the sigmoid activation In [7]: #weights and biases that we want to explore. weight1, bias1, weight2, bias2 weights_1 = 10 , 0 , 1 , 0 weights_2 = 10 , 5 , 1 , 0 weights_3 = 10 , 5 , 1 , - .5 weights_4 = 10 , 5 , 2 , - .5 weights_list = [ weights_1 , weights_2 , weights_3 , weights_4 ] #make our plot plt . figure ( figsize = [ 12 , 8 ]) for i , w_list in enumerate ( weights_list ): #make our weight dictionary then feed the dictionary as arguments to the FFN to get a prediction. #note how we have changed the activation function to sigmoid. w_dict = dict ( zip ([ \"w1\" , \"b1\" , \"w2\" , \"b2\" ], w_list )) y_train_pred = simple_FFN ( ** w_dict , activation = sig ) #make the plot plt . subplot ( 2 , 2 , i + 1 ) plt . plot ( x_train , y_train_pred , 'b' ) plt . ylim ([ - 1 , 1.6 ]) plt . xlim ([ - 1 , 1 ]) plt . title ( 'w1, b1, w2, b2 = {} ' . format ( w_list )) plt . ylabel ( \"y(x)\" ) plt . xlabel ( \"x\" ) plt . grid ( 'on' ) plt . tight_layout () Activity 1 Design a simple FFNN to fit a simple dataset Load the toyDataSet_1.csv from the current directory. Write an FFNN with one hidden layer of one neuron and fit the data. Between ReLU and Sigmoid , choose which activation function works better Make a plot with the ground truth data and the prediction Write a custom mean square error (MSE) or use the sklearn mean_squared_error() to evaluate the prediction For this example, don't split the data into training and test sets. Just fit and evaluate the predictions on the entire set. In [8]: def plot_toyModels ( x_data , y_data , y_pred = None ): \"\"\" Generates a plot of the data, as well as the predictions when provided. Arguments: x_data: x from training data y_data: y from training data y_pred: predicted y values \"\"\" if type ( y_data ) != type ( None ): plt . plot ( x_data , y_data , 'or' , label = 'data' ) if type ( y_pred ) != type ( None ): plt . plot ( x_data , y_pred , '-b' , linewidth = 4 , label = 'FFNN' , alpha = .7 ) plt . xlabel ( 'x' ) plt . ylabel ( 'y(x)' ) plt . legend () def mean_squared_error ( y_true , y_network ): \"\"\" Calculate the MSE between the true values and the predicted. Arguments: y_true: actual y from data y_pred: predicted y from model \"\"\" # This can also be loaded from sklearn or tensorflow # e.g. sklearn.metrics.mean_squared_error return ( ( y_true - y_network ) ** 2 ) . mean () In [9]: toySet_1 = pd . read_csv ( 'toyDataSet_1.csv' ) x_train = toySet_1 [ 'x' ] . values . reshape ( - 1 , 1 ) y_train = toySet_1 [ 'y' ] . values . reshape ( - 1 , 1 ) plot_toyModels ( x_train , y_train ) In [9]: ## your code here # set the network parameters w1 = b1 = w2 = b2 = # affine operation l1 = # activation (Choose between ReLu or Sigmoid) h = # output linear layer y_model_train = # Make a plot (use the ploting function defined earlier) plot_toyModels ( x_train , y_train , y_model_train ) # Use MSE to evaluate the prediction mse_toy = print ( 'The MSE for the training set is ' , np . round ( mse_toy , 5 )) Input In [9] w1 = &#94; SyntaxError : invalid syntax In [10]: ###################### ## Using ReLU ###################### # set the network parameters w1 = b1 = w2 = b2 = # affine operation l1 = w1 * x_train + b1 # activation (Choose between ReLu or Sigmoid) h = g ( l1 ) # for relu # output linear layer y_model_train = w2 * h + b2 # Make a plot the plot_toyModels function plt . figure ( figsize = [ 12 , 4 ]) plt . subplot ( 1 , 2 , 1 ) plot_toyModels ( x_train , y_train , y_model_train ) plt . title ( 'ReLU activation' ) # Evaluate the prediction mse_toy = mean_squared_error ( y_train , y_model_train ) print ( 'ReLU: The MSE for the training set is ' , np . round ( mse_toy , 5 )) ###################### ## Using sigmoid ###################### w1 = b1 = w2 = b2 = # affine operation l1 = w1 * x_train + b1 # activation (Choose between ReLu or Sigmoid) h = sig ( l1 ) # for sigmoid # output linear layer y_model_train = w2 * h + b2 # Make a plot the plot_toyModels function plt . subplot ( 1 , 2 , 2 ) plot_toyModels ( x_train , y_train , y_model_train ) plt . title ( 'Sigmoid activation' ) # Evaluate the prediction mse_toy = mean_squared_error ( y_train , y_model_train ) print ( 'Sigmoid: The MSE for the training set is ' , np . round ( mse_toy , 5 )) Input In [10] w1 = &#94; SyntaxError : invalid syntax Why does the choice of ReLU activation give lower MSE than Sigmoid? A function for a more complex Forward Pass Let's write a function for the forward propagation through an FFNN with one input, one linear output neuron, and one hidden layers with arbitrary number of neurons. General Scheme: One input vector: $x$ $$$$ Affine (linear) transformation: $l_1$ where $w_{1},~b_{1}$ are the parameter vectors (or $w_{1i},~b_{1i}$): $$l_1 = \\sum_{i=1}&#94;\\text{\\# neurons} w_{1i}x+b_{1i} = w&#94;T_1 x + b_1 = w_1 \\cdot x + b_1 = W_1\\cdot X$$ $$$$ Activation function (nonlinear transformation): $g(\\cdot)$ $$h = g(l_1)$$ $$$$ Linear Output layer with a vector for weights $w_o$ and a scalar bias $b_o$: $$y = w_o&#94;T h+b_o = w_o \\cdot h + b_o = W_o\\cdot H$$ NOTE for the future: You can have a nonlinear output neuron(s) (e.g. for classification tasks) by just activating the above linear output, namely $$y = w_o&#94;T h+b_o = w_o \\cdot h + b_o = \\sigma(W_o\\cdot H)$$ In [11]: def myFFNN ( X , W1 , Wo , activation = 'relu' ): \"\"\" This function gives the forward pass of a simple feed forward neural network. The network propagates a single input X through a single hidden layer of weights W1, and return a single output (yhat) through a linear output layer with weights Wo. The input and the weight matrices should be given. Network specifications: input dimensions = 1 output dimensions = 1 hidden layers = 1 **hidden neurons are determined by the size of W1 or W0** Parameters: Design Matrix: X: the design matrix on which to make the predictions. weights vectors: W1 : parameters of first layer Wo : parameters of output layer activation: The default activation is the relu. It can be changed to sigmoid \"\"\" # Input: # add a constant column for the biases to the input vector X ones = np . ones (( len ( X ), 1 )) l1 = X l1 = np . append ( l1 , ones , axis = 1 ) # hidden layer: Affine and activation a1 = np . dot ( W1 , l1 . T ) if activation == 'relu' : h1 = g ( a1 ) elif activation == 'sigmoid' : h1 = sig ( a1 ) # Output layer (linear layer) (2 steps) # (a) Add a const column the h1 for the affine transformation ones = np . ones (( len ( X ), 1 )) H = np . append ( h1 . T , ones , axis = 1 ) . T # (b) Affine a = np . dot ( Wo , H ) y_hat = a . T return y_hat Use the previous parameters in our forward propagation function to fit the toyDataSet_1.csv. Plot the results and print the associated loss (the MSE) In [12]: w11 = 2 b11 = 0.0 w21 = 1 b21 = 0.5 # make the parameters matrices # First layer W1 = np . array ([[ w11 , b11 ]]) # Output Layer (only one bias term) Wo = np . array ([[ w21 , b21 ]]) # run the model y_model_1 = myFFNN ( x_train , W1 , Wo ) # plot the prediction and the ground truth plot_toyModels ( x_train , y_train , y_model_1 ) # quantify your prediction Loss_1 = mean_squared_error ( y_train , y_model_1 ) print ( 'MSE Loss = ' , np . round ( Loss_1 , 4 )) MSE Loss = 0.0023 Q: Can we easily can we generalize this network architecture to have many inputs, outputs, and hidden layers? A: No, we will need Tensor algebra, and it is much easier to use deep learning packages like TensorFlow and PyTorch for this process. FFNN is a Universal Function Approximator Here we will explore which functions can be generated using a single-hidden layer network with many neurons. It is proved that an FFNN can approximate with arbitrary accuracy any continuous function if the network has a sufficient number of hidden neurons. For a rigorous proof you can check the original paper . In [13]: # Two Neurons NNet w11 = - .8 b11 = - .1 w12 = .4 b12 = - .1 w21 = 1.3 w22 = - .8 b2 = 0.5 # First Layer W1 = np . array ([[ w11 , b11 ], [ w12 , b12 ]]) # Output Layer (only one bias term) Wo = np . array ([[ w21 , w22 , b2 ]]) # run the model y_model_p = myFFNN ( x_train , W1 , Wo , activation = 'relu' ) plot_toyModels ( x_train , y_data = None , y_pred = y_model_p ) In [14]: # Three Neurons NNet w11 = - .1 b11 = .3 w12 = .9 b12 = - .1 w13 = .7 b13 = - .2 w21 = - 1. w22 = - .7 w33 = .8 b2 = 0.25 # First Layer W1 = np . array ([[ w11 , b11 ], [ w12 , b12 ], [ w13 , b13 ]]) # Output Layer (only one bias term) Wo = np . array ([[ w21 , w22 , w33 , b2 ]]) # run the model y_model_p = myFFNN ( x_train , W1 , Wo ) # plot the prediction and the ground truth plot_toyModels ( x_train , y_data = None , y_pred = y_model_p ) plt . show () In [15]: # Random numbers between a,b # (b-a) * np.random.random_sample((4, 4)) + a a = - 20 b = 20 # N neurons N = 50 # Create random parameter matrices W1 = ( b - a ) * np . random . random_sample (( N , 2 )) + a Wo = ( b - a ) * np . random . random_sample (( 1 , N + 1 )) + a # make a bigger interval x_train_p2 = np . linspace ( - 2 , 2 , 1000 ) x_train_p2 = x_train_p2 . reshape ( - 1 , 1 ) ## run the models and plot the predictions plt . figure ( figsize = [ 12 , 4 ]) # # RELU ACTIVATION y_model_p2 = myFFNN ( x_train_p2 , W1 , Wo , activation = 'relu' ) plt . subplot ( 1 , 2 , 1 ) plot_toyModels ( x_train_p2 , y_data = None , y_pred = y_model_p2 ) plt . title ( 'Relu activation' ) # ## SIGMOID ACTIVATION y_model_p2 = myFFNN ( x_train_p2 , W1 , Wo , activation = 'sigmoid' ) plt . subplot ( 1 , 2 , 2 ) plot_toyModels ( x_train_p2 , y_data = None , y_pred = y_model_p2 ) plt . title ( 'Sigmoid activation' ) plt . show () Run the above cell multiple times to see how many different predictions the network can make. 3. TensorFlow and Keras Keras, Sequential: [Source] ( https://keras.io/models/sequential/ ) There are many powerful deep learning packages to work with neural networks like TensorFlow and PyTorch . These packages provide both the forward and back propagations, and many other other functionalities. The forward pass is used to make predictions while the backward (back propagation) is used to train (optimize) a network. Training means to find the optimal parameters for a specific task. Here, we use TensorFlow (TF) and Keras to employ FFNN. Use Keras to fit the simple toyDataSet_1 dataset. Tune the weights manually. Learn the Sequential method At the end of this notebook we have a PyTorch implementation of the same network architecture. Next Lab: We will learn how to use backpropagation (fit method) to find the optimal parameters. Import packages from keras In [16]: from tensorflow.keras import layers from tensorflow.keras import models from tensorflow.keras import optimizers Import the toyDataSet_1 and define the weights used in Activity 1 for the ReLU architecture In [17]: toySet_1 = pd . read_csv ( 'toyDataSet_1.csv' ) x_train = toySet_1 [ 'x' ] . values . reshape ( - 1 , 1 ) y_train = toySet_1 [ 'y' ] . values . reshape ( - 1 , 1 ) w1 = 2 b1 = 0.0 w2 = 1 b2 = 0.5 Use Keras to build a model similar to our myFFNN defined earlier Below we will use the Keras' package to first define a sequential model called Single_neurons_model_fixedWeights . We will then use the add method to add a single hidden layer to the model with one neuron using ReLU activation. In [18]: model = models . Sequential ( name = 'Single_neurons_model_fixedWeights' ) ### hidden layer with 1 neuron (or node) model . add ( layers . Dense ( 1 , activation = 'relu' , input_shape = ( 1 ,))) # output layer, one neuron model . add ( layers . Dense ( 1 , activation = 'linear' )) model . summary () Model: \"Single_neurons_model_fixedWeights\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 1) 2 dense_1 (Dense) (None, 1) 2 ================================================================= Total params: 4 Trainable params: 4 Non-trainable params: 0 _________________________________________________________________ 2022-02-18 01:25:08.274076: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /.singularity.d/libs 2022-02-18 01:25:08.274169: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303) 2022-02-18 01:25:08.274243: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (holy2a02104.rc.fas.harvard.edu): /proc/driver/nvidia/version does not exist 2022-02-18 01:25:08.275305: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Read and change the network parameters In [19]: def print_weights ( model ): \"\"\" A function that reads, prints and changes the model weights/biases. Arguments: model: a Tensorflow model object \"\"\" weights = model . get_weights () print ( dict ( zip ([ \"w1\" , \"b1\" , \"w2\" , \"b2\" ], [ weight . flatten ()[ 0 ] for weight in weights ]))) print ( 'Initial values of the parameters' ) print_weights ( model ) # MANUALLY SETTING THE WEIGHTS/BIASES # Read the model's weight with the get_weights method weights = model . get_weights () # Update the weights/biases of the hidden layer weights [ 0 ][ 0 ] = np . array ([ w1 ]) #weights weights [ 1 ] = np . array ([ b1 ]) # biases # Adjust the weights/biases for the output layer weights [ 2 ] = np . array ([[ w2 ]]) # weights weights [ 3 ] = np . array ([ b2 ]) # bias # Apply the new weights/bias to the model model . set_weights ( weights ) print ( ' \\n After setting the parameters' ) print_weights ( model ) Initial values of the parameters {'w1': -0.8445749, 'b1': 0.0, 'w2': -0.93036306, 'b2': 0.0} After setting the parameters {'w1': 2.0, 'b1': 0.0, 'w2': 1.0, 'b2': 0.5} In [20]: # Pass the data to the predict method to get predictions y_model_tf1 = model . predict ( x_train ) # Plot the true values and our predictions plot_toyModels ( x_train , y_train , y_pred = y_model_tf1 ) # Calculate the MSE of our predicitons Loss_tf1 = mean_squared_error ( y_train , y_model_tf1 ) print ( 'MSE Loss = ' , np . round ( Loss_tf1 , 4 )) MSE Loss = 0.0023 Construct a more complex architecture Here we will build a FFNN with one hidden layer containing 20 neurons. In [21]: # Define a new sequential model called Many_neurons model_2 = models . Sequential ( name = 'Many_neurons' ) # Add a dense layer with 20 neurons and ReLU activation model_2 . add ( layers . Dense ( 20 , activation = 'relu' , # kernel_initializer='random_normal', bias_initializer='random_uniform', input_shape = ( 1 , ))) # Add the output layer, still with a single neuron model_2 . add ( layers . Dense ( 1 , activation = 'linear' )) # Inspect the model model_2 . summary () ## Run a forward pass and plot the output function y_model_2 = model_2 . predict ( x_train ) plt . plot ( x_train , y_model_2 , linewidth = 4 , label = 'many neurons' , alpha = 1 ) plt . xlabel ( 'x' ) plt . ylabel ( 'y(x)' ) plt . legend () Model: \"Many_neurons\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_2 (Dense) (None, 20) 40 dense_3 (Dense) (None, 1) 21 ================================================================= Total params: 61 Trainable params: 61 Non-trainable params: 0 _________________________________________________________________ Out[21]: Build a model with multiple hidden layers Below we create a new model with three hidden layers, each with a different number of neurons and activations. In [22]: model_3 = models . Sequential ( name = 'Many_layers_neurons' ) #### First hidden layer model_3 . add ( layers . Dense ( 20 , activation = 'relu' , kernel_initializer = 'random_uniform' , bias_initializer = 'random_uniform' , input_shape = ( 1 ,))) #### Second hidden layer model_3 . add ( layers . Dense ( 100 , activation = 'tanh' , kernel_initializer = 'random_normal' , bias_initializer = 'random_uniform' )) #### Third hidden layer model_3 . add ( layers . Dense ( 50 , activation = 'relu' , kernel_initializer = 'random_normal' , bias_initializer = 'random_uniform' )) # Add the output layer and check the summary model_3 . add ( layers . Dense ( 1 , activation = 'linear' )) model_3 . summary () # Define a new input vector # Notice that the architecture does not depend on the size of the input vector x_train_3 = np . linspace ( - 10 , 10 , 1500 ) ## Run a forward pass and plot the output function y_model_3 = model_3 . predict ( x_train_3 ) plt . plot ( x_train_3 , y_model_3 , linewidth = 4 , label = 'many layers with many neurons' , alpha = 1 ) plt . xlabel ( 'x' ) plt . ylabel ( 'y(x)' ) plt . legend () Model: \"Many_layers_neurons\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_4 (Dense) (None, 20) 40 dense_5 (Dense) (None, 100) 2100 dense_6 (Dense) (None, 50) 5050 dense_7 (Dense) (None, 1) 51 ================================================================= Total params: 7,241 Trainable params: 7,241 Non-trainable params: 0 _________________________________________________________________ Out[22]: Q: How many parameters does our Many_layers_neurons model have? Does it sound like a good idea to manually tune them? Stay Tuned... In the next Lab we will learn how to find the optimal parameters! 4. PyTorch Implementation: Extra Material PyTorch is another library used to build, train, and deploy deep learning models. Most course material will be performed using Tensorflow, but it is good to know about another common library you might encounter. A great PyTorch tutorial for an FFNN implementation can be found at: https://pytorch.org/tutorials/beginner/pytorch_with_examples.html In [24]: import torch dtype = torch . float In PyTorch we have to define everything as torch tensors instead of numpy matrices Though, we can transform numpy objects to pytorch tensors by using: torch.from_numpy(np_array) In [25]: # Create an input vector using PyTorch x = torch . linspace ( - 10 , 10 , 1000 ) print ( 'PyTorch tensor: ' , x . size ()) # Note how this is nearly the same as using Numpy x_ = np . linspace ( - 10 , 10 , 1000 ) print ( 'Numpy array: ' , x_ . shape ) # Convert a numpy array to a PyTorch tensor x = torch . from_numpy ( x_ ) print ( 'from numpy to PyTorch: ' , x . size ()) PyTorch tensor: torch.Size([1000]) Numpy array: (1000,) from numpy to PyTorch: torch.Size([1000]) Define the input tensor: Specify the type Reshape or unsqueeze. This is what pytorch wants to get In [26]: x = torch . linspace ( - 10 , 10 , 1000 , dtype = dtype ) print ( 'PyTorch Tensor: ' , x . size ()) # x=x.reshape(-1,1) x = x . unsqueeze ( - 1 ) print ( 'Reshaped tensor: ' , x . size ()) PyTorch Tensor: torch.Size([1000]) Reshaped tensor: torch.Size([1000, 1]) PyTorch also provides a sequential functionality Build the network architecture In [27]: model_p = torch . nn . Sequential ( torch . nn . Linear ( 1 , 10 ), torch . nn . ReLU (), torch . nn . Linear ( 10 , 1 )) print ( model_p ) Sequential( (0): Linear(in_features=1, out_features=10, bias=True) (1): ReLU() (2): Linear(in_features=10, out_features=1, bias=True) ) Forward pass To perform a forward pass in PyTorch, you pass the data directly to the model as an argument. In [28]: y_p = model_p ( x ) Trying to operate tensors may cause problems. For example, try to plot them In [29]: # plt.plot(x, y_p) We can convert PyTorch tensors to numpy by using: torchTensor.detach().numpy() In [30]: x_ = x . detach () . numpy () y_ = y_p . detach () . numpy () plt . plot ( x_ , y_ , linewidth = 4 , label = 'My First PyTorch model' ) plt . xlabel ( 'x' ) plt . ylabel ( 'y(x)' ) plt . legend () Out[30]: A deeper network In [31]: model_p2 = torch . nn . Sequential ( torch . nn . Linear ( 1 , 10 ), torch . nn . ReLU (), torch . nn . Linear ( 10 , 20 ), torch . nn . Sigmoid (), torch . nn . Linear ( 20 , 10 ), torch . nn . Tanh (), torch . nn . Linear ( 10 , 1 ) ) y_p2 = model_p2 ( x ) x2_ = x . detach () . numpy () y2_ = y_p2 . detach () . numpy () plt . plot ( x2_ , y2_ , linewidth = 4 , label = 'My First Deep PyTorch model' ) plt . xlabel ( 'x' ) plt . ylabel ( 'y(x)' ) plt . legend () Out[31]: End of Section In [ ]:","tags":"labs","url":"labs/lab04/notebook/"},{"title":"Lab 3: Hierarchical Bayesian Models","text":"Notebooks Hierarchical Models","tags":"labs","url":"labs/lab03/"},{"title":"Lab 03:","text":"CS109B Data Science 2: Advanced Topics in Data Science Lab 03 - Hierarchical Models in PyMC3 Harvard University Spring 2022 Instructors: Mark Glickman and Pavlos Protopapas Lab instructor and content: Eleni Angelaki Kaxiras In [1]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: In [2]: import warnings import arviz as az import matplotlib.pyplot as plt import numpy as np import pymc3 as pm import theano.tensor as tt from pymc3 import summary from pymc3 import Model , Normal , HalfNormal , model_to_graphviz , HalfCauchy from pymc3 import NUTS , sample , find_MAP from scipy import optimize warnings . simplefilter ( action = \"ignore\" , category = FutureWarning ) warnings . filterwarnings ( 'ignore' ) import numpy as np import matplotlib.pyplot as plt from matplotlib import gridspec import scipy.stats as stats import pandas as pd import seaborn as sns % matplotlib inline import warnings print ( 'Running on PyMC3 v {} ' . format ( pm . __version__ )) Running on PyMC3 v3.8 In [3]: %% javascript IPython . OutputArea . auto_scroll_threshold = 20000 ; Learning Objectives By the end of this lab, you should know how to create Bayesian hierarchical models in PyMC3 This lab maps to Homework 2. Hierarchical Models Gelman et al. famous radon dataset is a classic for hierarchical modeling. In this dataset the amount of the radioactive gas radon has been measured among different households in all county's of several states. Radon gas is known to be the highest cause of lung cancer in non-smokers. It is believed to be more strongly present in households containing a basement and to differ in amount present among types of soil. Here we'll investigate this differences and try to make predictions of radonlevels in different county's based on the county itself and the presence of a basement. In [4]: df = pd . read_csv ( '../data/radon.csv' , index_col = [ 0 ]) df [ 'log_radon' ] = df [ 'log_radon' ] . astype ( 'float' ) county_names = df . county . unique () county_idx = df . county_code . values n_counties = len ( df . county . unique ()) df . head () Out[4]: idnum state state2 stfips zip region typebldg floor room basement ... pcterr adjwt dupflag zipflag cntyfips county fips Uppm county_code log_radon 0 5081.0 MN MN 27.0 55735 5.0 1.0 1.0 3.0 N ... 9.7 1146.499190 1.0 0.0 1.0 AITKIN 27001.0 0.502054 0 0.832909 1 5082.0 MN MN 27.0 55748 5.0 1.0 0.0 4.0 Y ... 14.5 471.366223 0.0 0.0 1.0 AITKIN 27001.0 0.502054 0 0.832909 2 5083.0 MN MN 27.0 55748 5.0 1.0 0.0 4.0 Y ... 9.6 433.316718 0.0 0.0 1.0 AITKIN 27001.0 0.502054 0 1.098612 3 5084.0 MN MN 27.0 56469 5.0 1.0 0.0 4.0 Y ... 24.3 461.623670 0.0 0.0 1.0 AITKIN 27001.0 0.502054 0 0.095310 4 5085.0 MN MN 27.0 55011 3.0 1.0 0.0 4.0 Y ... 13.8 433.316718 0.0 0.0 3.0 ANOKA 27003.0 0.428565 1 1.163151 5 rows √ó 29 columns Each row in the dataframe represents the radon measurements for one house in a specific county including whether the house has a basement (floor = 0) or not (floor = 1). We are interested in whether having a basement increases the radon measured in the house. To keep things simple let's keep only the following three variables: county , log_radon , and floor In [5]: # keep only these variables data = df [[ 'county' , 'log_radon' , 'floor' ]] data . head () Out[5]: county log_radon floor 0 AITKIN 0.832909 1.0 1 AITKIN 0.832909 0.0 2 AITKIN 1.098612 0.0 3 AITKIN 0.095310 0.0 4 ANOKA 1.163151 0.0 Let's check how many different counties we have. We also notice that they have a different number of houses. Some have a large number of houses measured, some only 1. In [6]: data [ 'county' ] . value_counts () . head ( 5 ) Out[6]: ST LOUIS 116 HENNEPIN 105 DAKOTA 63 ANOKA 52 WASHINGTON 46 Name: county, dtype: int64 In [7]: data [ 'county' ] . value_counts ()[ - 5 :] Out[7]: STEVENS 2 MILLE LACS 2 MAHNOMEN 1 MURRAY 1 WILKIN 1 Name: county, dtype: int64 In [8]: # let's add a column that numbers the counties from 0 to n # raw_ids = np.unique(data['county']) # raw2newid = {x:np.where(raw_ids == x)[0][0] for x in raw_ids} # data['county_id'] = data['county'].map(raw2newid) # data 1 - Pooling: Same Linear Regression for all We can just pool all the data and estimate one big regression to asses the influence of having a basement on radon levels across all counties. Our model would be: \\begin{equation} y_{i} = \\alpha + \\beta*floor_{i} \\end{equation} Where $i$ represents the measurement (house), and floor contains a 0 or 1 if the house has a basement or not. By ignoring the county feature, we do not differenciate on counties. In [9]: with pm . Model () as pooled_model : # common priors for all a = pm . Normal ( 'a' , mu = 0 , sigma = 100 ) b = pm . Normal ( 'b' , mu = 0 , sigma = 100 ) # radon estimate radon_est = a + b * data [ 'floor' ] . values # likelihood after radon observations radon_obs = pm . Normal ( 'radon_obs' , mu = radon_est , observed = data [ 'log_radon' ]) # note here we enter the whole dataset In [10]: from pymc3 import model_to_graphviz model_to_graphviz ( pooled_model ) Out[10]: %3 cluster919 919 b b ~ Normal radon_obs radon_obs ~ Normal b->radon_obs a a ~ Normal a->radon_obs In [11]: with pooled_model : pooled_trace = sample ( 2000 , tune = 1000 , target_accept = 0.9 ) print ( f 'DONE' ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [b, a] Sampling 4 chains, 0 divergences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12000/12000 [00:03<00:00, 3781.83draws/s] DONE In [12]: pm . traceplot ( pooled_trace ); Remember, with the pooled model we have only one intercept, $\\alpha$, and only one slope, $\\beta$ for all the counties. Let's plot the regression lines. In [13]: # plot just a subset of the countries counties = [ 'HENNEPIN' , 'AITKIN' , 'WASHINGTON' , 'MURRAY' , 'YELLOW MEDICINE' , 'MAHNOMEN' ] plt . figure ( figsize = ( 10 , 5 )) rows = 2 gs = gridspec . GridSpec ( rows , len ( counties ) // rows ) for i , county in enumerate ( counties ): county_data = data . loc [ data [ 'county' ] == county ] x = np . linspace ( - 0.2 , 1.2 ) radon_est = pooled_trace [ 'a' ] . mean () + pooled_trace [ 'b' ] . mean () * x subplt = plt . subplot ( gs [ i ]) subplt . set_ylim ( 0. , 4. ) subplt . scatter ( county_data [ 'floor' ], county_data [ 'log_radon' ]) subplt . plot ( x , radon_est , c = 'r' , label = 'pooled line' ); subplt . set_xlabel ( 'floor' , fontsize = 10 ) subplt . set_ylabel ( 'radon level' , fontsize = 10 ) subplt . set_title ( str ( county ) + ' County' ) subplt . legend () plt . tight_layout () 2 - Unpooling: Separate Linear Regression for each county We believe that different counties have different relationships of radon and basements. Our model would be: \\begin{equation} radon_{i,c} = \\alpha_c + \\beta_c*floor_{i,c} \\end{equation} Where $i$ represents the measurement, $c$ the county, and floor contains a 0 or 1 if the house has a basement or not. Notice we have separate coefficients for each county in $a_c$ and $b_c$. They are totally different, they do not even come from the same distribution. We will do this for only one county, as an example. We pick HENNEPIN county. In [14]: # chose a county county = 'MEEKER' county_data = data . loc [ data [ 'county' ] == county ] county_data . head () Out[14]: county log_radon floor 479 MEEKER 0.875469 0.0 480 MEEKER 1.386294 0.0 481 MEEKER 1.987874 0.0 482 MEEKER 0.788457 0.0 483 MEEKER 1.193922 0.0 In [15]: #help(pm.Normal) In [16]: with pm . Model () as unpooled_model : mu_a = pm . Normal ( 'mu_a' , mu = 0. , sigma = 100 ) sigma_a = pm . HalfNormal ( 'sigma_a' , 5. ) mu_b = pm . Normal ( 'mu_b' , mu = 0. , sigma = 100 ) sigma_b = pm . HalfNormal ( 'sigma_b' , 5. ) a = pm . Normal ( 'a' , mu = mu_a , sigma = sigma_a ) b = pm . Normal ( 'b' , mu = mu_b , sigma = sigma_b ) radon_est = a + b * county_data [ 'floor' ] . values radon_obs = pm . Normal ( 'radon_like' , mu = radon_est , observed = county_data [ 'log_radon' ]) In [17]: model_to_graphviz ( unpooled_model ) Out[17]: %3 cluster5 5 a a ~ Normal radon_like radon_like ~ Normal a->radon_like sigma_b sigma_b ~ HalfNormal b b ~ Normal sigma_b->b sigma_a sigma_a ~ HalfNormal sigma_a->a b->radon_like mu_b mu_b ~ Normal mu_b->b mu_a mu_a ~ Normal mu_a->a In [18]: with unpooled_model : unpooled_trace = sample ( 2000 , tune = 1000 , target_accept = 0.9 ) print ( f 'DONE' ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [b, a, sigma_b, mu_b, sigma_a, mu_a] Sampling 4 chains, 465 divergences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12000/12000 [00:58<00:00, 205.25draws/s] There were 94 divergences after tuning. Increase `target_accept` or reparameterize. There were 184 divergences after tuning. Increase `target_accept` or reparameterize. The acceptance probability does not match the target. It is 0.7384761840644513, but should be close to 0.9. Try to increase the number of tuning steps. There were 88 divergences after tuning. Increase `target_accept` or reparameterize. There were 99 divergences after tuning. Increase `target_accept` or reparameterize. The acceptance probability does not match the target. It is 0.8329297259480425, but should be close to 0.9. Try to increase the number of tuning steps. The estimated number of effective samples is smaller than 200 for some parameters. DONE In [19]: pm . traceplot ( unpooled_trace ); Print the regression line for our chosen county alone. In [20]: county = 'MEEKER' county_data = data . loc [ data [ 'county' ] == county ] x = np . arange ( len ( county_data [ 'floor' ] . values )) radon_est_unpooled = unpooled_trace [ 'a' ] . mean () + unpooled_trace [ 'b' ] . mean () * county_data [ 'floor' ] . values xx = np . linspace ( - 0.2 , 1.2 ) radon_est_pooled = pooled_trace [ 'a' ] . mean () + pooled_trace [ 'b' ] . mean () * xx plt . scatter ( county_data [ 'floor' ], county_data [ 'log_radon' ]) plt . xlim ( - 0.1 , 1.1 ) plt . xlabel ( 'floor' , fontsize = 10 ) plt . ylabel ( 'radon level' , fontsize = 10 ) plt . title ( f ' { str ( county ) } county Radon levels' ) plt . plot ( x , radon_est_unpooled , c = 'g' , label = 'unpooled line' ); plt . plot ( xx , radon_est_pooled , c = 'r' , label = 'pooled line' ); plt . legend (); 3 - Partial pooling: Hierarchical Regression (Varying-Coefficients Model) Merely by the fact that all counties are counties, they share similarities, so there is a middle ground to both of these extremes. Specifically, we may assume that while $\\alpha_c$ and $\\beta_c$are different for each county as in the unpooled case, the coefficients are all drawn from the same distribution: \\begin{equation} radon_{i,c} = \\alpha_c + \\beta_c*floor_{i,c} \\end{equation} \\begin{equation} a_c \\sim \\mathcal{N}(\\mu_a,\\,\\sigma_a&#94;{2}) \\end{equation} \\begin{equation} b_c \\sim \\mathcal{N}(\\mu_b,\\,\\sigma_b&#94;{2}) \\end{equation} where the common parameters are: \\begin{eqnarray} \\mu_a \\sim \\mathcal{N}(0,\\,10) \\\\ \\sigma_a&#94;2 \\sim |\\mathcal{N}(0,\\,10)| \\\\ \\mu_b \\sim \\mathcal{N}(0,\\,10) \\\\ \\sigma_b&#94;2 \\sim |\\mathcal{N}(0,\\,10)| \\end{eqnarray} Add math for mu and sigma The different counties are effectively sharing information through the commin priors. We are thus observing what is known as shrinkage; modeling the groups not as independent from each other, neither as a single group but rather as related. NOTES We saw that some counties had only one sample, so if that house is a really old with old lead pipes, our prediction will be that all houses in this county have radon. On the other extreme, if we have a newer house with no radon then again we will have missleading results. In one case, you will overestimate the bad quality and in the other underestimate it. Under a hierarchical model, the miss-estimation of one group will be offset by the information provided by the other groups. As always gathering more data helps if this is an option. Defining the Model for the Hierarchical Model In [21]: with pm . Model () as hierarchical_model : # Hyperpriors for group nodes mu_a = pm . Normal ( 'mu_a' , mu = 0. , sigma = 100 ) sigma_a = pm . HalfNormal ( 'sigma_a' , 5. ) mu_b = pm . Normal ( 'mu_b' , mu = 0. , sigma = 100 ) sigma_b = pm . HalfNormal ( 'sigma_b' , 5. ) # Above we just set mu and sd to a fixed value while here we # plug in a common group distribution for all a and b (which are # vectors of length n_counties). # Intercept for each county, distributed around group mean mu_a a = pm . Normal ( 'a' , mu = mu_a , sigma = sigma_a , shape = n_counties ) # beta for each county, distributed around group mean mu_b b = pm . Normal ( 'b' , mu = mu_b , sigma = sigma_b , shape = n_counties ) # Model error #eps = pm.HalfCauchy('eps', 5.) radon_est = a [ county_idx ] + b [ county_idx ] * data [ 'floor' ] . values # Data likelihood with sigma for random error # radon_like = pm.Normal('radon_like', mu=radon_est, # sigma=eps, observed=data['log_radon']) # Data likelihood with sigma without random error radon_like = pm . Normal ( 'radon_like' , mu = radon_est , #sigma=eps, observed = data [ 'log_radon' ]) In [22]: model_to_graphviz ( hierarchical_model ) Out[22]: %3 cluster85 85 cluster919 919 sigma_b sigma_b ~ HalfNormal b b ~ Normal sigma_b->b mu_b mu_b ~ Normal mu_b->b sigma_a sigma_a ~ HalfNormal a a ~ Normal sigma_a->a mu_a mu_a ~ Normal mu_a->a radon_like radon_like ~ Normal b->radon_like a->radon_like Inference In [23]: with hierarchical_model : hierarchical_trace = pm . sample ( 2000 , tune = 2000 , target_accept = .9 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [b, a, sigma_b, mu_b, sigma_a, mu_a] Sampling 4 chains, 1,822 divergences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16000/16000 [01:02<00:00, 257.47draws/s] There were 8 divergences after tuning. Increase `target_accept` or reparameterize. There were 49 divergences after tuning. Increase `target_accept` or reparameterize. There were 1754 divergences after tuning. Increase `target_accept` or reparameterize. The acceptance probability does not match the target. It is 0.1690165871960057, but should be close to 0.9. Try to increase the number of tuning steps. There were 11 divergences after tuning. Increase `target_accept` or reparameterize. The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge. The estimated number of effective samples is smaller than 200 for some parameters. In [24]: pm . traceplot ( hierarchical_trace , var_names = [ 'mu_a' , 'mu_b' , 'sigma_a' , 'sigma_b' ]); In [25]: results = pm . summary ( hierarchical_trace ) results [: 10 ] Out[25]: mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat mu_a 1.462 0.048 1.374 1.560 0.002 0.001 912.0 912.0 681.0 1786.0 1.06 mu_b -0.624 0.089 -0.824 -0.476 0.006 0.005 215.0 193.0 224.0 379.0 1.09 a[0] 1.334 0.210 0.917 1.758 0.003 0.003 4753.0 2289.0 3168.0 2972.0 1.20 a[1] 1.043 0.119 0.830 1.276 0.017 0.013 47.0 44.0 55.0 2754.0 1.07 a[2] 1.493 0.220 1.027 1.877 0.027 0.019 68.0 68.0 61.0 2436.0 1.09 a[3] 1.504 0.190 1.131 1.893 0.004 0.003 2658.0 2658.0 1129.0 2511.0 1.17 a[4] 1.382 0.251 1.043 1.853 0.073 0.053 12.0 12.0 12.0 378.0 1.22 a[5] 1.545 0.249 1.050 1.909 0.057 0.042 19.0 18.0 21.0 2916.0 1.12 a[6] 1.746 0.179 1.369 2.046 0.021 0.015 72.0 72.0 76.0 3192.0 1.08 a[7] 1.634 0.252 1.126 1.980 0.065 0.048 15.0 14.0 17.0 1853.0 1.16 In [26]: # plot just a subset of the countries counties = [ 'HENNEPIN' , 'AITKIN' , 'WASHINGTON' , 'LAKE OF THE WOODS' , 'YELLOW MEDICINE' , 'ANOKA' ] plt . figure ( figsize = ( 10 , 5 )) rows = 2 gs = gridspec . GridSpec ( rows , len ( counties ) // rows ) for i , county in enumerate ( counties ): county_data = data . loc [ data [ 'county' ] == county ] subplt = plt . subplot ( gs [ i ]) # pooled line (single values coeff for all) xx = np . linspace ( - 0.2 , 1.2 ) radon_est = pooled_trace [ 'a' ] . mean () + pooled_trace [ 'b' ] . mean () * xx radon_est_hier = np . mean ( hierarchical_trace [ 'a' ][ i ]) + \\ np . mean ( hierarchical_trace [ 'b' ][ i ]) * xx # un-pooled (single subject) sns . regplot ( x = 'floor' , y = 'log_radon' , ci = None , label = 'unpooled' , data = county_data ) . set_title ( 'County ' + str ( county )) # hierarchical line subplt . set_ylim ( 0. , 4. ) subplt . scatter ( county_data [ 'floor' ], county_data [ 'log_radon' ]) subplt . plot ( xx , radon_est , c = 'r' , label = 'pooled' ); # plot the hierarchical, varying coefficient model subplt . plot ( xx , radon_est_hier , c = 'g' , label = 'hierarchical' ); subplt . set_xlabel ( 'floor' , fontsize = 10 ) subplt . set_ylabel ( 'radon level' , fontsize = 10 ) subplt . set_title ( str ( county ) + ' County' ) subplt . legend () plt . tight_layout () This tutorial is modified from PyMC3 docs: https://docs.pymc.io/notebooks/GLM-hierarchical.html","tags":"labs","url":"labs/lab03/notebook/"},{"title":"Lab 2: Bayesian Models","text":"Notebook Lab 2: PDFs Lab 2: Bayes","tags":"labs","url":"labs/lab02/"},{"title":"Lab 02:","text":"CS109B Data Science 2: Advanced Topics in Data Science Lab 02 - Bayesian Analysis Harvard University Spring 2022 Instructors: Mark Glickman and Pavlos Protopapas Lab instructor and content: Eleni Angelaki Kaxiras In [1]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: In [2]: import warnings import arviz as az import matplotlib.pyplot as plt import numpy as np import pymc3 as pm import theano.tensor as tt from pymc3 import summary warnings . simplefilter ( action = \"ignore\" , category = FutureWarning ) warnings . filterwarnings ( 'ignore' ) import numpy as np import matplotlib.pyplot as plt from matplotlib import gridspec import scipy.stats as stats import pandas as pd import seaborn as sns % matplotlib inline import warnings print ( 'Running on PyMC3 v {} ' . format ( pm . __version__ )) Running on PyMC3 v3.8 In [3]: %% javascript IPython . OutputArea . auto_scroll_threshold = 20000 ; In [4]: #pandas trick pd . options . display . max_columns = 50 # None -> No Restrictions pd . options . display . max_rows = 200 # None -> Be careful with this pd . options . display . max_colwidth = 100 pd . options . display . precision = 3 Learning Objectives By the end of this lab, you should be able to: Define a probabilistic model in the PyMC3 framework Run a sampling algorithm This lab corresponds to Lectures 3,4, and maps to Homework 2. 1. Bayes Rule We have data that we believe come from an underlying distribution of unknown parameters. If we find those parameters, we know everything about the process that generated this data and we can make inferences (create new data). \\begin{equation} \\label{eq:bayes} P(\\theta|\\textbf{D}) = \\frac{P(\\textbf{D} |\\theta) P(\\theta) }{P(\\textbf{D})} \\end{equation} $P(\\theta|\\textbf{D})$ is the posterior distribution, prob(hypothesis | data) $P(\\textbf{D} |\\theta)$ is the likelihood function, how probable is my data B for different values of the parameters $P(\\theta)$ is the marginal probability to observe the data, called the prior , this captures our belief about the data before observing it. $P(\\textbf{D})$ is the marginal distribution (sometimes called marginal likelihood) But what is $\\theta \\;$? $\\theta$ is an unknown yet fixed set of parameters. In Bayesian inference we express our belief about what $\\theta$ might be and instead of trying to guess $\\theta$ exactly, we look for its probability distribution . What that means is that we are looking for the parameters of that distribution. For example, for a Poisson distribution our $\\theta$ is only $\\lambda$. In a normal distribution, our $\\theta$ is often just $\\mu$ and $\\sigma$. Top 2. Introduction to pyMC3 PyMC3 is a Python library for programming Bayesian analysis, and more specifically, data creation, model definition, model fitting, and posterior analysis. It uses the concept of a model which contains assigned parametric statistical distributions to unknown quantities in the model. Within models we define random variables and their distributions. A distribution requires at least a name argument, and other parameters that define it. You may also use the logp() method in the model to build the model log-likelihood function. We define and fit the model. PyMC3 includes a comprehensive set of pre-defined statistical distributions that can be used as model building blocks. Although they are not meant to be used outside of a model , you can invoke them by using the prefix pm , as in pm.Normal . To use them outside of a model one needs to invoke them as pm.Normal.dist() . 3. Probability distributions in scipy and PyMC3 We can invoke probability distributions from scipy or directly from PyMC3 . Distributions in PyMC3 live within the context of models, although the framework provides a way to use the distributions outside of models. For a review of most common discete and continuous distributions see separate notebook. scipy Normal (a.k.a. Gaussian) \\begin{equation} X \\sim \\mathcal{N}(\\mu,\\,\\sigma&#94;{2}) \\end{equation} A Normal distribution can be parameterized either in terms of precision $\\tau$ or variance $\\sigma&#94;{2}$. The link between the two is given by \\begin{equation} \\tau = \\frac{1}{\\sigma&#94;{2}} \\end{equation} Expected value (mean) $\\mu$ Variance $\\frac{1}{\\tau}$ or $\\sigma&#94;{2}$ Parameters: mu: float , sigma: float or tau: float Range of values (-$\\infty$, $\\infty$) In [5]: plt . style . use ( 'seaborn-darkgrid' ) x = np . linspace ( - 5 , 5 , 1000 ) mus = [ 0. , 0. , 0. , - 2. ] sigmas = [ 0.4 , 1. , 2. , 0.4 ] for mu , sigma in zip ( mus , sigmas ): pdf = stats . norm . pdf ( x , mu , sigma ) plt . plot ( x , pdf , label = r '$\\mu$ = ' + f ' { mu } ,' + r '$\\sigma$ = ' + f ' { sigma } ' ) plt . xlabel ( 'random variable' , fontsize = 12 ) plt . ylabel ( 'probability density' , fontsize = 12 ) plt . legend ( loc = 1 ) plt . show () PyMC3 In [6]: az . style . use ( \"arviz-darkgrid\" ) a = pm . Poisson . dist ( mu = 4 ) b = pm . Normal . dist ( mu = 0 , sigma = 10 ) _ , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 4 )) az . plot_dist ( a . random ( size = 1000 ), color = \"C1\" , label = \"Poisson\" , ax = ax [ 0 ]) az . plot_dist ( b . random ( size = 1000 ), color = \"C2\" , label = \"Gaussian\" , ax = ax [ 1 ]) plt . show () Distributions in PyMC3 Information about PyMC3 functions including descriptions of distributions, sampling methods, and other functions, is available via the help command. In [7]: ## uncomment to look at the documentation #help(pm.Poisson) Top 3. Bayesian Linear Regression Define the Problem Our problem is the following: we want to perform multiple linear regression to predict an outcome variable $Y$ which depends on variables $\\bf{x}_1$ and $\\bf{x}_2$. We will model $Y$ as normally distributed observations with an expected value $mu$ that is a linear function of the two predictor variables, $\\bf{x}_1$ and $\\bf{x}_2$. \\begin{equation} Y \\sim \\mathcal{N}(\\mu,\\,\\sigma) \\end{equation} \\begin{equation} \\mu = \\beta_0 + \\beta_1 \\bf{x}_1 + \\beta_2 x_2 \\end{equation} where $\\sigma$ represents the measurement error (in this example, we will use $\\sigma = 10$). Note: In the code we give the value for the standard deviation $\\sigma$. We also choose the parameters to have normal distributions with those parameters set by us. \\begin{eqnarray} \\beta_i \\sim \\mathcal{N}(0,\\,10) \\\\ \\sigma \\sim |\\mathcal{N}(0,\\,10)| \\end{eqnarray} We will artificially create the data to predict on. We will then see if our model predicts them correctly. Artificially create some data to test our model. In [8]: np . random . seed ( 123 ) # True parameter values < --- our model does not see these sigma = 1 beta0 = 1 beta = [ 1 , 2.5 ] # Size of dataset size = 1000 # Predictor variable x1 = np . linspace ( 0 , 1. , size ) x2 = np . linspace ( 0 , 2. , size ) # Simulate outcome variable Y = beta0 + beta [ 0 ] * x1 + beta [ 1 ] * x2 + np . random . randn ( size ) * sigma In [9]: from mpl_toolkits.mplot3d import Axes3D fig = plt . figure ( figsize = ( 5 , 4 )) fontsize = 14 labelsize = 8 title = 'Observed Data ' + r '$Y(x_1,x_2)$' + ' (created artificially)' ax = fig . add_subplot ( 111 , projection = '3d' ) ax . scatter ( x1 , x2 , Y ) ax . set_xlabel ( r '$x_1$' , fontsize = fontsize ) ax . set_ylabel ( r '$x_2$' , fontsize = fontsize ) ax . set_zlabel ( r '$Y$' , fontsize = fontsize ) ax . tick_params ( labelsize = labelsize ) fig . suptitle ( title , fontsize = fontsize ) fig . tight_layout ( pad = .1 , w_pad = 10.1 , h_pad = 2. ) #fig.subplots_adjust(); #top=0.5 plt . tight_layout plt . show () Now let's see if our model will correctly predict the values for our unknown parameters, namely $b_0$, $b_1$, $b_2$ and $\\sigma$. Define the Model in PyMC3 In [10]: from pymc3 import Model , Normal , HalfNormal , model_to_graphviz , HalfCauchy from pymc3 import NUTS , sample , find_MAP from scipy import optimize Step1: Formulate the probability model for our data: $Y \\sim \\mathcal{N}(\\mu,\\,\\sigma&#94;{2})$. This the likelihood function and it's defined the same as the other distributions except that there is an observed argument which means its values are determined by the data. Y_obs = pm.Normal('Y_obs', mu=mu, sigma=sigma, observed=Y) Step2: Choose a prior distribution for our unknown parameters. beta0 = Normal('beta0', mu=0, sigma=10) # Note: betas is a vector of two variables, b1 and b2, (denoted by shape=2) # so, in array notation, our beta1 = betas[0], and beta2=betas[1] betas = Normal('betas', mu=0, sigma=10, shape=2) sigma = HalfNormal('sigma', sigma=1) Step3: Determine the posterior distribution, this is our main goal . Step4: Summarize important features of the posterior and/or plot the parameters. In [11]: with Model () as my_linear_model : # Priors for unknown model parameters, specifically created stochastic random variables # with Normal prior distributions for the regression coefficients, # and a half-normal distribution for the standard deviation of the observations. # These are our parameters. beta0 = Normal ( 'beta0' , mu = 0 , sd = 10 ) # Note: betas is a vector of two variables, b1 and b2, (denoted by shape=2) # so, in array notation, our beta1 = betas[0], and beta2=betas[1] betas = Normal ( 'betas' , mu = 0 , sd = 10 , shape = 2 ) sigma = HalfNormal ( 'sigma' , sd = 1 ) # mu is what is called a deterministic random variable, which implies that its # value is completely determined by its parents' values (betas and sigma in our case). # There is no uncertainty in the variable beyond that which is inherent in # the parents' values mu = beta0 + betas [ 0 ] * x1 + betas [ 1 ] * x2 # We could have defined mu as a Deterministic variable # mu = pm.Deterministic('mu', beta0 + betas[0]*x1 + betas[1]*x2) # Likelihood function = how probable is my observed data? # This is a special case of a stochastic variable that we call an observed stochastic. # It is identical to a standard stochastic, except that its observed argument, # which passes the data to the variable, indicates that the values for this variable # were observed, and should not be changed by any fitting algorithm applied to the model. # The data can be passed in the form of either a numpy.ndarray or pandas.DataFrame object. Y_obs = Normal ( 'Y_obs' , mu = mu , sd = sigma , observed = Y ) In [12]: Y_obs . distribution Out[12]: $\\text{None} \\sim \\text{Normal}(\\mathit{mu}=f(f(f(\\text{beta0}),~f(f(f(\\text{betas})),~array)),~f(f(f(\\text{betas})),~array)),~\\mathit{sigma}=f(\\text{sigma}))$ Note: If our problem was a classification for which we would use Logistic regression see below In [13]: model_to_graphviz ( my_linear_model ) Out[13]: %3 cluster2 2 cluster1,000 1,000 sigma sigma ~ HalfNormal Y_obs Y_obs ~ Normal sigma->Y_obs beta0 beta0 ~ Normal beta0->Y_obs betas betas ~ Normal betas->Y_obs Markov Chain Monte Carlo (MCMC) Simulations PyMC3 uses the No-U-Turn Sampler (NUTS) and the Random Walk Metropolis , two Markov chain Monte Carlo (MCMC) algorithms for sampling in posterior space. Monte Carlo gets into the name because when we sample in posterior space, we choose our next move via a pseudo-random process. NUTS is a sophisticated algorithm that can handle a large number of unknown (albeit continuous) variables. Fitting the Model with Sampling - Doing Inference See below for PyMC3's sampling method. As you can see it has quite a few parameters. Most of them are set to default values by the package. For some, it's useful to set your own values. pymc3.sampling.sample(draws=500, step=None, init='auto', n_init=200000, start=None, trace=None, chain_idx=0, chains=None, cores=None, tune=500, progressbar=True, model=None, random_seed=None, discard_tuned_samples=True, compute_convergence_checks=True, **kwargs) Parameters to set: draws (int): number of samples to draw, defaults to 500. step (MCMC method): implementation method for MCMC. Better to let pyMC3 assign the best one. If manually setting we can choose between some implementations such as Metropolis() or NUTS() . tune (int): number of iterations to tune for, a.k.a. the \"burn-in\" period, defaults to 500. target_accept (float in $[0, 1]$). The step size is tuned such that we approximate this acceptance rate. Higher values like 0.9 or 0.95 often work better for problematic posteriors. (optional) cores (int) number of chains to run in parallel, defaults to the number of CPUs in the system, but at most 4. pm.sample returns a pymc3.backends.base.MultiTrace object that contains the samples. We usually name it trace . All the information about the posterior is in trace , which also provides statistics about the sampler. In [14]: ## uncomment this to see more about pm.sample #help(pm.sample) Specify a NUTS() sampler It is the default and we expect good results. In [15]: with my_linear_model : print ( f 'Starting MCMC process' ) # draw 2000 posterior samples and run the default number of chains = 4 trace = sample ( 2000 , tune = 1000 , chains = 2 , target_accept = 0.95 ) print ( f 'DONE' ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Starting MCMC process Multiprocess sampling (2 chains in 4 jobs) NUTS: [sigma, betas, beta0] Sampling 2 chains, 0 divergences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [07:16<00:00, 13.74draws/s] DONE Model Plotting PyMC3 provides a variety of visualizations via plots: https://docs.pymc.io/api/plots.html . ArviZ is a Python library that works with PyMC3 for visualizing posterior distributions. One of them is the traceplot . The left column consists of a smoothed histogram (using kernel density estimation) of the marginal posteriors of each stochastic random variable while the right column contains the samples of the Markov chain plotted in sequential order. The beta variable, being vector-valued, produces two histograms and two sample traces, corresponding to both predictor coefficients. Let's compare with the true hidden parameter values sigma = 1, beta0 = 1 , beta1 = 1, beta2 = 2.5 In [44]: az . plot_posterior ( trace , var_names = [ 'beta0' , 'betas' ]); In [17]: from pymc3 import traceplot , compareplot , plot_posterior , forestplot traceplot ( trace ); The Highest Posterior Density (HPD) is the shortest interval that has the given probability indicated by the HPD. $\\hat{R}$ is a metric for comparing how well a chain has converged to the equilibrium distribution by comparing its behavior to other randomly initialized Markov chains. Multiple chains initialized from different initial conditions should give similar results. If all chains converge to the same equilibrium, $\\hat{R}$ will be 1. If the chains have not converged to a common distribution, $\\hat{R}$ will be > 1.01. $\\hat{R}$ is a necessary but not sufficient condition. For details on the $\\hat{R}$ see Gelman and Rubin (1992) . In [19]: pm . rhat ( trace ) Out[19]: Dimensions: (betas_dim_0: 2) Coordinates: * betas_dim_0 (betas_dim_0) int64 0 1 Data variables: beta0 float64 1.001 betas (betas_dim_0) float64 1.003 1.003 sigma float64 1.001 In [20]: forestplot ( trace , varnames = [ 'beta0' , 'betas' , 'sigma' ], r_hat = True ); Table of summary statistics hdi_3% & hdi_97% ‚Äî The lower and upper bounds of a ‚àΩ95% credible interval. mcse_mean, mcse_sd ‚Äî Average and stand. deviation of Monte Carlo standard error. ess_mean ‚Äî Estimated effective sample size from sampling number of draws for each parameter. Effective sample size should be close to actual size, it goes down when parameters become correlated. In [22]: # Then we will generate and display our table az . summary ( trace ) Out[22]: mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat beta0 1.014 0.063 0.887 1.121 0.001 0.001 2291.0 2287.0 2291.0 2065.0 1.0 betas[0] 1.218 8.824 -14.349 18.780 0.230 0.163 1466.0 1466.0 1478.0 1523.0 1.0 betas[1] 2.338 4.412 -6.722 9.858 0.115 0.084 1465.0 1378.0 1477.0 1497.0 1.0 sigma 1.002 0.023 0.958 1.042 0.000 0.000 2421.0 2414.0 2443.0 1893.0 1.0 Specify a Metropolis() sampler We do not expect good results. In [23]: #help(pm.backends.base.MultiTrace) In [24]: with my_linear_model : print ( f 'Starting MCMC process' ) # draw 2000 posterior samples and run the default number of chains = 4 trace_metropolis = sample ( 2000 , step = pm . Metropolis (), tune = 1000 ) #, target_accept=0.9) print ( f 'DONE' ) Starting MCMC process Multiprocess sampling (4 chains in 4 jobs) CompoundStep >Metropolis: [sigma] >Metropolis: [betas] >Metropolis: [beta0] Sampling 4 chains, 0 divergences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12000/12000 [00:05<00:00, 2360.97draws/s] The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge. The estimated number of effective samples is smaller than 200 for some parameters. DONE In [25]: from pymc3 import traceplot , compareplot , plot_posterior , forestplot traceplot ( trace_metropolis ); In [27]: az . plot_posterior ( trace_metropolis , var_names = [ 'beta0' , 'betas' ]); In [28]: #help(az.plot_posterior) In [29]: #help(pm.Normal) In [30]: # Then we will generate and display our table az . summary ( trace_metropolis ) Out[30]: mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat beta0 1.017 0.063 0.904 1.136 0.005 0.003 191.0 191.0 194.0 252.0 1.04 betas[0] 0.472 4.793 -9.535 5.495 2.359 1.803 4.0 4.0 5.0 17.0 2.21 betas[1] 2.709 2.388 0.212 7.667 1.175 0.899 4.0 4.0 5.0 17.0 2.19 sigma 1.003 0.023 0.957 1.045 0.001 0.001 866.0 864.0 870.0 731.0 1.01 In [31]: trace_metropolis . varnames Out[31]: ['beta0', 'betas', 'sigma_log__', 'sigma'] In [32]: #help(pm.backends.base.MultiTrace) This linear regression example is from the original paper on PyMC3: Salvatier J, Wiecki TV, Fonnesbeck C. 2016. Probabilistic programming in Python using PyMC3. PeerJ Computer Science 2:e55 https://doi.org/10.7717/peerj-cs.55 Model Prediction Let's sample directly from the posterior predictive distribution of our PyMC3 model. This way we can create some predicted regression lines. In [33]: with my_linear_model : post = pm . sample_posterior_predictive ( trace , samples = 4 ) post 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 44.42it/s] Out[33]: {'Y_obs': array([[ 0.66814578, 1.76554148, 1.41547496, ..., 5.83976609, 5.78308538, 6.09349731], [-0.07221673, 0.50181349, 1.25587781, ..., 8.43380066, 5.86848228, 6.34353836], [ 2.63758695, 0.90434407, 2.84861913, ..., 8.01730308, 7.42665533, 6.61252778], [ 0.12305881, 0.49877369, 1.54025709, ..., 7.20657886, 6.5785189 , 4.9889308 ]])} In [34]: trace [ 'beta0' ] . mean () Out[34]: 1.0142560725210414 In [35]: trace [ 'betas' ][:, 0 ] . mean () Out[35]: 1.2178921328018786 In [36]: np . meshgrid ( np . linspace ( 0 , 2 , 4 ), np . linspace ( 0 , 3 , 4 )) Out[36]: [array([[0. , 0.66666667, 1.33333333, 2. ], [0. , 0.66666667, 1.33333333, 2. ], [0. , 0.66666667, 1.33333333, 2. ], [0. , 0.66666667, 1.33333333, 2. ]]), array([[0., 0., 0., 0.], [1., 1., 1., 1.], [2., 2., 2., 2.], [3., 3., 3., 3.]])] In [37]: fig = plt . figure ( figsize = ( 5 , 4 )) fontsize = 14 labelsize = 8 title = 'Bayesian Regression Line and Observed Data' ax = fig . add_subplot ( 111 , projection = '3d' ) # predicted surface x1_pred , x2_pred = np . meshgrid ( np . linspace ( 0 , 2 , 10 ), np . linspace ( 0 , 3 , 10 )) b0 = trace [ 'beta0' ] . mean () b1 = trace [ 'betas' ][:, 0 ] . mean () b2 = trace [ 'betas' ][:, 1 ] . mean () for b0 , b in zip ( trace [ 'beta0' ][ 30 : 32 ], trace [ 'betas' ][ 30 : 32 ]): ax . plot_surface ( x1_pred , x2_pred , b0 + b [ 0 ] * x1_pred + b [ 1 ] * x2_pred , alpha = 0.1 , color = 'cyan' ) #z = b0 + b1*x1_pred + b2*x2_pred #ax.plot_surface(x1_pred, x2_pred, z, alpha=0.2, color='cyan') # original data ax . scatter ( x1 , x2 , Y , alpha = 1. ) ax . set_xlabel ( r '$x_1$' , fontsize = fontsize ) ax . set_ylabel ( r '$x_2$' , fontsize = fontsize ) ax . set_zlabel ( r '$Y$' , fontsize = fontsize ) ax . tick_params ( labelsize = labelsize ) fig . suptitle ( title , fontsize = fontsize ) fig . tight_layout ( pad = .1 , w_pad = 10.1 , h_pad = 2. ) plt . tight_layout plt . show () What about Logistic Regression? If the problem above was a classification that required a Logistic Regression, we would use the logistic function ( where $\\beta_0$ is the intercept, and $\\beta_i$ (i=1, 2, 3) determines the shape of the logistic function). \\begin{equation} Pr(Y=1|X_1,X_2,X3) = {\\frac{1}{1 + exp&#94;{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3)}}} \\end{equation} Since both $\\beta_0$ and the $\\beta_i$s can be any possitive or negative number, we can model them as gaussian random variables. \\begin{eqnarray} \\beta_0 \\sim \\mathcal{N}(\\mu,\\,\\sigma&#94;2) \\\\ \\beta_i \\sim \\mathcal{N}(\\mu_i,\\,\\sigma_i&#94;2) \\end{eqnarray} In PyMC3 we can model those as: pm.Normal('beta_0', mu=0, sigma=100) (where $\\mu$ and $\\sigma&#94;2$ can have some initial values that we assign them, e.g. 0 and 100) The deterministic variable would be the log-odds: \\begin{equation} p\\_logit = ln{\\frac{p}{1 - p}} = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 \\end{equation} or in code: p_logit = beta0 + beta_1 * X_1 + beta_2 * X_2 + beta_3 * X_3 To connect this variable ($\\ p\\_logit \\ $) with our observed data, we would use a Bernoulli as our likelihood. our_likelihood = pm.Bernoulli('our_likelihood', logit_p=p_logit, observed=our_data) Notice that the main difference with Linear Regression is the use of a Bernoulli distribution instead of a Gaussian distribution, and the use of the logistic function instead of the identity function. We could also use pm.Deterministic as follows: p_i = pm.Deterministic('p_i', pm.math.invlogit(beta0 + beta_1 * X_1 + beta_2 * X_2 + beta_3 * X_3) And then add this to the likelihood function with the parameter p (p: float Probability of success (0 < p < 1), instead of logit_p (logit_p: float Logit of success probability). Note that only one of p or logit_p can be specified. Then you could define the likelihood: likelh = pm.Bernoulli('likelh', p=p_i, observed=data) In [38]: # A reminder of what the logistic function looks like. # Play with parameters a and b to see the shape of the curve change b = 5. x = np . linspace ( - 8 , 8 , 100 ) plt . plot ( x , 1 / ( 1 + np . exp ( - b * x ))) plt . xlabel ( 'y' ) plt . ylabel ( 'y=logistic(x)' ) Out[38]: Text(0, 0.5, 'y=logistic(x)')","tags":"labs","url":"labs/lab02/bayes-nb/"},{"title":"Lab 02:","text":"CS109B Data Science 2: Advanced Topics in Data Science Lab 02 - Review of Probability Distributions Harvard University Spring 2022 Instructors: Mark Glickman and Pavlos Protopapas Lab instructor and content: Eleni Angelaki Kaxiras In [1]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: In [2]: import warnings import arviz as az import matplotlib.pyplot as plt import numpy as np import pymc3 as pm import theano.tensor as tt from pymc3 import summary warnings . simplefilter ( action = \"ignore\" , category = FutureWarning ) warnings . filterwarnings ( 'ignore' ) import numpy as np import matplotlib.pyplot as plt from matplotlib import gridspec import scipy.stats as stats import pandas as pd import seaborn as sns % matplotlib inline import warnings print ( 'Running on PyMC3 v {} ' . format ( pm . __version__ )) Running on PyMC3 v3.8 In [3]: %% javascript IPython . OutputArea . auto_scroll_threshold = 20000 ; In [4]: #pandas trick pd . options . display . max_columns = 50 # None -> No Restrictions pd . options . display . max_rows = 200 # None -> Be careful with this pd . options . display . max_colwidth = 100 pd . options . display . precision = 3 A review of Common Probability Distributions Discrete Distributions The random variable has a probability mass function (pmf) which measures the probability that our random variable will take a specific value $y$, denoted $P(Y=y)$. Bernoulli (binary outcome, success has probability $\\theta$, $one$ trial): $ P(Y=k) = \\theta&#94;k(1-\\theta)&#94;{1-k} $ Binomial (binary outcome, success has probability $\\theta$, $k$ sucesses, $n$ trials): \\begin{equation} P(Y=k) = {{n}\\choose{k}} \\cdot \\theta&#94;k(1-\\theta)&#94;{n-k} \\end{equation} Note : Binomial(1,$p$) = Bernouli($p$) Poisson (counts independent events occurring at a rate $\\lambda$) \\begin{equation} P\\left( Y=y|\\lambda \\right) = \\frac{{e&#94;{ - \\lambda } \\lambda &#94;y }}{{y!}} \\end{equation} y = 0,1,2,... Categorical, or Multinulli (random variables can take any of K possible categories, each having its own probability; this is a generalization of the Bernoulli distribution for a discrete variable with more than two possible outcomes, such as the roll of a die) Continuous Distributions The random variable has a probability density function (pdf) . Uniform (variable equally likely to be near each value in interval $(a,b)$) \\begin{equation} P(X = x) = \\frac{1}{b - a} \\end{equation} anywhere within the interval $(a, b)$, and zero elsewhere. Normal (a.k.a. Gaussian) \\begin{equation} X \\sim \\mathcal{N}(\\mu,\\,\\sigma&#94;{2}) \\end{equation} A Normal distribution can be parameterized either in terms of precision $\\tau$ or variance $\\sigma&#94;{2}$. The link between the two is given by \\begin{equation} \\tau = \\frac{1}{\\sigma&#94;{2}} \\end{equation} Expected mean $\\mu$ Variance $\\frac{1}{\\tau}$ or $\\sigma&#94;{2}$ Parameters: mu: float , sigma: float or tau: float Range of values (-$\\infty$, $\\infty$) Beta (where the variable ($\\theta$) takes on values in the interval $[0,1]$, and is parametrized by two positive parameters, $\\alpha$ and $\\beta$ that control the shape of the distribution. Note that Beta is a good distribution to use for priors (beliefs) because its range is $[0,1]$ which is the natural range for a probability and because we can model a wide range of functions by changing the $\\alpha$ and $\\beta$ parameters. Its density is: \\begin{equation} \\label{eq:beta} P(\\theta|a,b) = \\frac{1}{B(\\alpha, \\beta)} {\\theta}&#94;{\\alpha - 1} (1 - \\theta)&#94;{\\beta - 1} \\propto {\\theta}&#94;{\\alpha - 1} (1 - \\theta)&#94;{\\beta - 1} \\end{equation} where the normalisation constant, $B$, is a beta function of $\\alpha$ and $\\beta$, \\begin{equation} B(\\alpha, \\beta) = \\int_{t=0}&#94;1 t&#94;{\\alpha - 1} (1 - t)&#94;{\\beta - 1} dt. \\end{equation} 'Nice', unimodal distribution Range of values $[0, 1]$ Exponential Range of values [$0$, $\\infty$] Gamma Code Resources: Statistical Distributions in numpy/scipy: scipy.stats Statistical Distributions in pyMC3: distributions in PyMC3 (we will see when we look at PyMC3). Exercises: Discrete Probability Plots Poisson Change the value of $\\lambda$ in the Poisson PMF and see how the plot changes. Remember that the y-axis in a discrete probability distribution shows the probability of the random variable having a specific value in the x-axis. \\begin{equation} P\\left( X=y \\right|\\lambda) = \\frac{{e&#94;{ - \\lambda } \\lambda &#94;y }}{{y!}} \\end{equation} for $y \\ge0$. Routine is stats.poisson.pmf(x, lambda) . $\\lambda$ is our $\\theta$ in this case. $\\lambda$ is also the mean in this distribution. In [5]: plt . style . use ( 'seaborn-darkgrid' ) x = np . arange ( 0 , 60 ) for lam in [ 0.5 , 3 , 8 ]: pmf = stats . poisson . pmf ( x , lam ) plt . plot ( x , pmf , alpha = 0.5 , label = '$\\lambda$ = {} ' . format ( lam )) plt . xlabel ( 'random variable' , fontsize = 12 ) plt . ylabel ( 'probability' , fontsize = 12 ) plt . legend ( loc = 1 ) plt . ylim = ( - 0.1 ) plt . show () Binomial In [6]: plt . style . use ( 'seaborn-darkgrid' ) x = np . arange ( 0 , 50 ) ns = [ 10 , 17 ] ps = [ 0.5 , 0.7 ] for n , p in zip ( ns , ps ): pmf = stats . binom . pmf ( x , n , p ) plt . plot ( x , pmf , alpha = 0.5 , label = 'n = {} , p = {} ' . format ( n , p )) plt . xlabel ( 'x' , fontsize = 14 ) plt . ylabel ( 'f(x)' , fontsize = 14 ) plt . legend ( loc = 1 ) plt . show () Exercise: Continuous Distributions Plot Uniform Change the value of $\\mu$ in the Uniform PDF and see how the plot changes. Remember that the y-axis in a continuous probability distribution does not shows the actual probability of the random variable having a specific value in the x-axis because that probability is zero!. Instead, to see the probability that the variable is within a small margin we look at the integral below the curve of the PDF. The uniform is often used as a noninformative prior. Uniform - numpy.random.uniform(a=0.0, b=1.0, size) $\\alpha$ and $\\beta$ are our parameters. size is how many tries to perform. Our $\\theta$ is basically the combination of the parameters a,b. We can also call it \\begin{equation} \\mu = (a+b)/2 \\end{equation} In [7]: from scipy.stats import uniform r = uniform . rvs ( size = 1000 ) plt . plot ( r , uniform . pdf ( r ), 'r-' , lw = 5 , alpha = 0.6 , label = 'uniform pdf' ) plt . hist ( r , density = True , histtype = 'stepfilled' , alpha = 0.2 ) plt . ylabel ( r 'probability density' ) plt . xlabel ( f 'random variable' ) plt . legend ( loc = 'best' , frameon = False ) plt . show () Beta We get an amazing set of shapes by tweaking the two parameters $a$ and $b$! Notice that for $a=b=1.$ we get a constant. From then on, as the values increase, we get a curve that looks more and more Gaussian. In [8]: from scipy.stats import beta fontsize = 15 alphas = [ 0.5 , 0.5 , 1. , 3. , 6. ] betas = [ 0.5 , 1. , 1. , 3. , 6. ] x = np . linspace ( 0 , 1 , 1000 ) colors = [ 'red' , 'green' , 'blue' , 'black' , 'pink' ] fig , ax = plt . subplots ( figsize = ( 8 , 5 )) for a , b , colors in zip ( alphas , betas , colors ): dist = beta ( a , b ) plt . plot ( x , dist . pdf ( x ), c = colors , label = f 'a= { a } , b= { b } ' ) ax . set_ylim ( 0 , 3 ) ax . set_xlabel ( r '$\\theta$' , fontsize = fontsize ) ax . set_ylabel ( r 'P ($\\theta|\\alpha,\\beta)$' , fontsize = fontsize ) ax . set_title ( 'Beta Distribution' , fontsize = fontsize * 1.2 ) ax . legend ( loc = 'best' ) fig . show (); Gaussian In [9]: y = pm . Exponential . dist ( lam = 2 ), #y = pm.Binomial.dist(n=10, p=0.5) type ( y ) #print(y.logp(4).eval()) #plt.plot(y.random(size=30)) Out[9]: tuple In [10]: plt . style . use ( 'seaborn-darkgrid' ) x = np . linspace ( - 5 , 5 , 1000 ) mus = [ 0. , 0. , 0. , - 2. ] sigmas = [ 0.4 , 1. , 2. , 0.4 ] for mu , sigma in zip ( mus , sigmas ): pdf = stats . norm . pdf ( x , mu , sigma ) plt . plot ( x , pdf , label = r '$\\mu$ = ' + f ' { mu } ,' + r '$\\sigma$ = ' + f ' { sigma } ' ) plt . xlabel ( 'random variable' , fontsize = 12 ) plt . ylabel ( 'probability density' , fontsize = 12 ) plt . legend ( loc = 1 ) plt . show () At home : Prove the formula mentioned in class which gives the probability density for a Beta distribution with parameters $2$ and $5$: $p(\\theta|2,5) = 30 \\cdot \\theta(1 - \\theta)&#94;4$ References : Distributions in PyMC3 Information about PyMC3 functions including descriptions of distributions, sampling methods, and other functions, is available via the help command.","tags":"labs","url":"labs/lab02/pdfs-nb/"},{"title":"Lab 1: Clustering","text":"Notebook Lab 1: Intro Lab 1: Clustering","tags":"labs","url":"labs/lab01/"},{"title":"Lab 1: Clustering","text":"CS109B Data Science 2: Advanced Topics in Data Science Lecture 01 - Clustering with Python Harvard University Spring 2022 Instructors: Mark Glickman and Pavlos Protopapas Lab Instructor: Eleni Kaxiras Content: Eleni Kaxiras, Chris Tanner, and Will Claybaugh In [1]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES # import requests # from IPython.core.display import HTML # styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\").text # HTML(styles) In [2]: import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from sklearn import preprocessing from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans from sklearn.feature_extraction.text import CountVectorizer from scipy.spatial import distance from sklearn.metrics import silhouette_samples , silhouette_score import matplotlib.cm as cm from sklearn.cluster import DBSCAN from sklearn.neighbors import NearestNeighbors from sklearn.preprocessing import StandardScaler from sklearn.datasets import make_blobs import scipy.cluster.hierarchy as hac from scipy.spatial.distance import pdist import scipy.cluster.hierarchy as hac from scipy.spatial.distance import pdist % matplotlib inline In [3]: from gap_statistic import OptimalK In [4]: from sklearn.datasets import make_blobs from sklearn.datasets import make_classification from sklearn.datasets import load_digits import warnings warnings . filterwarnings ( 'ignore' ) Learning Objectives Understand the common distance metrics (e.g., Euclidean, Manhattan, Hamming). Understand the difference between a vector matrix and a distance matrix Understand how different clustering algorithms work (e.g., k-means, Hierarchical, DBSCAN). For home: Review what PCA is and know the differences between PCA and clustering, Table of Contents PCA Refresher Preparing the data Choosing a distance or dissimilarity metric Clustering algorithms and measuring the quality of clusters Extra: Clustering for images Unsupervised Learning, Cluster Analysis, and Classification Review : What is unsupervised learning? What is Cluster Analysis? Is the response variable included in the algorithm? What does it mean to perform classification? 1. PCA Refresher image source: [1] Review What is PCA? How can it be useful? What are its limitations? image source: [1] Sklearn's sklearn.decomposition.PCA uses the LAPACK library written in Fortran 90 (based on the LINPACK library from the 70s) which provides routines for solving systems of simultaneous linear equations, least-squares solutions of linear systems of equations. How to use the sklearn PCA package: a. Instantiate a new PCA object: pca_transformer = PCA() b. Fit some data (learns the transformation based on this data): fitted_pca = pca_transformer.fit(data_frame) c. Transform the data to the reduced dimensions: pca_df = fitted_pca.transform(data_frame) Using two distinct steps (i.e., (b) and (c)) to fit and transform our data allows one the flexibility to transform any dataset according to our learned fit() . Alternatively, if you know you only want to transform a single dataset, you can combine (b) and (c) into one step: Fit and transform: pca_df = pca_transformer.fit_transform(pca_df) Note: We fit on the training set and transform both training and test set. Example: Playing with synthetic data Sklearn has methods for generating synthetic datasets . They can be quite useful for testing clustering for classification purposes. In [5]: n_features = 2 n_classes = 1 plt . title ( f 'Features = { n_features } ' , fontsize = 'medium' ) X1 , Y1 = make_classification ( n_features = n_features , n_redundant = 0 , n_informative = 1 , n_clusters_per_class = 1 , n_classes = n_classes ) colors = [ \"#4EACC5\" , \"#FF9C34\" , \"#4E9A06\" ] plt . scatter ( X1 [:, 0 ], X1 [:, 1 ], marker = 'o' , c = Y1 , s = 25 , edgecolor = 'k' ); class_df = pd . DataFrame ( X1 , Y1 ) . reset_index ( drop = True ) class_df = class_df . rename ( columns = { 0 : 'feature1' , 1 : 'feature2' }) plt . axis ( 'equal' ); Out[5]: (-1.488981045742101, 3.2948130634705572, -2.007589788641174, 2.621466822917984) In [6]: X1 . shape , Y1 . shape Out[6]: ((100, 2), (100,)) In [7]: class_df . head () Out[7]: feature1 feature2 0 1.012265 -0.072951 1 -0.474946 -1.625100 2 0.265944 1.600704 3 1.826242 -1.519311 4 -0.424610 0.802087 In [8]: scaler = StandardScaler () scaler . fit ( class_df ) scaled_df = pd . DataFrame ( scaler . transform ( class_df )) In [9]: pca_transformer = PCA ( n_components = 2 ) fitted_pca = pca_transformer . fit ( class_df ) fitted_pca . explained_variance_ratio_ Out[9]: array([0.591269, 0.408731]) In [10]: pca_df = pd . DataFrame ( fitted_pca . transform ( class_df )) pca_df = pca_df . rename ( columns = { 0 : 'pc1' , 1 : 'pc2' }) pca_df . head () Out[10]: pc1 pc2 0 -0.164327 -0.128186 1 -2.304623 0.072059 2 0.682850 1.496747 3 -0.791398 -1.664837 4 -0.371981 1.541299 In [11]: fitted_pca . explained_variance_ratio_ Out[11]: array([0.591269, 0.408731]) For more read: \" Importance of feature scaling \" Sklearn's StandardScaler 2 - Preparing the data Discussion: To scale or not to scale? Depends For more read: \" Importance of feature scaling \" Sklearn's StandardScaler 3 - Distance and dissimilarity metrics The Euclidean norm (or length) of a vector $\\textbf{v}=[v_1,v_2,..,v_n]&#94;T$ in $\\mathbb{R}&#94;n$ is the nonnegative scalar \\begin{aligned} \\lVert \\textbf{v} \\rVert = \\sqrt{\\textbf{v}\\cdot \\textbf{v}} = \\sqrt{{v_1}&#94;2+{v_2}&#94;2+\\cdots+{v_n}&#94;2} \\end{aligned} The Manhattan norm of the same vector is the nonnegative scalar \\begin{aligned} \\lVert \\textbf{v} \\rVert = \\lvert \\textbf{v} \\rvert = \\lvert v_1 \\rvert + \\lvert v_2 \\rvert + \\cdots + \\lvert v_n \\rvert \\end{aligned} The distance between two vectors $\\textbf{v}$ and $\\textbf{u}$ is defined by $d(\\textbf{v}, \\textbf{u}) = \\lVert \\textbf{v} - \\textbf{u} \\rVert$ Let's practice on the diagram below; we are concerned with measuring the distance between two points, $\\textbf{p}=(p_1,p_2)$ and $\\textbf{q}=(q_1,q_2)$. (edited from Wikipedia.org) Euclidean distance: The Euclidean distance measures the shortest path between the two points, navigating through all dimensions: $d(\\textbf{p}, \\textbf{q}) = \\lVert \\textbf{p} - \\textbf{q} \\rVert = \\sqrt{{(p_1-q_1)}&#94;2+{(p_2-q_2)}&#94;2}$ For vectors in $\\mathbb{R}&#94;n$: $d(\\textbf{p}, \\textbf{q}) = \\lVert \\textbf{p} - \\textbf{q} \\rVert = \\sqrt{{(p_1-q_1)}&#94;2+{(p_2-q_2)}&#94;2+\\cdots +{(p_n-q_n)}&#94;2}$ Manhattan distance: The Manhattan distance measures the cumulative difference between the two points, across all dimensions. $d_1(\\textbf{p}, \\textbf{q}) = \\lVert \\textbf{p} - \\textbf{q} \\rVert_1 = \\sum_{i=1}&#94;{n} \\mid p_i-q_1 \\mid$ Extra: Cosine distance $\\cos{\\theta} = \\frac{\\textbf{q}\\textbf{q}}{\\lVert \\textbf{p}\\rVert \\lVert\\textbf{q} \\rVert} $ In [12]: count_vect = CountVectorizer () sent0 = \"Biden is here\" sent1 = \"President is coming here\" corpus = [ sent0 , sent1 ] sentences = count_vect . fit_transform ( corpus ) v1 = sentences . toarray ()[ 0 ] v2 = sentences . toarray ()[ 1 ] print ( f 'v1 = { v1 } , \\n v2 = { v2 } ' ) # pretty print df = pd . DataFrame ( sentences . toarray (), \\ columns = count_vect . get_feature_names (), index = [ 'Sentence 0' , 'Sentence 1' ]) print ( f 'distance = { distance . cosine ( v1 , v2 ) } ' ) df v1 = [1 0 1 1 0], v2 = [0 1 1 1 1] distance = 0.42264973081037416 Out[12]: biden coming here is president Sentence 0 1 0 1 1 0 Sentence 1 0 1 1 1 1 Note : Normally cosine value=0 means that the two vectors are orthogonal to each other. scipy implements cosine as 1-cosine , so cosine=0 means no connection and cosine=1 means orthogonal. Cosine metric is used in Collaborative Filtering (Recommender systems for movies). Hamming Distance (extra): If our two elements of comparison can be represented a sequence of discrete items, it can be useful to measure how many of their elements differ. For example: Mahmoud and Mahmood differ by just 1 character and thus have a hamming distance of 1. 10101 and 01101 have a hamming distance of 2. Mary and Barry have a hamming distance of 3 (m->b, y->r, null->y). Note : the last example may seem sub-optimal, as we could transform Mary to Barry by just 2 operations (substituting the M with a B, then adding an 'r'). So, their so-called edit distance is smaller than their Hamming distance. The very related Levenshtein distance here can handle this, and thus tends to be more appropriate for Strings. 4 - Clustering Algorithms Question: Why do we care about clustering? How/why is it useful? We will now walk through three clustering algorithms, first discussing them at a high-level, then showing how to implement them with Python libraries. Let's first load and scale our data, so that particular dimensions don't naturally dominate in their contributions in the distant calculations: In [13]: multishapes = pd . read_csv ( \"data/multishapes.csv\" ) ms_df = multishapes [[ 'x' , 'y' ]] ms_df . plot . scatter ( x = 'x' , y = 'y' , c = 'Blue' , \\ title = \"Multishapes data\" , \\ figsize = ( 5.5 , 4.2 )) plt . show () ms_df . head () Out[13]: x y 0 -0.803739 -0.853053 1 0.852851 0.367618 2 0.927180 -0.274902 3 -0.752626 -0.511565 4 0.706846 0.810679 In [14]: # displays our summary statistics of our data ms_df . describe () Out[14]: x y count 1100.000000 1100.000000 mean -0.081222 -0.625431 std 0.644967 1.176170 min -1.489180 -3.353462 25% -0.478839 -1.126752 50% -0.132920 -0.297040 75% 0.366072 0.250817 max 1.492208 1.253874 In [15]: scaler = StandardScaler () scaler = scaler . fit ( ms_df ) print ( scaler . mean_ ) scaled_df = scaler . transform ( ms_df ) ###### if I had a test set I would transform here: # test_scaled_df = scaler.transform(test_df) ################################################## scaled_df = pd . DataFrame ( scaled_df , \\ index = multishapes [ 'shape' ], columns = ms_df . columns ) scaled_df . describe () [-0.08122171 -0.6254313 ] Out[15]: x y count 1.100000e+03 1.100000e+03 mean -3.108624e-17 -2.779595e-16 std 1.000455e+00 1.000455e+00 min -2.183985e+00 -2.320473e+00 25% -6.167723e-01 -4.264248e-01 50% -8.019252e-02 2.793306e-01 75% 6.938298e-01 7.453401e-01 max 2.440659e+00 1.598544e+00 Very important reminder!! If you have a training and a test set, always .fit() your scaler only to the training set, and then .transform() both sets. Let's plot this data with and without scaling In [16]: # plot our data msplot = ms_df . plot . scatter ( x = 'x' , y = 'y' , c = 'Black' , \\ title = \"Multishapes data (no scaling)\" , \\ figsize = ( 5.5 , 4.2 )) msplot . set_xlabel ( \"X\" ) msplot . set_ylabel ( \"Y\" ) plt . show () In [17]: # plots our data msplot = scaled_df . plot . scatter ( x = 'x' , y = 'y' , c = 'Black' , \\ title = \"Multishapes data (w/ scaling)\" , \\ figsize = ( 5.5 , 4.2 )) msplot . set_xlabel ( \"X\" ) msplot . set_ylabel ( \"Y\" ) plt . show () 3a. $k$-Means clustering: Code (via sklearn ): In [18]: ms_kmeans = KMeans ( n_clusters = 2 , init = 'random' , n_init = 3 , random_state = 109 ) ms_kmeans . fit ( scaled_df ) Out[18]: KMeans(init='random', n_clusters=2, n_init=3, random_state=109) Now that we've run k-means, we can look at various attributes of our clusters. Full documenation is here . In [19]: display ( ms_kmeans . cluster_centers_ ) display ( ms_kmeans . labels_ [ 0 : 10 ]) array([[-0.44316951, -1.56366452], [ 0.1434568 , 0.50616818]]) array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32) Plotting Take note of matplotlib's c= argument to color items in the plot, along with our stacking two different plotting functions in the same plot. In [20]: plt . figure ( figsize = ( 10 , 10 )) plt . scatter ( scaled_df [ 'x' ], scaled_df [ 'y' ], c = ms_kmeans . labels_ ); plt . scatter ( ms_kmeans . cluster_centers_ [:, 0 ], ms_kmeans . cluster_centers_ [:, 1 ], c = 'r' , marker = 'h' , s = 100 ); Out[20]: Quality of Clusters A - Inertia Inertia measures the total squared distance from points to their cluster's centroid. We obviously want this distance to be relatively small. If we increase the number of clusters, it will naturally make the average distance smaller. If every point has its own cluster, then our distance would be 0. That's obviously not an ideal way to cluster. One way to determine a reasonable number of clusters to simply try many different clusterings as we vary k , and each time, measure the overall inertia. In [21]: wss = [] for i in range ( 1 , 11 ): fitx = KMeans ( n_clusters = i , init = 'random' , n_init = 5 , random_state = 109 ) . fit ( scaled_df ) wss . append ( fitx . inertia_ ) plt . figure ( figsize = ( 11 , 8.5 )) plt . plot ( range ( 1 , 11 ), wss , 'bx-' ) plt . xlabel ( 'Number of clusters $k$' ) plt . ylabel ( 'Inertia' ) plt . title ( 'The Elbow Method showing the optimal $k$' ) plt . show () Look for the place(s) where distance stops decreasing as much (i.e., the 'elbow' of the curve). It seems that 4 would be a good number of clusters, as a higher k yields diminishing returns. Exercise : Run K-means again with 4 clusters this time. B - Silhouette Let's say we have a data point $i$, and the cluster it belongs to is referred to as $C(i)$. One way to measure the quality of a cluster $C(i)$ is to measure how close its data points are to each other (within-cluster) compared to nearby, other clusters $C(j)$. This is what Silhouette Scores provide for us. The range is [-1,1]; 0 indicates a point on the decision boundary (equal average closeness to points intra-cluster and out-of-cluster), and negative values mean that datum might be better in a different cluster. Specifically, let $a(i)$ denote the average distance data point $i$ is to the other points in the same cluster: Similarly, we can also compute the average distance that data point $i$ is to all other clusters. The cluster that yields the minimum distance is denoted by $b(i)$: Hopefully our data point $i$ is much closer, on average, to points within its own cluster (i.e., $a(i)$ than it is to its closest neighboring cluster $b(i)$). The silhouette score quantifies this as $s(i)$: NOTE: If data point $i$ belongs to its own cluster (no other points), then the silhouette score is set to 0 (otherwise, $a(i)$ would be undefined). The silhouette score plotted below is the overall average across all points in our dataset. The silhouette_score() function is available in sklearn . We can manually loop over values of K (for applying k-Means algorithm), then plot its silhouette score. In [22]: scores = [ 0 ] for i in range ( 2 , 11 ): fitx = KMeans ( n_clusters = i , init = 'random' , n_init = 46 , random_state = 109 ) . fit ( scaled_df ) score = silhouette_score ( scaled_df , fitx . labels_ ) scores . append ( score ) plt . figure ( figsize = ( 11 , 8.5 )) plt . plot ( range ( 1 , 11 ), np . array ( scores ), 'bx-' ) plt . xlabel ( 'Number of clusters $k$' ) plt . ylabel ( 'Average Silhouette' ) plt . title ( 'The Elbow Method showing the optimal $k$' ) plt . show () Visualizing all Silhoutte scores for a particular clustering Below, we borrow from an sklearn example. The second plot may be overkill. The second plot is just the scaled data. It is not a PCA plot If you only need the raw silhouette scores, use the silhouette_samples() function In [23]: # modified code from # http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html def silplot ( X , clusterer , pointlabels = None ): cluster_labels = clusterer . labels_ n_clusters = clusterer . n_clusters # Create a subplot with 1 row and 2 columns fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 ) fig . set_size_inches ( 11 , 8.5 ) # The 1st subplot is the silhouette plot # The silhouette coefficient can range from -1, 1 but in this example all # lie within [-0.1, 1] ax1 . set_xlim ([ - 0.1 , 1 ]) # The (n_clusters+1)*10 is for inserting blank space between silhouette # plots of individual clusters, to demarcate them clearly. ax1 . set_ylim ([ 0 , len ( X ) + ( n_clusters + 1 ) * 10 ]) # The silhouette_score gives the average value for all the samples. # This gives a perspective into the density and separation of the formed # clusters silhouette_avg = silhouette_score ( X , cluster_labels ) print ( \"For n_clusters = \" , n_clusters , \", the average silhouette_score is \" , silhouette_avg , \".\" , sep = \"\" ) # Compute the silhouette scores for each sample sample_silhouette_values = silhouette_samples ( X , cluster_labels ) y_lower = 10 for i in range ( 0 , n_clusters ): # Aggregate the silhouette scores for samples belonging to # cluster i, and sort them ith_cluster_silhouette_values = \\ sample_silhouette_values [ cluster_labels == i ] ith_cluster_silhouette_values . sort () size_cluster_i = ith_cluster_silhouette_values . shape [ 0 ] y_upper = y_lower + size_cluster_i color = cm . nipy_spectral ( float ( i ) / n_clusters ) ax1 . fill_betweenx ( np . arange ( y_lower , y_upper ), 0 , ith_cluster_silhouette_values , facecolor = color , edgecolor = color , alpha = 0.7 ) # Label the silhouette plots with their cluster numbers at the middle ax1 . text ( - 0.05 , y_lower + 0.5 * size_cluster_i , str ( i )) # Compute the new y_lower for next plot y_lower = y_upper + 10 # 10 for the 0 samples ax1 . set_title ( \"The silhouette plot for the various clusters.\" ) ax1 . set_xlabel ( \"The silhouette coefficient values\" ) ax1 . set_ylabel ( \"Cluster label\" ) # The vertical line for average silhouette score of all the values ax1 . axvline ( x = silhouette_avg , color = \"red\" , linestyle = \"--\" ) ax1 . set_yticks ([]) # Clear the yaxis labels / ticks ax1 . set_xticks ([ - 0.1 , 0 , 0.2 , 0.4 , 0.6 , 0.8 , 1 ]) # 2nd Plot showing the actual clusters formed colors = cm . nipy_spectral ( cluster_labels . astype ( float ) / n_clusters ) ax2 . scatter ( X [:, 0 ], X [:, 1 ], marker = '.' , s = 200 , lw = 0 , alpha = 0.7 , c = colors , edgecolor = 'k' ) xs = X [:, 0 ] ys = X [:, 1 ] if pointlabels is not None : for i in range ( len ( xs )): plt . text ( xs [ i ], ys [ i ], pointlabels [ i ]) # Labeling the clusters centers = clusterer . cluster_centers_ # Draw white circles at cluster centers ax2 . scatter ( centers [:, 0 ], centers [:, 1 ], marker = 'o' , c = \"white\" , alpha = 1 , s = 200 , edgecolor = 'k' ) for i , c in enumerate ( centers ): ax2 . scatter ( c [ 0 ], c [ 1 ], marker = '$ %d $' % int ( i ), alpha = 1 , s = 50 , edgecolor = 'k' ) ax2 . set_title ( \"The visualization of the clustered data.\" ) ax2 . set_xlabel ( \"Feature space for the 1st feature\" ) ax2 . set_ylabel ( \"Feature space for the 2nd feature\" ) plt . suptitle (( \"Silhouette analysis for KMeans clustering on sample data \" \"with n_clusters = %d \" % n_clusters ), fontsize = 14 , fontweight = 'bold' ) In [24]: ms_kmeans = KMeans ( n_clusters = 4 , init = 'random' , n_init = 3 , random_state = 109 ) . fit ( scaled_df ) # plot a fancy silhouette plot silplot ( scaled_df . values , ms_kmeans ) For n_clusters = 4, the average silhouette_score is 0.45880539445085916. C - Gap Statistic The gap statistic compares within-cluster distances (such as in silhouette), but instead of comparing against the second-best existing cluster for that point, it compares our clustering's overall average to the average we'd see if the data were generated at random (we'd expect randomly generated data to not necessarily have any inherit patterns that can be easily clustered). In essence, the within-cluster distances (in the elbow plot) will go down just becuse we have more clusters. We additionally calculate how much they'd go down on non-clustered data with the same spread as our data and subtract that trend out to produce the plot below. The original paper is : \" Estimating the number of clusters in a data set via the gap statistic \" (Tibshirani et al.). As suggested in the paper, we would choose the value of $\\hat{k}$ (number of clusters) such that $\\hat{k}$ = smallest $k$ such that Gap($k$) $\\geq$ Gap($k+1$) - $s_{k+1}$. We compare the actual Gap value of the k point to the lower bar of the Gap value of the k+1 point. The following graph should make it clearer. The plot is from the original paper (Fig. 2) (dashed lines are mine) We could argue that we should have chosen the largest value (k =3) instead of the first value that satisfies the Gap statistic equation (k=1 in this case). If you're able to compute for a range of k, then you can choose the maximum. For example, in the graph above, since we're computing over k=1,..,10, we could choose k=3. The original paper although it suggests that we look at the whole range, chooses k=1 in the case above; if you see the raw data plotted (Fig. 2 in the paper) you will also notice that there is really not much structure for cluster subdivision but we should always investigate the whole plot. Also, it's very computationally intensive to compute the Gap statistic. Additionally, you can use domain knowledge or whatever information you have about the data to choose k. The gap statistic is implemented by Miles Granger in the gap_statistic ( https://github.com/milesgranger/gap_statistic ) Python library. The library also implements the Gap$&#94;*$ statistic described in \"A comparison of Gap statistic definitions with and with-out logarithm function( https://core.ac.uk/download/pdf/12172514.pdf )\" (Mohajer, M., Englmeier, K. H., & Schmid, V. J., 2011) which is less conservative but tends to perform suboptimally when clusters overlap. Generating synthetic data again In [25]: plt . title ( \"Four blobs\" , fontsize = 'small' ) X1 , Y1 = make_blobs ( n_features = 2 , centers = 4 ) # centers is number of classes plt . scatter ( X1 [:, 0 ], X1 [:, 1 ], marker = 'o' , c = Y1 , s = 25 , edgecolor = 'k' ); blob_df = pd . DataFrame ( X1 , Y1 ) plt . axis ( 'equal' ); blob_df . head () Out[25]: 0 1 1 -0.780056 6.247700 3 4.942465 -2.969769 2 3.552698 -7.304807 1 -1.574659 6.366932 2 7.879188 -7.814325 In [26]: gs_obj = OptimalK () n_clusters = gs_obj ( X1 , n_refs = 500 , cluster_array = np . arange ( 1 , 15 )) print ( 'Optimal number of clusters: ' , n_clusters ) Optimal number of clusters: 4 In [27]: ms_kmeans = KMeans ( n_clusters = n_clusters , init = 'random' , \\ n_init = 3 , random_state = 109 ) . fit ( X1 ) plt . figure ( figsize = ( 5 , 5 )) plt . scatter ( X1 [:, 0 ], X1 [:, 1 ], marker = 'o' , c = Y1 , s = 25 , edgecolor = 'k' ); plt . scatter ( ms_kmeans . cluster_centers_ [:, 0 ], \\ ms_kmeans . cluster_centers_ [:, 1 ], c = 'r' , marker = 'h' , s = 200 ); plt . axis ( 'equal' ); Out[27]: (-3.3025435783231623, 10.473280979534094, -10.94762035999889, 8.835835742038647) In [28]: def display_gapstat_with_errbars ( gap_df ): gaps = gap_df [ \"gap_value\" ] . values diffs = gap_df [ \"diff\" ] err_bars = np . zeros ( len ( gap_df )) err_bars [ 1 :] = diffs [: - 1 ] - gaps [: - 1 ] + gaps [ 1 :] plt . scatter ( gap_df [ \"n_clusters\" ], gap_df [ \"gap_value\" ]) plt . errorbar ( gap_df [ \"n_clusters\" ], gap_df [ \"gap_value\" ], yerr = err_bars , capsize = 6 ) plt . xlabel ( \"Number of Clusters\" ) plt . ylabel ( \"Gap Statistic\" ) plt . show () display_gapstat_with_errbars ( gs_obj . gap_df ) For more information about the gap_stat package, please see the full documentation here . 3b. Agglomerative Clustering Code (via scipy ): There are many different cluster-merging criteria, one of which is Ward's criteria. Ward's optimizes having the lowest total within-cluster distances, so it merges the two clusters that will harm this objective least. scipy 's agglomerative clustering function implements Ward's method. In [29]: USArrests = pd . read_csv ( 'data/USArrests2.csv' ) In [30]: df = USArrests [[ 'SexualAssault' , 'Assault' , 'UrbanPop' , 'Homicide' ]] arrests_scaled = pd . DataFrame ( preprocessing . scale ( df ), index = USArrests [ 'State' ], columns = df . columns ) Understanding the notion of a vector (data) matrix and a distance or dissimilarity matrix In [31]: # data matrix df . iloc [[ 3 , 15 , 40 ]] Out[31]: SexualAssault Assault UrbanPop Homicide 3 19.5 190 50 8.8 15 18.0 115 66 6.0 40 12.8 86 45 3.8 In [32]: # distance matrix from sklearn.metrics import pairwise_distances pairwise_distances ( df . iloc [[ 3 , 15 , 40 ]], metric = 'euclidean' ) Out[32]: array([[ 0. , 76.75343641, 104.45520571], [ 76.75343641, 0. , 36.24748267], [104.45520571, 36.24748267, 0. ]]) In [33]: # import scipy.cluster.hierarchy as hac plt . figure ( figsize = ( 11 , 8.5 )) dist_mat = pdist ( arrests_scaled , metric = \"euclidean\" ) ward_data = hac . ward ( dist_mat ) hac . dendrogram ( ward_data , labels = USArrests [ \"State\" ] . values ); plt . show () Discussion : How do you read a plot like the above? What are valid options for number of clusters, and how can you tell? Are some more valid than others? Lessons: It's expensive: $O(n&#94;3)$ time complexity and $O(n&#94;2)$ space complexity. Many choices for linkage criteria Every node gets clustered (no child left behind) Example of the use of hierarchical clustering as an exploratory tool Clustering our data in a hierarchical way can reveal associations between them. We can exploit those associations later in the model. 3c. DBSCAN Clustering DBSCAN uses an intuitive notion of denseness to define clusters, rather than defining clusters by a central point as in k-means. Code (via sklearn ): DBscan is implemented in sklearn , but there aren't great automated tools for searching for the optimal epsilon parameter. For full documentation, please visit this page In [34]: plt . figure ( figsize = ( 11 , 8.5 )) fitted_dbscan = DBSCAN ( eps = 0.2 ) . fit ( scaled_df ) plt . scatter ( scaled_df [ 'x' ], scaled_df [ 'y' ], c = fitted_dbscan . labels_ ); Out[34]: Note: the dark purple dots are not clustered with anything else. They are lone singletons. You can validate such by setting epsilon to a very small value, and increase the min_samples to a high value. Under these conditions, nothing would cluster, and yet all dots become dark purple. Exercise : Experiment with the above code by changing its epsilon value and the min_samples (what is the default value for it, since the above code doesn't specify a value?) Lessons: Can cluster non-linear relationships very well; potential for more natural, arbritrarily shaped groupings Does not require specifying the # of clusters (i.e., k ); the algorithm determines such Robust to outliers Doesn't guarantee that every (or ANY) item will be clustered 5. Extra: Clustering for images Breakroom Exercise : Let's load the MNIST digits and try to use $k$-means to cluster them. In [35]: digits = load_digits () print ( digits . data . shape , digits . images . shape , digits . target . shape ) (1797, 64) (1797, 8, 8) (1797,) In [36]: plt . gray () plt . figure ( figsize = ( 8 , 5 )) for i in range ( 12 ): plt . subplot ( 4 , 3 , i + 1 ); plt . imshow ( digits . images [ i ]); In [37]: scaler = StandardScaler () scaler . fit ( digits . data ) digits_scaled = scaler . transform ( digits . data ) In [38]: digits_scaled . shape Out[38]: (1797, 64) In [39]: n_digits = 10 kmeans = KMeans ( init = \"random\" , n_clusters = n_digits , random_state = 22 ) kmeans . fit ( digits_scaled ) Out[39]: KMeans(init='random', n_clusters=10, random_state=22) In [40]: cluster_assignments = zip ( digits_scaled , kmeans . labels_ ) In [41]: # inititalize cluster dict clusters = {} for i in range ( n_digits ): clusters [ i ] = [] clusters Out[41]: {0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: []} In [42]: for idx , j in enumerate ( kmeans . labels_ ): clusters [ j ] . append ( digits . target [ idx ]) Check out the cluster quality manually In [43]: for num in range ( 10 ): df = pd . Series ( clusters [ num ]) print ( f 'Cluster = { num } ' ) print ( df . value_counts ()) Cluster = 0 0 176 6 1 dtype: int64 Cluster = 1 2 113 8 11 3 7 7 5 1 1 dtype: int64 Cluster = 2 1 59 9 19 8 12 4 6 2 5 7 3 6 1 dtype: int64 Cluster = 3 2 43 1 27 3 1 dtype: int64 Cluster = 4 3 158 9 144 8 50 5 40 2 3 dtype: int64 Cluster = 5 7 18 4 8 9 3 dtype: int64 Cluster = 6 4 158 5 2 7 1 0 1 dtype: int64 Cluster = 7 6 178 8 4 5 2 0 1 dtype: int64 Cluster = 8 7 152 1 94 8 94 2 13 3 13 9 11 4 8 5 4 6 1 dtype: int64 Cluster = 9 5 134 3 4 9 3 8 3 4 1 1 1 dtype: int64 References: [1] A Tutorial on Principal Component Analysis","tags":"labs","url":"labs/lab01/notebook-clustering/"},{"title":"Lab 1: Intro to the environment","text":"CS109B Data Science 2: Advanced Topics in Data Science Lab 01 - Coding Environment Setup Harvard University Spring 2022 Instructors: Pavlos Protopapas and Mark Glickman Lab Instructor: Eleni Kaxiras In [2]: import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Learning Goals The purpose of this lab is to help you set up the coding environment for CS109B 1. Getting class material Option 1A: Download directly from Ed Use the >> to download. Option 1B: Cloning the class repo and then copying the contents in a different directory so you can make changes. You may access the code used in class by cloning the class repo: https://github.com/Harvard-IACS/2022-CS109B Open the Terminal in your computer and go to the Directory where you want to clone the repo. Then run git clone https://github.com/Harvard-IACS/2022-CS109B.git If you have already cloned the repo, OR if new material is added (happens every day), go inside the '/2022-CS109B/' directory and run git pull Caution: If you change the notebooks and then run git pull your changes will be overwritten. So create a playground folder and copy the folder with the notebook with which you want to work. 2. Running code: Option 2A: Using your local environment Use Virtual Environments: we cannot stress this enough! Isolating your projects inside specific environments helps you manage dependencies and therefore keep your sanity. You can recover from mess-ups by simply deleting an environment. Sometimes certain installation of libraries conflict with one another. The two most popular tools for setting up environments are: conda (a package and environment manager) pip (a Python package manager) with virtualenv (a tool for creating environments) We recommend using conda package installation and environments. conda installs packages from the Anaconda Repository and Anaconda Cloud, whereas pip installs packages from PyPI. Even if you are using conda as your primary package installer and are inside a conda environment, you can still use pip install for those rare packages that are not included in the conda ecosystem. See here for more details on how to manage Conda Environments . Use the cs109b.yml file to create an environment: $ conda env create -f cs109b.yml $ conda activate cs109b We have included the packages that you will need in the cs109b.yml file. Option 2B: Using Cloud Resources Using FAS OnDemand (supported by CS109b) FAS provides a platform, accessible via the FAS OnDemand menu link in Canvas . Most of the libraries such as keras, tensorflow, pandas, etc., are pre-installed. If a library is missing you may install it via the Terminal. NOTE : The AWS platform is funded by FAS for the purposes of the class. You are not allowed to use it for purposes not related to this course. Make sure you stop your instance as soon as you do not need it. Information on how to use the platform is displayed when you click the link. For more see Fas OnDemand Guide . Using Google Colab (on your own) Google's Colab platform https://colab.research.google.com/ offers a GPU enviromnent to test your ideas, it's fast, free, with the only caveat that your files persist only for 12 hours (last time we checked). The solution is to keep your files in a repository and just clone it each time you use Colab. Using AWS in the Cloud (on your own) For those of you who want to have your own machines in the Cloud to run whatever you want, Amazon Web Services is a (paid) solution. For more see: https://docs.aws.amazon.com/polly/latest/dg/setting-up.html Remember, AWS is a paid service, so if you let your machine run for days you will get charged! 3. Ensuring everything is installed correctly Some of the packages we will need for this class Clustering : Sklearn - https://scikit-learn.org/stable/ scipy - https://www.scipy.org gap_statistic (by Miles Granger) - https://anaconda.org/milesgranger/gap-statistic/notebook Bayes : pymc3 - https://docs.pymc.io Neural Networks : keras - https://www.tensorflow.org/guide/keras Exercise 1: Run the following cells to make sure these packages load correctly in our environment. In [5]: from sklearn import datasets iris = datasets . load_iris () digits = datasets . load_digits () digits . target # you should see [0, 1, 2, ..., 8, 9, 8] Out[5]: array([0, 1, 2, ..., 8, 9, 8]) In [18]: from scipy import misc import matplotlib.pyplot as plt face = misc . face () face . shape , type ( face ) Out[18]: ((768, 1024, 3), numpy.ndarray) In [20]: face [ 1 : 3 , 1 : 3 ] Out[20]: array([[[110, 103, 121], [130, 122, 143]], [[ 94, 87, 105], [115, 108, 126]]], dtype=uint8) In [6]: plt . imshow ( face ) plt . show () # you should see a racoon In [10]: import pymc3 as pm print ( 'Running PyMC3 v {} ' . format ( pm . __version__ )) # you should see 'Running on PyMC3 v3.8' Running PyMC3 v3.8 In [11]: # making sure you have gap_statistic from gap_statistic import OptimalK 4. Plotting matplotlib and seaborn matplotlib seaborn: statistical data visualization . seaborn works great with pandas . It can also be customized easily. Here is the basic seaborn tutorial: Seaborn tutorial . Plotting a function of 2 variables using contours In optimization, our objective function will often be a function of two or more variables. While it's hard to visualize a function of more than 3 variables, it's very informative to plot one of 2 variables. To do this we use contours. First we define the $x$ and $y$ variables and then construct their pairs using meshgrid . Plot the function $f(x,y) = x&#94;2+y&#94;2$ In [12]: import seaborn as sn In [30]: x = np . linspace ( - 0.1 , 0.1 , 50 ) y = np . linspace ( - 0.1 , 0.1 , 100 ) xx , yy = np . meshgrid ( x , y ) z = np . sqrt ( xx ** 2 + yy ** 2 ) plt . contour ( x , y , z ); 5. We will be using keras via tensorflow TensorFlow is a framework for representing complicated ML algorithms and executing them in any platform, from a phone to a distributed system using GPUs. Developed by Google Brain, TensorFlow is used very broadly today. Keras , is a high-level API, created by Fran√ßois Chollet, and used for fast prototyping, advanced research, and production. tf.keras is now maintained by Tensorflow. Exercise 2: Run the following cells to make sure you have the basic libraries to do deep learning In [14]: from __future__ import absolute_import , division , print_function , unicode_literals # TensorFlow and tf.keras import tensorflow as tf from tensorflow.keras import layers from tensorflow.keras import models from tensorflow.keras.layers import Dense from tensorflow.keras.models import Sequential from tensorflow.keras.regularizers import l2 tf . keras . backend . clear_session () # For easy reset of notebook state. # You should see a >=2.3.0 here! # If you do not, upgrade your env to tensorflow==2.3.0 print ( tf . __version__ ) print ( tf . keras . __version__ ) 2.3.0 2.4.0 In [15]: # Check if your machine has NVIDIA GPUs. hasGPU = tf . config . list_physical_devices () print ( f 'My computer has the following devices: { hasGPU } ' ) My computer has the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')]","tags":"labs","url":"labs/lab01/notebook-intro/"},{"title":"CS109b: Advanced Topics in Data Science","text":"Spring 2022 Pavlos Protopapas & Mark Glickman Advanced Topics in Data Science (CS109b) is the second half of a one-year introduction to data science. Building upon the material in Introduction to Data Science, the course introduces advanced methods for data wrangling, data visualization, statistical modeling, and prediction. Topics include big data, multiple deep learning architectures such as CNNs, RNNs, transformers, language models, autoencoders, and generative models as well as basic Bayesian methods, and unsupervised learning. Helpline: cs109b2022@gmail.com Lectures: Mon & Wed 9:45‚Äê11 am Labs: Fri 9:45-11 am Advanced Sections: Weds 2:15-3:30 pm (see schedule for specific dates) Office Hours: See Ed Post Course material can be viewed in the public GitHub repository . Previous Material 2021 2020 2019 2018","tags":"pages","url":"pages/cs109b-advanced-topics-in-data-science/"}]}